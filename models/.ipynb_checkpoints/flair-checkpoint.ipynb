{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from flair.models import TextClassifier\n",
    "\n",
    "from flair.data import TaggedCorpus\n",
    "from flair.data_fetcher import  NLPTaskDataFetcher, NLPTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Mongo database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "\n",
    "db = client['glvis_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4125"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(db['flair_chunk'].find({'text':'in'})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4657"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term = 'in'\n",
    "pipeline = {\n",
    "    '$text': {'$search': f'{term}'}\n",
    "}\n",
    "\n",
    "len(list(db['flair_chunk'].find(pipeline)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract hidden representations from flair's pretrained Chunking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_col = db['flair_chunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_tagger = SequenceTagger.load('chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_type = 'np'\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(corpus.get_all_sentences()):\n",
    "    print(f'Start sentence {i}')\n",
    "    \n",
    "    # Define hook to get intermediate values\n",
    "    hidden_states = torch.zeros(len(sentence), 1, 512)\n",
    "    def hook(m, i):\n",
    "        hidden_states.copy_(i[0].data)\n",
    "                                \n",
    "    h = pos_tagger.linear.register_forward_pre_hook(hook)\n",
    "                                \n",
    "    pos_tagger.predict(sentence)\n",
    "                                \n",
    "    spans = sentence.get_spans('chunk')\n",
    "                                \n",
    "    # Informaction to store: the named entities, their predicted labels, probabilities and hidden states\n",
    "    # If there are multiple words for one entity, take the average value of hidden states\n",
    "    # and record the number of words in the entity\n",
    "    \n",
    "    for span in spans:\n",
    "        entry = {}\n",
    "        entry['text'] = span.text\n",
    "        entry['tag'] = span.tag\n",
    "        entry['score'] = span.score\n",
    "        entry['token_num'] = len(span.tokens)\n",
    "        \n",
    "        idx = [token.idx-1 for token in span.tokens]\n",
    "        entry['linear_layer_state'] = hidden_states[idx, :, :].mean(dim=0).squeeze().tolist()\n",
    "        \n",
    "        db_col.insert_one(entry)\n",
    "    \n",
    "    h.remove()\n",
    "    \n",
    "    print(f'Finish sentence {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract hidden representations from flair's pretrained POS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_col = db['flair_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-20 13:17:23,367 loading file /home/snie/.flair/models/en-pos-ontonotes-v0.2.pt\n"
     ]
    }
   ],
   "source": [
    "pos_tagger = SequenceTagger.load('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path='/home/snie/Workspace/glvis/models/data/conll/OntoNotes-5.0-NER-BIO/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-20 13:17:53,943 Reading data from /home/snie/Workspace/glvis/models/data/conll/OntoNotes-5.0-NER-BIO/ontoner\n",
      "2019-03-20 13:17:53,944 Train: /home/snie/Workspace/glvis/models/data/conll/OntoNotes-5.0-NER-BIO/ontoner/onto.train.ner\n",
      "2019-03-20 13:17:53,945 Dev: /home/snie/Workspace/glvis/models/data/conll/OntoNotes-5.0-NER-BIO/ontoner/onto.development.ner\n",
      "2019-03-20 13:17:53,945 Test: None\n"
     ]
    }
   ],
   "source": [
    "corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.ONTONER, base_path=base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dictionary = corpus.make_tag_dictionary(tag_type='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_col.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_set = set()\n",
    "for entry in db_col.find():\n",
    "    tag_set.add(entry['tag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$',\n",
       " \"''\",\n",
       " ',',\n",
       " '-LRB-',\n",
       " '-RRB-',\n",
       " '.',\n",
       " ':',\n",
       " 'ADD',\n",
       " 'AFX',\n",
       " 'CC',\n",
       " 'CD',\n",
       " 'DT',\n",
       " 'EX',\n",
       " 'FW',\n",
       " 'HYPH',\n",
       " 'IN',\n",
       " 'JJ',\n",
       " 'JJR',\n",
       " 'JJS',\n",
       " 'LS',\n",
       " 'MD',\n",
       " 'NFP',\n",
       " 'NN',\n",
       " 'NNP',\n",
       " 'NNPS',\n",
       " 'NNS',\n",
       " 'PDT',\n",
       " 'POS',\n",
       " 'PRP',\n",
       " 'PRP$',\n",
       " 'RB',\n",
       " 'RBR',\n",
       " 'RBS',\n",
       " 'RP',\n",
       " 'SYM',\n",
       " 'TO',\n",
       " 'UH',\n",
       " 'VB',\n",
       " 'VBD',\n",
       " 'VBG',\n",
       " 'VBN',\n",
       " 'VBP',\n",
       " 'VBZ',\n",
       " 'WDT',\n",
       " 'WP',\n",
       " 'WP$',\n",
       " 'WRB',\n",
       " 'XX',\n",
       " '``'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(corpus.get_all_sentences()[71090:]):\n",
    "    print(f'Start sentence {i}')\n",
    "    \n",
    "    # Define hook to get intermediate values\n",
    "    hidden_states = torch.zeros(len(sentence), 1, 512)\n",
    "    def hook(m, i):\n",
    "        hidden_states.copy_(i[0].data)\n",
    "                                \n",
    "    h = pos_tagger.linear.register_forward_pre_hook(hook)\n",
    "                                \n",
    "    pos_tagger.predict(sentence)\n",
    "                                \n",
    "    spans = sentence.get_spans('pos')\n",
    "                                \n",
    "    # Informaction to store: the named entities, their predicted labels, probabilities and hidden states\n",
    "    # If there are multiple words for one entity, take the average value of hidden states\n",
    "    # and record the number of words in the entity\n",
    "    \n",
    "    for span in spans:\n",
    "        entry = {}\n",
    "        entry['text'] = span.text\n",
    "        entry['tag'] = span.tag\n",
    "        entry['score'] = span.score\n",
    "        entry['token_num'] = len(span.tokens)\n",
    "        \n",
    "        idx = span.tokens[0].idx - 1\n",
    "        entry['linear_layer_state'] = hidden_states[idx, :, :].squeeze().tolist()\n",
    "        \n",
    "        db_col.insert_one(entry)\n",
    "    \n",
    "    h.remove()\n",
    "    \n",
    "    print(f'Finish sentence {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract hidden representations from flair's pretrained NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_col = db['flair_ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 14:12:05,408 loading file /home/snie/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    }
   ],
   "source": [
    "ner = SequenceTagger.load('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-19 14:12:09,359 Reading data from data/conll/conll_03\n",
      "2019-03-19 14:12:09,360 Train: data/conll/conll_03/eng.train\n",
      "2019-03-19 14:12:09,360 Dev: data/conll/conll_03/eng.testa\n",
      "2019-03-19 14:12:09,360 Test: data/conll/conll_03/eng.testb\n"
     ]
    }
   ],
   "source": [
    "corpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_03, base_path='data/conll/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dictionary = corpus.make_tag_dictionary(tag_type='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_set = set()\n",
    "counter = 0\n",
    "for entry in db['flair_ner'].find():\n",
    "    counter += 1\n",
    "    tag_set.add(entry['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC', 'MISC', 'ORG', 'PER'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " 'O',\n",
       " 'S-ORG',\n",
       " 'S-MISC',\n",
       " 'B-PER',\n",
       " 'E-PER',\n",
       " 'S-LOC',\n",
       " 'B-ORG',\n",
       " 'E-ORG',\n",
       " 'I-PER',\n",
       " 'S-PER',\n",
       " 'B-MISC',\n",
       " 'I-MISC',\n",
       " 'E-MISC',\n",
       " 'I-ORG',\n",
       " 'B-LOC',\n",
       " 'E-LOC',\n",
       " 'I-LOC',\n",
       " '<START>',\n",
       " '<STOP>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_dictionary.get_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22137"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.get_all_sentences())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_col.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(corpus.get_all_sentences()):\n",
    "    print(f'Start sentence {i}')\n",
    "    \n",
    "    # Define hook to get intermediate values\n",
    "    hidden_states = torch.zeros(len(sentence), 1, 512)\n",
    "    def hook(m, i):\n",
    "        hidden_states.copy_(i[0].data)\n",
    "                                \n",
    "    h = ner.linear.register_forward_pre_hook(hook)\n",
    "                                \n",
    "    ner.predict(sentence)\n",
    "                                \n",
    "    spans = sentence.get_spans('ner')\n",
    "                                \n",
    "    # Informaction to store: the named entities, their predicted labels, probabilities and hidden states\n",
    "    # If there are multiple words for one entity, take the average value of hidden states\n",
    "    # and record the number of words in the entity\n",
    "    \n",
    "    for span in spans:\n",
    "        entry = {}\n",
    "        entry['text'] = span.text\n",
    "        entry['tag'] = span.tag\n",
    "        entry['score'] = span.score\n",
    "        entry['token_num'] = len(span.tokens)\n",
    "        \n",
    "        idx = [token.idx-1 for token in span.tokens]\n",
    "        entry['linear_layer_state'] = hidden_states[idx, :, :].mean(dim=0).squeeze().tolist()\n",
    "        \n",
    "        db_col.insert_one(entry)\n",
    "    \n",
    "    h.remove()\n",
    "    \n",
    "    print(f'Finish sentence {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract hidden states from pretrained en-sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = os.listdir('data/aclImdb_v1/aclImdb/train/pos/')\n",
    "train_neg = os.listdir('data/aclImdb_v1/aclImdb/train/neg/')\n",
    "test_pos = os.listdir('data/aclImdb_v1/aclImdb/test/pos/')\n",
    "test_neg = os.listdir('data/aclImdb_v1/aclImdb/test/neg/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = []\n",
    "for name in train_pos:\n",
    "    with open('data/aclImdb_v1/aclImdb/train/pos/' + name, 'r') as f:\n",
    "        pos_data.append(f.readline())\n",
    "for name in test_pos:\n",
    "    with open('data/aclImdb_v1/aclImdb/test/pos/' + name, 'r') as f:\n",
    "        pos_data.append(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_data = []\n",
    "for name in train_neg:\n",
    "    with open('data/aclImdb_v1/aclImdb/train/neg/' + name, 'r') as f:\n",
    "        neg_data.append(f.readline())\n",
    "for name in test_neg:\n",
    "    with open('data/aclImdb_v1/aclImdb/test/neg/' + name, 'r') as f:\n",
    "        neg_data.append(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_model = TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hook to get intermediate values\n",
    "records = torch.zeros(batch_size, 2048)\n",
    "\n",
    "def hook(m, i, o):\n",
    "    print(i[0].data.shape)\n",
    "    records.copy_(i[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the hook to model\n",
    "h = sent_model.decoder.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390.625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_data) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(neg_data) // batch_size):\n",
    "    sentences = [Sentence(s) for s in neg_data[batch_size * i: batch_size * (i+1)]]\n",
    "    sent_model.predict(sentences, mini_batch_size=batch_size)\n",
    "    labels = [sen.labels[0].to_dict() for sen in sentences]\n",
    "    \n",
    "    val_list = records.tolist()\n",
    "    \n",
    "    db_entries = [{\n",
    "        'sentence': neg_data[batch_size * i + ix],\n",
    "        'reduce_mean': val_list[ix],\n",
    "        'label': labels[ix]\n",
    "    } for ix in range(len(sentences))]\n",
    "    \n",
    "    val_collection.insert_many(db_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattened val_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rec in val_collection.find():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 'POSITIVE', 'confidence': 1.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_fields = [\n",
    "    {'$addFields': {'sentiment': '$label.value', 'confidence': '$label.confidence'}},\n",
    "    {'$out': 'flattened'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = db['flattened']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x7f4fd14ab9c8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete label fields in the document store\n",
    "flattened.update_many({}, {'$unset': {'label': ''}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add index to val_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_collection = db['glove_6b_300d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_collection.drop_index('word_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word_text'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_collection.create_index([('word', pymongo.TEXT)], default_language='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test search on the index\n",
    "cur = val_collection.find({\n",
    "    '$and': [\n",
    "        {'$text': {'$search': 'happy'}}, \n",
    "        {'sentiment': 'NEGATIVE'},\n",
    "        {'confidence': {'$eq': 1.0}}\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(cur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(term):\n",
    "    pipeline = {\n",
    "        '$text': {'$search': term}\n",
    "    }\n",
    "\n",
    "    return list(val_collection.find(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = query('\\\"movie\\\"\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.array([elm['val'] for elm in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(vectors, axis=0)\n",
    "std = np.mean(vectors, axis=0)\n",
    "stats = [\n",
    "    {\n",
    "        'dim': i,\n",
    "        'mean': val[0],\n",
    "        'std': val[1]\n",
    "    } for i, val in enumerate(zip(mean, std))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    {\"$group\": {\"_id\": \"$sentence\", \"count\": {\"$sum\": 1}}},\n",
    "    {\"$match\": {\"count\": {\"$gt\": 1 }}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = val_collection.aggregate(pipeline, allowDiskUse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

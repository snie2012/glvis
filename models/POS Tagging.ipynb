{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible libraries and models to use: flair, fastai, allennlp, huggingface\n",
    "\n",
    "### Summary report: https://nlpprogress.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Official Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2afc030c90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.5525,  0.6355, -0.3968]]),\n",
       " tensor([[-0.6571, -1.6428,  0.9803]]),\n",
       " tensor([[-0.0421, -0.8206,  0.3133]]),\n",
       " tensor([[-1.1352,  0.3773, -0.2824]]),\n",
       " tensor([[-2.5667, -1.4303,  0.5009]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2682,  0.0304, -0.1526]],\n",
      "\n",
      "        [[-0.5370,  0.0346, -0.1958]],\n",
      "\n",
      "        [[-0.3947,  0.0391, -0.1217]],\n",
      "\n",
      "        [[-0.1854,  0.0740, -0.0979]],\n",
      "\n",
      "        [[-0.3600,  0.0893,  0.0215]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[-0.3600,  0.0893,  0.0215]]], grad_fn=<StackBackward>), tensor([[[-1.1298,  0.4467,  0.0254]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0388, -0.9874, -1.2962],\n",
      "        [-0.9900, -1.0083, -1.3334],\n",
      "        [-0.9792, -1.0514, -1.2912],\n",
      "        [-0.9556, -1.0550, -1.3197],\n",
      "        [-0.9470, -1.0579, -1.3284]])\n",
      "tensor([[-0.0882, -2.5320, -5.3084],\n",
      "        [-3.8870, -0.0483, -3.6236],\n",
      "        [-2.2643, -3.0358, -0.1648],\n",
      "        [-0.0950, -2.6379, -3.9579],\n",
      "        [-4.1251, -0.0204, -5.5152]])\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flair (https://github.com/zalandoresearch/flair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a sentence\n",
    "sentence = Sentence('I love Berlin .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the POS tagger\n",
    "pos_tagger = SequenceTagger.load('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence: \"I love Berlin .\" - 4 Tokens]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceTagger(\n",
       "  (embeddings): StackedEmbeddings(\n",
       "    (list_embedding_0): CharLMEmbeddings(\n",
       "      (lm): LanguageModel(\n",
       "        (drop): Dropout(p=0.25)\n",
       "        (encoder): Embedding(275, 100)\n",
       "        (rnn): LSTM(100, 2048)\n",
       "        (decoder): Linear(in_features=2048, out_features=275, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (list_embedding_1): CharLMEmbeddings(\n",
       "      (lm): LanguageModel(\n",
       "        (drop): Dropout(p=0.25)\n",
       "        (encoder): Embedding(275, 100)\n",
       "        (rnn): LSTM(100, 2048)\n",
       "        (decoder): Linear(in_features=2048, out_features=275, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (embedding2nn): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (rnn): LSTM(4096, 256, bidirectional=True)\n",
       "  (linear): Linear(in_features=512, out_features=53, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_output = torch.zeros(4, 512)\n",
    "def lstm_hook(m, i, o):\n",
    "    print(len(o))\n",
    "    lstm_output.copy_(o[0].data)\n",
    "lstm = pos_tagger.rnn.register_forward_hook(lstm_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f2a4eea3cf8>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.zeros(4, 1, 53)\n",
    "def output_hook(m, i, o): output.copy_(o.data)\n",
    "pos_tagger.linear.register_forward_hook(output_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PackedSequence(data=tensor([[-1.4883e-02, -3.6345e-02,  9.4824e-06,  ..., -7.4751e-01,\n",
      "         -4.9711e-01, -8.1787e-01],\n",
      "        [-2.0132e-06,  3.1252e-01,  7.3382e-04,  ..., -2.5950e-03,\n",
      "         -5.1696e-01, -2.6363e-03],\n",
      "        [ 6.0661e-01,  3.4886e-04, -2.9972e-01,  ..., -5.8422e-01,\n",
      "         -3.0078e-01, -8.5479e-02],\n",
      "        [ 8.6339e-01, -8.2297e-05,  1.2962e-02,  ..., -5.1039e-01,\n",
      "          7.5236e-01,  1.4459e-01]], device='cuda:0'), batch_sizes=tensor([1, 1, 1, 1])), (tensor([[[ 8.6339e-01, -8.2297e-05,  1.2962e-02, -5.2747e-03,  2.8766e-02,\n",
      "          -2.3255e-03, -1.6458e-03, -5.3015e-01,  1.2371e-04, -1.4159e-02,\n",
      "           1.2232e-01, -1.5929e-01,  7.7751e-04,  7.3277e-01,  5.7955e-01,\n",
      "          -7.2455e-01,  7.5977e-01,  6.6417e-01,  8.3169e-01,  1.6522e-02,\n",
      "           1.2402e-01,  7.5080e-01, -8.6025e-01, -7.2322e-01,  2.6157e-01,\n",
      "          -1.4576e-02,  5.4367e-01,  2.2868e-02, -6.2168e-05,  2.3959e-04,\n",
      "           1.2088e-01,  2.5572e-01,  2.4748e-01, -1.6540e-01, -1.0256e-03,\n",
      "           7.6035e-01, -2.7907e-03, -1.0987e-03,  1.1586e-04, -4.7525e-01,\n",
      "           7.2113e-01,  4.3589e-01, -5.4250e-03, -3.8552e-01,  6.8723e-01,\n",
      "           4.7789e-03,  1.6430e-03, -5.9408e-01,  4.3598e-02, -3.5659e-02,\n",
      "          -2.9209e-02, -2.5596e-04, -7.2837e-03, -1.2439e-03,  1.3635e-03,\n",
      "           5.1719e-01,  7.0959e-01,  2.9773e-02, -1.1944e-02, -6.8658e-01,\n",
      "          -7.5715e-02,  7.1876e-02, -2.1645e-01,  2.0668e-01,  8.2477e-04,\n",
      "           5.0946e-05,  1.9576e-01, -3.1146e-03, -5.4517e-02,  4.7817e-01,\n",
      "           6.5883e-01, -7.7328e-05,  6.1124e-04,  1.6086e-02,  2.5710e-02,\n",
      "          -3.7643e-04, -1.3366e-03, -5.7513e-01, -7.5587e-01, -1.0639e-04,\n",
      "          -3.7005e-03, -1.9372e-01, -1.3628e-03,  1.6621e-03, -2.1366e-01,\n",
      "           1.9469e-03, -4.3274e-01,  8.8760e-03,  1.9572e-05,  5.0142e-04,\n",
      "          -3.0277e-01, -6.9515e-01, -8.4036e-03,  5.1134e-02,  7.3247e-01,\n",
      "          -4.4552e-01, -9.0639e-04, -4.8697e-01,  7.5183e-01,  6.5118e-01,\n",
      "           2.1081e-03, -6.6157e-01, -2.8927e-03,  7.1952e-01, -4.3556e-02,\n",
      "           6.7953e-06, -5.0279e-01, -4.3033e-04,  9.2809e-03, -1.2676e-01,\n",
      "          -6.5865e-01,  6.1853e-02, -2.6872e-03,  5.1193e-01,  2.2517e-01,\n",
      "          -4.4642e-01,  4.1991e-01,  3.2507e-01,  4.4524e-04, -4.5593e-04,\n",
      "           3.4829e-01, -5.1822e-01, -7.8166e-01,  4.0129e-06, -4.8517e-01,\n",
      "           5.6558e-01, -1.2862e-01, -8.3411e-03,  8.6777e-02,  5.6654e-01,\n",
      "           6.3654e-01, -8.9564e-02,  2.7719e-03, -7.5288e-01, -7.1801e-04,\n",
      "          -6.9620e-01,  9.2303e-02, -7.4568e-02, -1.5605e-01, -2.3491e-01,\n",
      "          -4.1914e-01,  4.8247e-01,  2.5158e-02,  8.8502e-03,  3.4214e-01,\n",
      "           5.8478e-01,  7.3568e-01, -2.7727e-01, -1.5276e-02,  2.5045e-02,\n",
      "          -1.9640e-01, -2.2582e-05, -3.9137e-04,  4.0625e-01,  5.0928e-01,\n",
      "          -7.4868e-01,  1.5125e-03,  3.7695e-02,  6.0506e-01,  7.1493e-01,\n",
      "          -1.4122e-03,  5.9429e-02, -1.2176e-03, -7.9118e-02,  7.0656e-01,\n",
      "          -2.1274e-03, -5.2224e-03,  3.8339e-01, -9.5472e-03,  3.5465e-02,\n",
      "           4.8192e-01,  1.4899e-05, -4.2292e-01, -5.1604e-04,  3.3542e-03,\n",
      "          -4.8556e-01, -2.3650e-01,  2.0344e-06,  3.5478e-02, -2.1249e-03,\n",
      "          -1.0888e-04,  1.3170e-05,  1.4047e-01,  3.5602e-01, -2.8909e-03,\n",
      "          -7.5526e-01, -6.3569e-01, -7.7593e-03,  4.8422e-01,  1.8401e-05,\n",
      "           6.3878e-01,  1.3028e-01, -3.2679e-03, -7.5864e-01,  3.7532e-01,\n",
      "           5.4118e-01, -4.8726e-01, -7.0412e-01,  5.5524e-01,  6.6840e-01,\n",
      "          -7.4586e-01,  4.7405e-01, -5.3556e-01, -1.3380e-01, -5.7113e-01,\n",
      "          -2.0959e-01,  7.0832e-04, -3.9652e-01, -8.4839e-02, -3.6386e-01,\n",
      "          -3.5214e-03, -9.7685e-02, -4.3129e-03, -6.6450e-03,  4.8305e-01,\n",
      "          -7.3130e-01,  9.1449e-02,  1.4568e-02, -5.7429e-02, -2.7527e-03,\n",
      "           2.0810e-01,  9.3812e-01,  1.5117e-02,  2.3748e-03,  3.8290e-01,\n",
      "          -7.3188e-01,  1.7807e-02, -3.5782e-02, -2.1853e-04, -2.0899e-03,\n",
      "          -5.6661e-02,  6.5145e-01,  2.5267e-02, -1.2240e-01, -1.1907e-01,\n",
      "           9.0946e-03, -5.9191e-01,  4.2908e-02, -5.1800e-02,  2.1770e-07,\n",
      "          -5.0206e-02,  1.3222e-01, -1.6414e-01,  5.4419e-01, -1.3324e-02,\n",
      "           9.7500e-04,  7.5174e-01, -6.3023e-01,  6.1948e-01,  4.8303e-02,\n",
      "          -2.4022e-01, -6.5483e-06, -7.5583e-01,  1.7561e-02,  5.7352e-02,\n",
      "          -1.3407e-01]],\n",
      "\n",
      "        [[-2.6729e-03, -5.5149e-02, -1.5596e-05,  6.9317e-03,  6.8732e-02,\n",
      "           2.1285e-02, -1.0179e-01, -7.5503e-01,  3.5214e-02,  5.2348e-02,\n",
      "          -5.2441e-01, -9.0491e-02, -5.5588e-01, -2.4109e-01, -1.0079e-03,\n",
      "          -1.0050e-04,  7.6056e-01,  2.1587e-03,  4.3845e-03,  8.6399e-01,\n",
      "           2.4626e-02,  6.0883e-02, -1.0603e-04,  7.5426e-01, -6.9387e-03,\n",
      "           5.1572e-06, -5.4135e-02,  2.2505e-01, -1.4248e-01,  4.3586e-05,\n",
      "           4.1361e-06, -7.2111e-01,  5.5184e-04,  3.4250e-05,  4.7567e-01,\n",
      "          -7.0861e-01,  5.0313e-01, -7.8437e-05, -6.7334e-01,  9.1585e-03,\n",
      "          -5.7172e-03,  6.9001e-02,  1.5086e-05, -7.5587e-01,  1.2726e-01,\n",
      "          -3.1915e-01, -6.3265e-01, -1.2842e-02,  7.5294e-01, -4.5109e-01,\n",
      "          -1.1692e-01,  5.0272e-01,  1.9310e-05, -7.9006e-01, -6.4822e-01,\n",
      "          -5.7604e-05,  7.4323e-01, -1.4423e-06,  7.5432e-01, -2.6136e-06,\n",
      "           8.4920e-01, -6.5323e-05, -6.3045e-02,  9.9838e-03, -7.0228e-01,\n",
      "          -6.5868e-02,  3.5247e-07,  7.4629e-01, -1.4853e-01, -6.9728e-01,\n",
      "           4.7033e-01,  1.1812e-01,  4.6931e-04,  6.4620e-01, -1.6374e-01,\n",
      "           7.6575e-01,  2.9713e-01, -9.7344e-02, -7.6758e-06,  1.8469e-01,\n",
      "          -4.2140e-01, -7.2905e-01,  7.6055e-01, -4.8074e-04,  5.6438e-01,\n",
      "           6.6935e-04, -2.4369e-01,  9.6801e-07, -3.9924e-01,  4.1170e-04,\n",
      "          -6.5299e-04,  8.6103e-03,  2.5896e-01, -4.0611e-01, -9.1255e-01,\n",
      "           2.7179e-01,  6.9345e-01, -4.5339e-02, -1.3154e-06, -7.4763e-06,\n",
      "           2.2638e-01,  3.9890e-01, -1.2630e-02,  3.7882e-01,  6.9748e-01,\n",
      "          -8.5958e-01,  5.6182e-01, -1.1241e-01,  5.0896e-02, -4.3435e-06,\n",
      "           9.5182e-01,  6.7869e-01,  6.9927e-01,  6.1698e-07,  7.5928e-01,\n",
      "          -1.0145e-01, -1.8823e-03,  3.2212e-02,  1.6416e-02,  7.0759e-01,\n",
      "          -7.5857e-01,  9.5129e-01,  9.5044e-01,  1.1645e-01, -1.3153e-04,\n",
      "          -7.1005e-01, -1.5254e-02,  1.2634e-01,  5.6843e-01,  7.5567e-01,\n",
      "           6.7876e-01,  3.2517e-03, -8.9337e-11,  4.4688e-02,  2.4206e-02,\n",
      "           3.5420e-01, -2.3466e-03,  1.0441e-01,  1.4563e-01, -5.9812e-05,\n",
      "           8.3256e-02, -3.7557e-04,  7.6475e-01,  6.0326e-01, -1.5958e-05,\n",
      "          -7.0635e-04, -3.4083e-02,  9.7244e-04, -3.9776e-01, -7.0888e-02,\n",
      "          -2.7629e-03, -5.3350e-06,  2.0502e-09,  6.0818e-01, -3.3481e-01,\n",
      "          -3.6858e-02, -7.4437e-01,  5.9294e-01, -5.2043e-01,  7.6937e-01,\n",
      "           4.7259e-04, -6.4208e-01,  5.3617e-01,  7.9249e-02,  7.9878e-01,\n",
      "           1.9696e-04,  1.9414e-07,  6.9107e-02,  2.1130e-01, -1.3631e-06,\n",
      "           7.6664e-01, -1.5544e-02,  7.7580e-02,  8.6669e-04, -7.8866e-01,\n",
      "           3.1984e-01, -1.4282e-02,  1.0385e-05, -5.7473e-01, -7.9796e-04,\n",
      "          -7.8098e-02,  6.9534e-01,  7.3461e-01, -1.7592e-02, -2.8449e-04,\n",
      "           2.5881e-02, -6.6700e-04,  1.7461e-03, -2.3047e-01,  1.2834e-01,\n",
      "          -1.2542e-04,  5.1269e-01, -6.5369e-01, -3.8284e-02, -5.6008e-01,\n",
      "           7.6120e-01, -3.0689e-04, -1.7449e-05, -9.7548e-01,  9.3303e-08,\n",
      "           8.2560e-02,  1.5392e-06, -3.5932e-03, -2.1965e-02,  7.3075e-01,\n",
      "           7.6157e-01,  5.7652e-05,  4.2221e-01,  1.0073e-03, -3.5016e-01,\n",
      "          -7.1951e-02, -5.6899e-05, -9.1491e-06, -9.5940e-06, -2.6802e-02,\n",
      "           1.6377e-02, -6.2465e-01, -6.9331e-01, -1.4498e-03, -5.0387e-09,\n",
      "           6.6856e-01,  4.7772e-02, -1.7363e-03, -7.3625e-03,  1.1120e-02,\n",
      "          -1.1554e-02,  2.6459e-01, -5.6541e-04, -7.6088e-01,  1.8780e-01,\n",
      "           6.1821e-04,  5.1538e-01,  4.8698e-01,  2.5475e-07,  1.4688e-02,\n",
      "           3.5894e-04, -5.1799e-01,  8.1600e-01, -7.7900e-02, -9.6922e-01,\n",
      "          -3.6928e-01,  1.6435e-01,  4.4502e-01, -6.8466e-01, -2.5676e-01,\n",
      "          -6.8643e-04, -5.3311e-01,  1.3863e-04,  7.7983e-01, -7.3619e-01,\n",
      "          -3.0234e-05, -6.8647e-02,  4.3560e-07, -7.4751e-01, -4.9711e-01,\n",
      "          -8.1787e-01]]], device='cuda:0'), tensor([[[ 1.3077e+00, -8.5816e-04,  7.2467e-01, -5.6756e-03,  2.8796e-02,\n",
      "          -8.2772e-01, -6.2185e-03, -7.7540e-01,  5.8974e-02, -9.7896e-01,\n",
      "           6.4522e-01, -1.6089e-01,  1.2457e-02,  9.7861e-01,  1.0022e+00,\n",
      "          -9.9466e-01,  9.9845e-01,  8.5578e-01,  1.1936e+00,  3.4435e-01,\n",
      "           9.8913e-01,  9.8294e-01, -1.3141e+00, -9.1616e-01,  1.0001e+00,\n",
      "          -9.9967e-01,  6.3369e-01,  2.8096e-02, -6.4137e-01,  9.5901e-01,\n",
      "           1.3955e-01,  2.8687e-01,  5.1042e-01, -1.8869e-01, -1.8641e-01,\n",
      "           9.9815e-01, -2.9415e-03, -9.9761e-01,  9.8204e-01, -5.2915e-01,\n",
      "           9.9836e-01,  5.0804e-01, -1.0562e-01, -4.2144e-01,  9.9273e-01,\n",
      "           1.4263e-02,  1.7609e+00, -6.9739e-01,  4.3628e-02, -5.6893e-01,\n",
      "          -9.9458e-01, -1.9883e-03, -3.7244e-01, -1.9699e+00,  9.9774e-01,\n",
      "           9.2166e-01,  9.5954e-01,  3.0395e-02, -1.2341e-02, -1.3924e+00,\n",
      "          -3.1103e-01,  6.7596e-01, -3.9327e-01,  7.4080e-01,  8.7779e-04,\n",
      "           2.8023e-01,  5.3673e-01, -3.1147e-03, -7.0205e-02,  1.0689e+00,\n",
      "           9.7436e-01, -8.2797e-01,  1.0410e+00,  1.6115e-02,  5.2856e-01,\n",
      "          -5.1935e-04, -1.3980e-03, -9.9902e-01, -1.0049e+00, -7.9868e-01,\n",
      "          -4.8191e-03, -5.2721e-01, -4.6306e-02,  9.8973e-01, -9.4633e-01,\n",
      "           1.9473e-03, -9.8936e-01,  7.5396e-01,  8.7714e-03,  1.2563e+00,\n",
      "          -8.1858e-01, -9.9238e-01, -1.0373e-02,  5.1205e-02,  9.3405e-01,\n",
      "          -4.8966e-01, -9.0639e-04, -7.3622e-01,  1.0005e+00,  8.5483e-01,\n",
      "           1.5433e-02, -1.0000e+00, -1.7090e+00,  9.9115e-01, -5.9547e-02,\n",
      "           1.7014e-04, -9.5644e-01, -1.1099e-01,  1.3735e-01, -1.2758e-01,\n",
      "          -7.9049e-01,  6.2534e-02, -2.6879e-03,  1.0792e+00,  9.9976e-01,\n",
      "          -1.0025e+00,  9.9967e-01,  3.3733e-01,  3.9535e-03, -4.5706e-04,\n",
      "           4.1703e-01, -1.1704e+00, -1.1263e+00,  7.4939e-01, -9.9832e-01,\n",
      "           6.9637e-01, -1.3404e-01, -6.7002e-01,  8.7211e-02,  6.4252e-01,\n",
      "           7.7600e-01, -1.9937e-01,  3.3260e-01, -9.8266e-01, -9.7282e-01,\n",
      "          -8.6070e-01,  9.9998e-01, -7.8395e-02, -1.5739e-01, -2.3938e-01,\n",
      "          -1.8386e+00,  9.7199e-01,  3.1311e-02,  3.9178e-01,  1.3224e+00,\n",
      "           6.7466e-01,  1.0150e+00, -2.8502e-01, -1.8274e-01,  2.5453e-02,\n",
      "          -9.9216e-01, -5.4604e-03, -1.1951e-03,  5.5024e-01,  5.9544e-01,\n",
      "          -1.0215e+00,  1.5177e-03,  9.8357e-01,  9.9956e-01,  9.0993e-01,\n",
      "          -1.1770e+00,  1.0111e+00, -8.8242e-01, -1.0643e-01,  8.9291e-01,\n",
      "          -2.1643e-03, -9.7585e-01,  1.9137e+00, -1.0322e-02,  2.8225e-01,\n",
      "           2.0850e+00,  3.7996e-02, -4.5939e-01, -1.6606e+00,  5.9626e-01,\n",
      "          -8.1418e-01, -3.4430e-01,  4.9787e-03,  9.6140e-01, -9.9883e-01,\n",
      "          -7.0062e-01,  7.5295e-01,  6.2516e-01,  3.7737e-01, -2.9663e-03,\n",
      "          -9.8510e-01, -9.9696e-01, -1.1544e-01,  9.1548e-01,  1.1440e-02,\n",
      "           8.2725e-01,  1.3181e-01, -1.6767e-01, -9.9580e-01,  4.1002e-01,\n",
      "           1.1158e+00, -5.5123e-01, -9.9983e-01,  6.2694e-01,  8.3802e-01,\n",
      "          -9.6461e-01,  9.5754e-01, -7.1132e-01, -5.1539e-01, -9.4095e-01,\n",
      "          -1.2008e+00,  9.3167e-01, -4.2003e-01, -9.2377e-01, -3.8169e-01,\n",
      "          -8.5565e-02, -9.6327e-01, -9.9796e-01, -4.8392e-02,  5.2696e-01,\n",
      "          -1.0732e+00,  1.3617e+00,  2.7111e-01, -1.1561e-01, -9.9096e-01,\n",
      "           1.9359e+00,  1.7296e+00,  1.5469e-02,  1.6332e-01,  4.0383e-01,\n",
      "          -1.0025e+00,  4.9195e-01, -6.1521e-01, -3.4093e-03, -9.9932e-01,\n",
      "          -7.4659e-02,  7.7924e-01,  1.2312e+00, -9.9999e-01, -9.4464e-01,\n",
      "           9.2430e-03, -7.2542e-01,  1.4464e-01, -1.5763e+00,  6.4457e-07,\n",
      "          -6.5371e-02,  1.3408e-01, -1.6891e-01,  6.3182e-01, -6.4400e-01,\n",
      "           6.9037e-01,  9.7961e-01, -7.4999e-01,  7.4534e-01,  4.8993e-02,\n",
      "          -9.7161e-01, -4.0189e-04, -9.8653e-01,  5.6913e-02,  5.7551e-02,\n",
      "          -8.1948e-01]],\n",
      "\n",
      "        [[-2.9382e-03, -5.5686e-02, -4.3147e-01,  6.9318e-03,  6.8859e-02,\n",
      "           1.2399e+00, -1.0214e-01, -1.0003e+00,  3.5230e-02,  5.2395e-02,\n",
      "          -5.9108e-01, -9.9111e-02, -6.6015e-01, -2.5551e-01, -8.9752e-03,\n",
      "          -1.0811e+00,  9.9975e-01,  2.3433e-03,  4.3865e-03,  1.4183e+00,\n",
      "           3.8933e-01,  6.3584e-02, -2.4178e-01,  9.9635e-01, -9.5833e-02,\n",
      "           9.9504e-01, -5.4234e-02,  1.8689e+00, -1.5517e-01,  4.3591e-05,\n",
      "           6.8270e-04, -9.8880e-01,  1.1300e-01,  9.1762e-01,  9.9983e-01,\n",
      "          -9.5402e-01,  8.6147e-01, -1.3670e-04, -1.7521e+00,  9.1591e-03,\n",
      "          -9.1422e-01,  6.9133e-02,  9.0813e-01, -9.8678e-01,  9.9965e-01,\n",
      "          -3.3070e-01, -9.7071e-01, -1.2860e-02,  9.9937e-01, -4.8606e-01,\n",
      "          -1.1748e-01,  7.5383e-01,  2.0639e-05, -1.0774e+00, -7.7241e-01,\n",
      "          -1.0002e+00,  9.5777e-01, -2.6063e-03,  9.8734e-01, -1.8404e-04,\n",
      "           1.2533e+00, -2.8415e-04, -9.9997e-01,  1.7528e+00, -8.8959e-01,\n",
      "          -1.5366e+00,  5.0449e-04,  9.7140e-01, -1.0084e+00, -9.6954e-01,\n",
      "           5.1055e-01,  9.9991e-01,  9.9902e-01,  7.6950e-01, -2.5352e-01,\n",
      "           1.0101e+00,  9.9998e-01, -1.1076e+00, -5.6083e-04,  9.7404e-01,\n",
      "          -4.4950e-01, -9.2671e-01,  9.9997e-01, -9.6000e-01,  9.8901e-01,\n",
      "           3.3387e-01, -2.6105e-01,  3.6743e-01, -5.8809e-01,  1.2401e-03,\n",
      "          -9.9694e-01,  8.6105e-03,  2.8749e-01, -4.7713e-01, -1.5833e+00,\n",
      "           2.7928e-01,  8.5508e-01, -6.7229e-01, -2.1096e-02, -2.2181e-01,\n",
      "           6.5525e-01,  1.9968e+00, -1.3860e-02,  3.9908e-01,  1.0002e+00,\n",
      "          -1.3106e+00,  6.3548e-01, -1.1746e-01,  1.0027e+00, -5.9703e-02,\n",
      "           1.8907e+00,  9.8178e-01,  8.9727e-01,  2.7489e-01,  9.9772e-01,\n",
      "          -9.9799e-01, -5.4302e-02,  1.0000e+00,  1.7610e-02,  1.2708e+00,\n",
      "          -9.9635e-01,  1.8482e+00,  1.8366e+00,  3.0534e-01, -1.2957e+00,\n",
      "          -8.9049e-01, -9.7753e-01,  1.2702e-01,  6.6676e-01,  9.8848e-01,\n",
      "           8.5114e-01,  2.7807e-01, -3.7595e-05,  5.3680e-02,  2.4225e-02,\n",
      "           3.7097e-01, -1.9840e-01,  3.5959e-01,  4.7793e-01, -9.0886e-02,\n",
      "           9.1430e-01, -5.7197e-04,  1.0204e+00,  6.9828e-01, -1.1191e-01,\n",
      "          -7.1014e-04, -3.5621e-01,  5.3362e-01, -4.2099e-01, -7.1684e-02,\n",
      "          -1.0000e+00, -9.9689e-01,  1.3490e-05,  7.0723e-01, -3.8521e-01,\n",
      "          -3.6876e-02, -9.6092e-01,  6.9774e-01, -9.8669e-01,  1.9663e+00,\n",
      "           1.2888e+00, -9.4993e-01,  1.0000e+00,  7.9864e-02,  2.9246e+00,\n",
      "           2.7589e-01,  3.4088e-04,  9.9997e-01,  2.1567e-01, -1.3288e+00,\n",
      "           1.0122e+00, -9.9672e-01,  1.8224e-01,  2.8728e-02, -1.9440e+00,\n",
      "           3.3209e-01, -1.5071e-02,  1.8852e+00, -9.9996e-01, -8.6959e-04,\n",
      "          -8.2848e-02,  9.1118e-01,  9.3877e-01, -9.9538e-01, -6.2970e-01,\n",
      "           1.1471e+00, -6.9263e-04,  3.1403e-01, -2.3469e-01,  9.5401e-01,\n",
      "          -1.0875e-02,  5.6729e-01, -9.3766e-01, -6.4425e-02, -9.9878e-01,\n",
      "           9.9915e-01, -3.1969e-03, -2.1354e-03, -3.5942e+00,  3.7340e-03,\n",
      "           8.2749e-02,  4.1593e-06, -9.9569e-01, -3.0140e-02,  9.6508e-01,\n",
      "           9.9997e-01,  8.5457e-03,  4.5040e-01,  1.1388e+00, -9.1250e-01,\n",
      "          -9.9997e-01, -7.0385e-03, -9.5660e-01, -3.9829e-03, -9.9993e-01,\n",
      "           1.6380e-02, -9.9527e-01, -9.0982e-01, -8.6558e-01, -1.0563e-02,\n",
      "           9.9995e-01,  4.8242e-02, -9.8228e-01, -2.2135e-02,  8.0302e-01,\n",
      "          -1.1554e-02,  9.9284e-01, -1.0424e+00, -9.9835e-01,  9.5605e-01,\n",
      "           1.3916e-01,  6.9929e-01,  1.7586e+00,  1.6757e-03,  1.4691e-02,\n",
      "           1.0014e+00, -9.7446e-01,  1.5437e+00, -1.6243e-01, -2.0796e+00,\n",
      "          -3.9961e-01,  1.9827e-01,  5.0354e-01, -8.3835e-01, -2.6825e-01,\n",
      "          -9.8537e-01, -9.9765e-01,  1.1308e+00,  1.0454e+00, -9.9412e-01,\n",
      "          -3.0234e-05, -7.0091e-02,  1.6619e-02, -1.0022e+00, -5.4595e-01,\n",
      "          -1.2121e+00]]], device='cuda:0')))\n",
      "(PackedSequence(data=tensor([[-1.4883e-02, -3.6345e-02,  9.4824e-06,  ..., -7.4751e-01,\n",
      "         -4.9711e-01, -8.1787e-01],\n",
      "        [-2.0132e-06,  3.1252e-01,  7.3382e-04,  ..., -2.5950e-03,\n",
      "         -5.1696e-01, -2.6363e-03],\n",
      "        [ 6.0661e-01,  3.4886e-04, -2.9972e-01,  ..., -5.8422e-01,\n",
      "         -3.0078e-01, -8.5479e-02],\n",
      "        [ 8.6339e-01, -8.2297e-05,  1.2962e-02,  ..., -5.1039e-01,\n",
      "          7.5236e-01,  1.4459e-01]], device='cuda:0'), batch_sizes=tensor([1, 1, 1, 1])), (tensor([[[ 8.6339e-01, -8.2297e-05,  1.2962e-02, -5.2747e-03,  2.8766e-02,\n",
      "          -2.3255e-03, -1.6458e-03, -5.3015e-01,  1.2371e-04, -1.4159e-02,\n",
      "           1.2232e-01, -1.5929e-01,  7.7751e-04,  7.3277e-01,  5.7955e-01,\n",
      "          -7.2455e-01,  7.5977e-01,  6.6417e-01,  8.3169e-01,  1.6522e-02,\n",
      "           1.2402e-01,  7.5080e-01, -8.6025e-01, -7.2322e-01,  2.6157e-01,\n",
      "          -1.4576e-02,  5.4367e-01,  2.2868e-02, -6.2168e-05,  2.3959e-04,\n",
      "           1.2088e-01,  2.5572e-01,  2.4748e-01, -1.6540e-01, -1.0256e-03,\n",
      "           7.6035e-01, -2.7907e-03, -1.0987e-03,  1.1586e-04, -4.7525e-01,\n",
      "           7.2113e-01,  4.3589e-01, -5.4250e-03, -3.8552e-01,  6.8723e-01,\n",
      "           4.7789e-03,  1.6430e-03, -5.9408e-01,  4.3598e-02, -3.5659e-02,\n",
      "          -2.9209e-02, -2.5596e-04, -7.2837e-03, -1.2439e-03,  1.3635e-03,\n",
      "           5.1719e-01,  7.0959e-01,  2.9773e-02, -1.1944e-02, -6.8658e-01,\n",
      "          -7.5715e-02,  7.1876e-02, -2.1645e-01,  2.0668e-01,  8.2477e-04,\n",
      "           5.0946e-05,  1.9576e-01, -3.1146e-03, -5.4517e-02,  4.7817e-01,\n",
      "           6.5883e-01, -7.7328e-05,  6.1124e-04,  1.6086e-02,  2.5710e-02,\n",
      "          -3.7643e-04, -1.3366e-03, -5.7513e-01, -7.5587e-01, -1.0639e-04,\n",
      "          -3.7005e-03, -1.9372e-01, -1.3628e-03,  1.6621e-03, -2.1366e-01,\n",
      "           1.9469e-03, -4.3274e-01,  8.8760e-03,  1.9572e-05,  5.0142e-04,\n",
      "          -3.0277e-01, -6.9515e-01, -8.4036e-03,  5.1134e-02,  7.3247e-01,\n",
      "          -4.4552e-01, -9.0639e-04, -4.8697e-01,  7.5183e-01,  6.5118e-01,\n",
      "           2.1081e-03, -6.6157e-01, -2.8927e-03,  7.1952e-01, -4.3556e-02,\n",
      "           6.7953e-06, -5.0279e-01, -4.3033e-04,  9.2809e-03, -1.2676e-01,\n",
      "          -6.5865e-01,  6.1853e-02, -2.6872e-03,  5.1193e-01,  2.2517e-01,\n",
      "          -4.4642e-01,  4.1991e-01,  3.2507e-01,  4.4524e-04, -4.5593e-04,\n",
      "           3.4829e-01, -5.1822e-01, -7.8166e-01,  4.0129e-06, -4.8517e-01,\n",
      "           5.6558e-01, -1.2862e-01, -8.3411e-03,  8.6777e-02,  5.6654e-01,\n",
      "           6.3654e-01, -8.9564e-02,  2.7719e-03, -7.5288e-01, -7.1801e-04,\n",
      "          -6.9620e-01,  9.2303e-02, -7.4568e-02, -1.5605e-01, -2.3491e-01,\n",
      "          -4.1914e-01,  4.8247e-01,  2.5158e-02,  8.8502e-03,  3.4214e-01,\n",
      "           5.8478e-01,  7.3568e-01, -2.7727e-01, -1.5276e-02,  2.5045e-02,\n",
      "          -1.9640e-01, -2.2582e-05, -3.9137e-04,  4.0625e-01,  5.0928e-01,\n",
      "          -7.4868e-01,  1.5125e-03,  3.7695e-02,  6.0506e-01,  7.1493e-01,\n",
      "          -1.4122e-03,  5.9429e-02, -1.2176e-03, -7.9118e-02,  7.0656e-01,\n",
      "          -2.1274e-03, -5.2224e-03,  3.8339e-01, -9.5472e-03,  3.5465e-02,\n",
      "           4.8192e-01,  1.4899e-05, -4.2292e-01, -5.1604e-04,  3.3542e-03,\n",
      "          -4.8556e-01, -2.3650e-01,  2.0344e-06,  3.5478e-02, -2.1249e-03,\n",
      "          -1.0888e-04,  1.3170e-05,  1.4047e-01,  3.5602e-01, -2.8909e-03,\n",
      "          -7.5526e-01, -6.3569e-01, -7.7593e-03,  4.8422e-01,  1.8401e-05,\n",
      "           6.3878e-01,  1.3028e-01, -3.2679e-03, -7.5864e-01,  3.7532e-01,\n",
      "           5.4118e-01, -4.8726e-01, -7.0412e-01,  5.5524e-01,  6.6840e-01,\n",
      "          -7.4586e-01,  4.7405e-01, -5.3556e-01, -1.3380e-01, -5.7113e-01,\n",
      "          -2.0959e-01,  7.0832e-04, -3.9652e-01, -8.4839e-02, -3.6386e-01,\n",
      "          -3.5214e-03, -9.7685e-02, -4.3129e-03, -6.6450e-03,  4.8305e-01,\n",
      "          -7.3130e-01,  9.1449e-02,  1.4568e-02, -5.7429e-02, -2.7527e-03,\n",
      "           2.0810e-01,  9.3812e-01,  1.5117e-02,  2.3748e-03,  3.8290e-01,\n",
      "          -7.3188e-01,  1.7807e-02, -3.5782e-02, -2.1853e-04, -2.0899e-03,\n",
      "          -5.6661e-02,  6.5145e-01,  2.5267e-02, -1.2240e-01, -1.1907e-01,\n",
      "           9.0946e-03, -5.9191e-01,  4.2908e-02, -5.1800e-02,  2.1770e-07,\n",
      "          -5.0206e-02,  1.3222e-01, -1.6414e-01,  5.4419e-01, -1.3324e-02,\n",
      "           9.7500e-04,  7.5174e-01, -6.3023e-01,  6.1948e-01,  4.8303e-02,\n",
      "          -2.4022e-01, -6.5483e-06, -7.5583e-01,  1.7561e-02,  5.7352e-02,\n",
      "          -1.3407e-01]],\n",
      "\n",
      "        [[-2.6729e-03, -5.5149e-02, -1.5596e-05,  6.9317e-03,  6.8732e-02,\n",
      "           2.1285e-02, -1.0179e-01, -7.5503e-01,  3.5214e-02,  5.2348e-02,\n",
      "          -5.2441e-01, -9.0491e-02, -5.5588e-01, -2.4109e-01, -1.0079e-03,\n",
      "          -1.0050e-04,  7.6056e-01,  2.1587e-03,  4.3845e-03,  8.6399e-01,\n",
      "           2.4626e-02,  6.0883e-02, -1.0603e-04,  7.5426e-01, -6.9387e-03,\n",
      "           5.1572e-06, -5.4135e-02,  2.2505e-01, -1.4248e-01,  4.3586e-05,\n",
      "           4.1361e-06, -7.2111e-01,  5.5184e-04,  3.4250e-05,  4.7567e-01,\n",
      "          -7.0861e-01,  5.0313e-01, -7.8437e-05, -6.7334e-01,  9.1585e-03,\n",
      "          -5.7172e-03,  6.9001e-02,  1.5086e-05, -7.5587e-01,  1.2726e-01,\n",
      "          -3.1915e-01, -6.3265e-01, -1.2842e-02,  7.5294e-01, -4.5109e-01,\n",
      "          -1.1692e-01,  5.0272e-01,  1.9310e-05, -7.9006e-01, -6.4822e-01,\n",
      "          -5.7604e-05,  7.4323e-01, -1.4423e-06,  7.5432e-01, -2.6136e-06,\n",
      "           8.4920e-01, -6.5323e-05, -6.3045e-02,  9.9838e-03, -7.0228e-01,\n",
      "          -6.5868e-02,  3.5247e-07,  7.4629e-01, -1.4853e-01, -6.9728e-01,\n",
      "           4.7033e-01,  1.1812e-01,  4.6931e-04,  6.4620e-01, -1.6374e-01,\n",
      "           7.6575e-01,  2.9713e-01, -9.7344e-02, -7.6758e-06,  1.8469e-01,\n",
      "          -4.2140e-01, -7.2905e-01,  7.6055e-01, -4.8074e-04,  5.6438e-01,\n",
      "           6.6935e-04, -2.4369e-01,  9.6801e-07, -3.9924e-01,  4.1170e-04,\n",
      "          -6.5299e-04,  8.6103e-03,  2.5896e-01, -4.0611e-01, -9.1255e-01,\n",
      "           2.7179e-01,  6.9345e-01, -4.5339e-02, -1.3154e-06, -7.4763e-06,\n",
      "           2.2638e-01,  3.9890e-01, -1.2630e-02,  3.7882e-01,  6.9748e-01,\n",
      "          -8.5958e-01,  5.6182e-01, -1.1241e-01,  5.0896e-02, -4.3435e-06,\n",
      "           9.5182e-01,  6.7869e-01,  6.9927e-01,  6.1698e-07,  7.5928e-01,\n",
      "          -1.0145e-01, -1.8823e-03,  3.2212e-02,  1.6416e-02,  7.0759e-01,\n",
      "          -7.5857e-01,  9.5129e-01,  9.5044e-01,  1.1645e-01, -1.3153e-04,\n",
      "          -7.1005e-01, -1.5254e-02,  1.2634e-01,  5.6843e-01,  7.5567e-01,\n",
      "           6.7876e-01,  3.2517e-03, -8.9337e-11,  4.4688e-02,  2.4206e-02,\n",
      "           3.5420e-01, -2.3466e-03,  1.0441e-01,  1.4563e-01, -5.9812e-05,\n",
      "           8.3256e-02, -3.7557e-04,  7.6475e-01,  6.0326e-01, -1.5958e-05,\n",
      "          -7.0635e-04, -3.4083e-02,  9.7244e-04, -3.9776e-01, -7.0888e-02,\n",
      "          -2.7629e-03, -5.3350e-06,  2.0502e-09,  6.0818e-01, -3.3481e-01,\n",
      "          -3.6858e-02, -7.4437e-01,  5.9294e-01, -5.2043e-01,  7.6937e-01,\n",
      "           4.7259e-04, -6.4208e-01,  5.3617e-01,  7.9249e-02,  7.9878e-01,\n",
      "           1.9696e-04,  1.9414e-07,  6.9107e-02,  2.1130e-01, -1.3631e-06,\n",
      "           7.6664e-01, -1.5544e-02,  7.7580e-02,  8.6669e-04, -7.8866e-01,\n",
      "           3.1984e-01, -1.4282e-02,  1.0385e-05, -5.7473e-01, -7.9796e-04,\n",
      "          -7.8098e-02,  6.9534e-01,  7.3461e-01, -1.7592e-02, -2.8449e-04,\n",
      "           2.5881e-02, -6.6700e-04,  1.7461e-03, -2.3047e-01,  1.2834e-01,\n",
      "          -1.2542e-04,  5.1269e-01, -6.5369e-01, -3.8284e-02, -5.6008e-01,\n",
      "           7.6120e-01, -3.0689e-04, -1.7449e-05, -9.7548e-01,  9.3303e-08,\n",
      "           8.2560e-02,  1.5392e-06, -3.5932e-03, -2.1965e-02,  7.3075e-01,\n",
      "           7.6157e-01,  5.7652e-05,  4.2221e-01,  1.0073e-03, -3.5016e-01,\n",
      "          -7.1951e-02, -5.6899e-05, -9.1491e-06, -9.5940e-06, -2.6802e-02,\n",
      "           1.6377e-02, -6.2465e-01, -6.9331e-01, -1.4498e-03, -5.0387e-09,\n",
      "           6.6856e-01,  4.7772e-02, -1.7363e-03, -7.3625e-03,  1.1120e-02,\n",
      "          -1.1554e-02,  2.6459e-01, -5.6541e-04, -7.6088e-01,  1.8780e-01,\n",
      "           6.1821e-04,  5.1538e-01,  4.8698e-01,  2.5475e-07,  1.4688e-02,\n",
      "           3.5894e-04, -5.1799e-01,  8.1600e-01, -7.7900e-02, -9.6922e-01,\n",
      "          -3.6928e-01,  1.6435e-01,  4.4502e-01, -6.8466e-01, -2.5676e-01,\n",
      "          -6.8643e-04, -5.3311e-01,  1.3863e-04,  7.7983e-01, -7.3619e-01,\n",
      "          -3.0234e-05, -6.8647e-02,  4.3560e-07, -7.4751e-01, -4.9711e-01,\n",
      "          -8.1787e-01]]], device='cuda:0'), tensor([[[ 1.3077e+00, -8.5816e-04,  7.2467e-01, -5.6756e-03,  2.8796e-02,\n",
      "          -8.2772e-01, -6.2185e-03, -7.7540e-01,  5.8974e-02, -9.7896e-01,\n",
      "           6.4522e-01, -1.6089e-01,  1.2457e-02,  9.7861e-01,  1.0022e+00,\n",
      "          -9.9466e-01,  9.9845e-01,  8.5578e-01,  1.1936e+00,  3.4435e-01,\n",
      "           9.8913e-01,  9.8294e-01, -1.3141e+00, -9.1616e-01,  1.0001e+00,\n",
      "          -9.9967e-01,  6.3369e-01,  2.8096e-02, -6.4137e-01,  9.5901e-01,\n",
      "           1.3955e-01,  2.8687e-01,  5.1042e-01, -1.8869e-01, -1.8641e-01,\n",
      "           9.9815e-01, -2.9415e-03, -9.9761e-01,  9.8204e-01, -5.2915e-01,\n",
      "           9.9836e-01,  5.0804e-01, -1.0562e-01, -4.2144e-01,  9.9273e-01,\n",
      "           1.4263e-02,  1.7609e+00, -6.9739e-01,  4.3628e-02, -5.6893e-01,\n",
      "          -9.9458e-01, -1.9883e-03, -3.7244e-01, -1.9699e+00,  9.9774e-01,\n",
      "           9.2166e-01,  9.5954e-01,  3.0395e-02, -1.2341e-02, -1.3924e+00,\n",
      "          -3.1103e-01,  6.7596e-01, -3.9327e-01,  7.4080e-01,  8.7779e-04,\n",
      "           2.8023e-01,  5.3673e-01, -3.1147e-03, -7.0205e-02,  1.0689e+00,\n",
      "           9.7436e-01, -8.2797e-01,  1.0410e+00,  1.6115e-02,  5.2856e-01,\n",
      "          -5.1935e-04, -1.3980e-03, -9.9902e-01, -1.0049e+00, -7.9868e-01,\n",
      "          -4.8191e-03, -5.2721e-01, -4.6306e-02,  9.8973e-01, -9.4633e-01,\n",
      "           1.9473e-03, -9.8936e-01,  7.5396e-01,  8.7714e-03,  1.2563e+00,\n",
      "          -8.1858e-01, -9.9238e-01, -1.0373e-02,  5.1205e-02,  9.3405e-01,\n",
      "          -4.8966e-01, -9.0639e-04, -7.3622e-01,  1.0005e+00,  8.5483e-01,\n",
      "           1.5433e-02, -1.0000e+00, -1.7090e+00,  9.9115e-01, -5.9547e-02,\n",
      "           1.7014e-04, -9.5644e-01, -1.1099e-01,  1.3735e-01, -1.2758e-01,\n",
      "          -7.9049e-01,  6.2534e-02, -2.6879e-03,  1.0792e+00,  9.9976e-01,\n",
      "          -1.0025e+00,  9.9967e-01,  3.3733e-01,  3.9535e-03, -4.5706e-04,\n",
      "           4.1703e-01, -1.1704e+00, -1.1263e+00,  7.4939e-01, -9.9832e-01,\n",
      "           6.9637e-01, -1.3404e-01, -6.7002e-01,  8.7211e-02,  6.4252e-01,\n",
      "           7.7600e-01, -1.9937e-01,  3.3260e-01, -9.8266e-01, -9.7282e-01,\n",
      "          -8.6070e-01,  9.9998e-01, -7.8395e-02, -1.5739e-01, -2.3938e-01,\n",
      "          -1.8386e+00,  9.7199e-01,  3.1311e-02,  3.9178e-01,  1.3224e+00,\n",
      "           6.7466e-01,  1.0150e+00, -2.8502e-01, -1.8274e-01,  2.5453e-02,\n",
      "          -9.9216e-01, -5.4604e-03, -1.1951e-03,  5.5024e-01,  5.9544e-01,\n",
      "          -1.0215e+00,  1.5177e-03,  9.8357e-01,  9.9956e-01,  9.0993e-01,\n",
      "          -1.1770e+00,  1.0111e+00, -8.8242e-01, -1.0643e-01,  8.9291e-01,\n",
      "          -2.1643e-03, -9.7585e-01,  1.9137e+00, -1.0322e-02,  2.8225e-01,\n",
      "           2.0850e+00,  3.7996e-02, -4.5939e-01, -1.6606e+00,  5.9626e-01,\n",
      "          -8.1418e-01, -3.4430e-01,  4.9787e-03,  9.6140e-01, -9.9883e-01,\n",
      "          -7.0062e-01,  7.5295e-01,  6.2516e-01,  3.7737e-01, -2.9663e-03,\n",
      "          -9.8510e-01, -9.9696e-01, -1.1544e-01,  9.1548e-01,  1.1440e-02,\n",
      "           8.2725e-01,  1.3181e-01, -1.6767e-01, -9.9580e-01,  4.1002e-01,\n",
      "           1.1158e+00, -5.5123e-01, -9.9983e-01,  6.2694e-01,  8.3802e-01,\n",
      "          -9.6461e-01,  9.5754e-01, -7.1132e-01, -5.1539e-01, -9.4095e-01,\n",
      "          -1.2008e+00,  9.3167e-01, -4.2003e-01, -9.2377e-01, -3.8169e-01,\n",
      "          -8.5565e-02, -9.6327e-01, -9.9796e-01, -4.8392e-02,  5.2696e-01,\n",
      "          -1.0732e+00,  1.3617e+00,  2.7111e-01, -1.1561e-01, -9.9096e-01,\n",
      "           1.9359e+00,  1.7296e+00,  1.5469e-02,  1.6332e-01,  4.0383e-01,\n",
      "          -1.0025e+00,  4.9195e-01, -6.1521e-01, -3.4093e-03, -9.9932e-01,\n",
      "          -7.4659e-02,  7.7924e-01,  1.2312e+00, -9.9999e-01, -9.4464e-01,\n",
      "           9.2430e-03, -7.2542e-01,  1.4464e-01, -1.5763e+00,  6.4457e-07,\n",
      "          -6.5371e-02,  1.3408e-01, -1.6891e-01,  6.3182e-01, -6.4400e-01,\n",
      "           6.9037e-01,  9.7961e-01, -7.4999e-01,  7.4534e-01,  4.8993e-02,\n",
      "          -9.7161e-01, -4.0189e-04, -9.8653e-01,  5.6913e-02,  5.7551e-02,\n",
      "          -8.1948e-01]],\n",
      "\n",
      "        [[-2.9382e-03, -5.5686e-02, -4.3147e-01,  6.9318e-03,  6.8859e-02,\n",
      "           1.2399e+00, -1.0214e-01, -1.0003e+00,  3.5230e-02,  5.2395e-02,\n",
      "          -5.9108e-01, -9.9111e-02, -6.6015e-01, -2.5551e-01, -8.9752e-03,\n",
      "          -1.0811e+00,  9.9975e-01,  2.3433e-03,  4.3865e-03,  1.4183e+00,\n",
      "           3.8933e-01,  6.3584e-02, -2.4178e-01,  9.9635e-01, -9.5833e-02,\n",
      "           9.9504e-01, -5.4234e-02,  1.8689e+00, -1.5517e-01,  4.3591e-05,\n",
      "           6.8270e-04, -9.8880e-01,  1.1300e-01,  9.1762e-01,  9.9983e-01,\n",
      "          -9.5402e-01,  8.6147e-01, -1.3670e-04, -1.7521e+00,  9.1591e-03,\n",
      "          -9.1422e-01,  6.9133e-02,  9.0813e-01, -9.8678e-01,  9.9965e-01,\n",
      "          -3.3070e-01, -9.7071e-01, -1.2860e-02,  9.9937e-01, -4.8606e-01,\n",
      "          -1.1748e-01,  7.5383e-01,  2.0639e-05, -1.0774e+00, -7.7241e-01,\n",
      "          -1.0002e+00,  9.5777e-01, -2.6063e-03,  9.8734e-01, -1.8404e-04,\n",
      "           1.2533e+00, -2.8415e-04, -9.9997e-01,  1.7528e+00, -8.8959e-01,\n",
      "          -1.5366e+00,  5.0449e-04,  9.7140e-01, -1.0084e+00, -9.6954e-01,\n",
      "           5.1055e-01,  9.9991e-01,  9.9902e-01,  7.6950e-01, -2.5352e-01,\n",
      "           1.0101e+00,  9.9998e-01, -1.1076e+00, -5.6083e-04,  9.7404e-01,\n",
      "          -4.4950e-01, -9.2671e-01,  9.9997e-01, -9.6000e-01,  9.8901e-01,\n",
      "           3.3387e-01, -2.6105e-01,  3.6743e-01, -5.8809e-01,  1.2401e-03,\n",
      "          -9.9694e-01,  8.6105e-03,  2.8749e-01, -4.7713e-01, -1.5833e+00,\n",
      "           2.7928e-01,  8.5508e-01, -6.7229e-01, -2.1096e-02, -2.2181e-01,\n",
      "           6.5525e-01,  1.9968e+00, -1.3860e-02,  3.9908e-01,  1.0002e+00,\n",
      "          -1.3106e+00,  6.3548e-01, -1.1746e-01,  1.0027e+00, -5.9703e-02,\n",
      "           1.8907e+00,  9.8178e-01,  8.9727e-01,  2.7489e-01,  9.9772e-01,\n",
      "          -9.9799e-01, -5.4302e-02,  1.0000e+00,  1.7610e-02,  1.2708e+00,\n",
      "          -9.9635e-01,  1.8482e+00,  1.8366e+00,  3.0534e-01, -1.2957e+00,\n",
      "          -8.9049e-01, -9.7753e-01,  1.2702e-01,  6.6676e-01,  9.8848e-01,\n",
      "           8.5114e-01,  2.7807e-01, -3.7595e-05,  5.3680e-02,  2.4225e-02,\n",
      "           3.7097e-01, -1.9840e-01,  3.5959e-01,  4.7793e-01, -9.0886e-02,\n",
      "           9.1430e-01, -5.7197e-04,  1.0204e+00,  6.9828e-01, -1.1191e-01,\n",
      "          -7.1014e-04, -3.5621e-01,  5.3362e-01, -4.2099e-01, -7.1684e-02,\n",
      "          -1.0000e+00, -9.9689e-01,  1.3490e-05,  7.0723e-01, -3.8521e-01,\n",
      "          -3.6876e-02, -9.6092e-01,  6.9774e-01, -9.8669e-01,  1.9663e+00,\n",
      "           1.2888e+00, -9.4993e-01,  1.0000e+00,  7.9864e-02,  2.9246e+00,\n",
      "           2.7589e-01,  3.4088e-04,  9.9997e-01,  2.1567e-01, -1.3288e+00,\n",
      "           1.0122e+00, -9.9672e-01,  1.8224e-01,  2.8728e-02, -1.9440e+00,\n",
      "           3.3209e-01, -1.5071e-02,  1.8852e+00, -9.9996e-01, -8.6959e-04,\n",
      "          -8.2848e-02,  9.1118e-01,  9.3877e-01, -9.9538e-01, -6.2970e-01,\n",
      "           1.1471e+00, -6.9263e-04,  3.1403e-01, -2.3469e-01,  9.5401e-01,\n",
      "          -1.0875e-02,  5.6729e-01, -9.3766e-01, -6.4425e-02, -9.9878e-01,\n",
      "           9.9915e-01, -3.1969e-03, -2.1354e-03, -3.5942e+00,  3.7340e-03,\n",
      "           8.2749e-02,  4.1593e-06, -9.9569e-01, -3.0140e-02,  9.6508e-01,\n",
      "           9.9997e-01,  8.5457e-03,  4.5040e-01,  1.1388e+00, -9.1250e-01,\n",
      "          -9.9997e-01, -7.0385e-03, -9.5660e-01, -3.9829e-03, -9.9993e-01,\n",
      "           1.6380e-02, -9.9527e-01, -9.0982e-01, -8.6558e-01, -1.0563e-02,\n",
      "           9.9995e-01,  4.8242e-02, -9.8228e-01, -2.2135e-02,  8.0302e-01,\n",
      "          -1.1554e-02,  9.9284e-01, -1.0424e+00, -9.9835e-01,  9.5605e-01,\n",
      "           1.3916e-01,  6.9929e-01,  1.7586e+00,  1.6757e-03,  1.4691e-02,\n",
      "           1.0014e+00, -9.7446e-01,  1.5437e+00, -1.6243e-01, -2.0796e+00,\n",
      "          -3.9961e-01,  1.9827e-01,  5.0354e-01, -8.3835e-01, -2.6825e-01,\n",
      "          -9.8537e-01, -9.9765e-01,  1.1308e+00,  1.0454e+00, -9.9412e-01,\n",
      "          -3.0234e-05, -7.0091e-02,  1.6619e-02, -1.0022e+00, -5.4595e-01,\n",
      "          -1.2121e+00]]], device='cuda:0')))\n",
      "2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PackedSequence' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-7941c3afae29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpos_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/glvis/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentences, mini_batch_size)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                 \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_labels_and_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_tags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/glvis/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py\u001b[0m in \u001b[0;36mforward_labels_and_loss\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_labels_and_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obtain_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/glvis/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0msentence_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/glvis/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                 raise RuntimeError(\n",
      "\u001b[0;32m<ipython-input-167-791fb3ee3a7f>\u001b[0m in \u001b[0;36mlstm_hook\u001b[0;34m(m, i, o)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlstm_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlstm_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PackedSequence' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "pos_tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_output[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

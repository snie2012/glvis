{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract hidden representations from flair's pretrained Chunking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from flair.models import TextClassifier\n",
    "\n",
    "from flair.data import TaggedCorpus\n",
    "from flair.data_fetcher import  NLPTaskDataFetcher, NLPTask\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "\n",
    "db = client['glvis_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_col = db['flair_chunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-20 12:40:07,155 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.2/NP-conll2000--h256-l1-b32-%2Bnews-forward%2Bnews-backward--v0.2/en-chunk-conll2000-v0.2.pt not found in cache, downloading to /tmp/tmp0l09i918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247739475/247739475 [00:59<00:00, 4161783.21B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-20 12:41:07,279 copying /tmp/tmp0l09i918 to cache at /home/snie/.flair/models/en-chunk-conll2000-v0.2.pt\n",
      "2019-03-20 12:41:07,430 removing temp file /tmp/tmp0l09i918\n",
      "2019-03-20 12:41:07,431 loading file /home/snie/.flair/models/en-chunk-conll2000-v0.2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunk_tagger = SequenceTagger.load('chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-20 13:13:10,586 Reading data from /home/snie/.flair/datasets/conll_2000\n",
      "2019-03-20 13:13:10,586 Train: /home/snie/.flair/datasets/conll_2000/train.txt\n",
      "2019-03-20 13:13:10,586 Dev: None\n",
      "2019-03-20 13:13:10,587 Test: /home/snie/.flair/datasets/conll_2000/test.txt\n"
     ]
    }
   ],
   "source": [
    "corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_type = 'np'\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_set = set()\n",
    "for entry in db_col.find():\n",
    "    tag_set.add(entry['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VP',\n",
       " 'CONJP',\n",
       " 'PRT',\n",
       " 'ADVP',\n",
       " 'UCP',\n",
       " 'SBAR',\n",
       " 'NP',\n",
       " 'LST',\n",
       " 'PP',\n",
       " 'INTJ',\n",
       " 'ADJP']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tag_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10948\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus.get_all_sentences()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_col.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(corpus.get_all_sentences()):\n",
    "    print(f'Start sentence {i}')\n",
    "    \n",
    "    # Define hook to get intermediate values\n",
    "    hidden_states = torch.zeros(len(sentence), 1, 512)\n",
    "    def hook(m, i):\n",
    "        hidden_states.copy_(i[0].data)\n",
    "                                \n",
    "    h = chunk_tagger.linear.register_forward_pre_hook(hook)\n",
    "                                \n",
    "    chunk_tagger.predict(sentence)\n",
    "                                \n",
    "    spans = sentence.get_spans(tag_type)\n",
    "                                \n",
    "    # Informaction to store: the named entities, their predicted labels, probabilities and hidden states\n",
    "    # If there are multiple words for one entity, take the average value of hidden states\n",
    "    # and record the number of words in the entity\n",
    "    \n",
    "    for span in spans:\n",
    "        entry = {}\n",
    "        entry['text'] = span.text\n",
    "        entry['tag'] = span.tag\n",
    "        entry['score'] = span.score\n",
    "        entry['token_num'] = len(span.tokens)\n",
    "        \n",
    "        idx = [token.idx-1 for token in span.tokens]\n",
    "        entry['linear_layer_state'] = hidden_states[idx, :, :].mean(dim=0).squeeze().tolist()\n",
    "        \n",
    "        db_col.insert_one(entry)\n",
    "    \n",
    "    h.remove()\n",
    "    \n",
    "    print(f'Finish sentence {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train chunk model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from flair.data import TaggedCorpus\n",
    "from flair.data_fetcher import  NLPTaskDataFetcher, NLPTask\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-17 23:28:59,562 Reading data from /home/snie/.flair/datasets/conll_2000\n",
      "2019-03-17 23:28:59,562 Train: /home/snie/.flair/datasets/conll_2000/train.txt\n",
      "2019-03-17 23:28:59,563 Dev: None\n",
      "2019-03-17 23:28:59,563 Test: /home/snie/.flair/datasets/conll_2000/test.txt\n"
     ]
    }
   ],
   "source": [
    "# 1. get the corpus\n",
    "corpus: TaggedCorpus = NLPTaskDataFetcher.load_corpus(NLPTask.CONLL_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'np'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize embeddings\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "    WordEmbeddings('extvec'),\n",
    "    FlairEmbeddings('news-forward'),\n",
    "    FlairEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-17 23:29:17,911 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:29:17,911 Evaluation method: MICRO_F1_SCORE\n",
      "2019-03-17 23:29:17,945 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:29:18,634 epoch 1 - iter 0/252 - loss 115.07946777\n",
      "2019-03-17 23:29:35,887 epoch 1 - iter 25/252 - loss 59.17966197\n",
      "2019-03-17 23:29:53,432 epoch 1 - iter 50/252 - loss 43.37734552\n",
      "2019-03-17 23:30:10,464 epoch 1 - iter 75/252 - loss 35.03818862\n",
      "2019-03-17 23:30:28,253 epoch 1 - iter 100/252 - loss 29.57320743\n",
      "2019-03-17 23:30:45,902 epoch 1 - iter 125/252 - loss 25.79143439\n",
      "2019-03-17 23:31:03,494 epoch 1 - iter 150/252 - loss 23.05912499\n",
      "2019-03-17 23:31:21,495 epoch 1 - iter 175/252 - loss 20.92689903\n",
      "2019-03-17 23:31:39,219 epoch 1 - iter 200/252 - loss 19.28091297\n",
      "2019-03-17 23:31:56,303 epoch 1 - iter 225/252 - loss 17.88000640\n",
      "2019-03-17 23:32:14,494 epoch 1 - iter 250/252 - loss 16.73480138\n",
      "2019-03-17 23:32:15,049 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:32:15,050 EPOCH 1 done: loss 16.7210 - lr 0.1000 - bad epochs 0\n",
      "2019-03-17 23:32:37,475 DEV  : loss 4.25139236 - f-score 0.9271 - acc 0.8641\n",
      "2019-03-17 23:33:20,695 TEST : loss 4.24018192 - f-score 0.9290 - acc 0.8675\n",
      "2019-03-17 23:33:36,079 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:33:36,363 epoch 2 - iter 0/252 - loss 5.63378429\n",
      "2019-03-17 23:33:43,075 epoch 2 - iter 25/252 - loss 6.15807489\n",
      "2019-03-17 23:33:49,663 epoch 2 - iter 50/252 - loss 5.87231801\n",
      "2019-03-17 23:33:56,677 epoch 2 - iter 75/252 - loss 5.76678001\n",
      "2019-03-17 23:34:03,238 epoch 2 - iter 100/252 - loss 5.73880428\n",
      "2019-03-17 23:34:09,612 epoch 2 - iter 125/252 - loss 5.65854249\n",
      "2019-03-17 23:34:14,793 epoch 2 - iter 150/252 - loss 5.59712391\n",
      "2019-03-17 23:34:19,276 epoch 2 - iter 175/252 - loss 5.50823383\n",
      "2019-03-17 23:34:26,134 epoch 2 - iter 200/252 - loss 5.46672360\n",
      "2019-03-17 23:34:33,123 epoch 2 - iter 225/252 - loss 5.37179874\n",
      "2019-03-17 23:34:39,864 epoch 2 - iter 250/252 - loss 5.27269994\n",
      "2019-03-17 23:34:40,038 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:34:40,039 EPOCH 2 done: loss 5.2698 - lr 0.1000 - bad epochs 0\n",
      "2019-03-17 23:34:50,223 DEV  : loss 3.11529493 - f-score 0.9388 - acc 0.8848\n",
      "2019-03-17 23:35:12,532 TEST : loss 3.18658328 - f-score 0.9415 - acc 0.8895\n",
      "2019-03-17 23:35:30,106 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:35:30,408 epoch 3 - iter 0/252 - loss 3.12496185\n",
      "2019-03-17 23:35:37,255 epoch 3 - iter 25/252 - loss 3.93072647\n",
      "2019-03-17 23:35:44,142 epoch 3 - iter 50/252 - loss 4.10378017\n",
      "2019-03-17 23:35:51,282 epoch 3 - iter 75/252 - loss 4.11254212\n",
      "2019-03-17 23:35:58,075 epoch 3 - iter 100/252 - loss 4.06534458\n",
      "2019-03-17 23:36:03,841 epoch 3 - iter 125/252 - loss 3.98441444\n",
      "2019-03-17 23:36:09,195 epoch 3 - iter 150/252 - loss 3.98309655\n",
      "2019-03-17 23:36:14,758 epoch 3 - iter 175/252 - loss 3.97420232\n",
      "2019-03-17 23:36:20,453 epoch 3 - iter 200/252 - loss 4.00227541\n",
      "2019-03-17 23:36:26,277 epoch 3 - iter 225/252 - loss 3.97478113\n",
      "2019-03-17 23:36:31,634 epoch 3 - iter 250/252 - loss 3.94991202\n",
      "2019-03-17 23:36:31,795 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:36:31,796 EPOCH 3 done: loss 3.9508 - lr 0.1000 - bad epochs 0\n",
      "2019-03-17 23:36:39,760 DEV  : loss 2.41965890 - f-score 0.9475 - acc 0.9003\n",
      "2019-03-17 23:36:54,249 TEST : loss 2.55277300 - f-score 0.9509 - acc 0.9065\n",
      "2019-03-17 23:37:10,510 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:37:10,816 epoch 4 - iter 0/252 - loss 5.04678822\n",
      "2019-03-17 23:37:17,662 epoch 4 - iter 25/252 - loss 3.81240976\n",
      "2019-03-17 23:37:24,403 epoch 4 - iter 50/252 - loss 3.69964106\n",
      "2019-03-17 23:37:30,841 epoch 4 - iter 75/252 - loss 3.60748097\n",
      "2019-03-17 23:37:37,727 epoch 4 - iter 100/252 - loss 3.47740941\n",
      "2019-03-17 23:37:44,510 epoch 4 - iter 125/252 - loss 3.45290592\n",
      "2019-03-17 23:37:51,377 epoch 4 - iter 150/252 - loss 3.44346647\n",
      "2019-03-17 23:37:57,987 epoch 4 - iter 175/252 - loss 3.43000424\n",
      "2019-03-17 23:38:04,665 epoch 4 - iter 200/252 - loss 3.37687245\n",
      "2019-03-17 23:38:11,779 epoch 4 - iter 225/252 - loss 3.42623255\n",
      "2019-03-17 23:38:18,549 epoch 4 - iter 250/252 - loss 3.38901977\n",
      "2019-03-17 23:38:18,741 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:38:18,742 EPOCH 4 done: loss 3.3876 - lr 0.1000 - bad epochs 0\n",
      "2019-03-17 23:38:28,757 DEV  : loss 2.24462676 - f-score 0.9518 - acc 0.9080\n",
      "2019-03-17 23:38:48,853 TEST : loss 2.50530767 - f-score 0.9497 - acc 0.9042\n",
      "2019-03-17 23:39:03,019 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:39:05,711 epoch 5 - iter 0/252 - loss 1.69019163\n",
      "2019-03-17 23:39:11,143 epoch 5 - iter 25/252 - loss 3.07177502\n",
      "2019-03-17 23:39:16,547 epoch 5 - iter 50/252 - loss 2.98467457\n",
      "2019-03-17 23:39:21,414 epoch 5 - iter 75/252 - loss 2.96168573\n",
      "2019-03-17 23:39:25,930 epoch 5 - iter 100/252 - loss 2.94647904\n",
      "2019-03-17 23:39:31,849 epoch 5 - iter 125/252 - loss 3.00415577\n",
      "2019-03-17 23:39:38,668 epoch 5 - iter 150/252 - loss 2.99387326\n",
      "2019-03-17 23:39:45,384 epoch 5 - iter 175/252 - loss 2.96328180\n",
      "2019-03-17 23:39:52,286 epoch 5 - iter 200/252 - loss 2.99742837\n",
      "2019-03-17 23:39:59,357 epoch 5 - iter 225/252 - loss 3.02139159\n",
      "2019-03-17 23:40:05,960 epoch 5 - iter 250/252 - loss 2.97679312\n",
      "2019-03-17 23:40:06,144 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:40:06,145 EPOCH 5 done: loss 2.9793 - lr 0.1000 - bad epochs 0\n",
      "2019-03-17 23:40:15,834 DEV  : loss 1.89830506 - f-score 0.9543 - acc 0.9127\n",
      "2019-03-17 23:40:37,926 TEST : loss 2.10201812 - f-score 0.9569 - acc 0.9173\n",
      "2019-03-17 23:40:53,518 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:40:54,361 epoch 6 - iter 0/252 - loss 1.93750262\n",
      "2019-03-17 23:41:01,148 epoch 6 - iter 25/252 - loss 2.69047181\n",
      "2019-03-17 23:41:08,250 epoch 6 - iter 50/252 - loss 2.76007491\n",
      "2019-03-17 23:41:13,786 epoch 6 - iter 75/252 - loss 2.74105873\n",
      "2019-03-17 23:41:19,127 epoch 6 - iter 100/252 - loss 2.64100967\n",
      "2019-03-17 23:41:24,718 epoch 6 - iter 125/252 - loss 2.63577491\n",
      "2019-03-17 23:41:30,463 epoch 6 - iter 150/252 - loss 2.62346455\n",
      "2019-03-17 23:41:36,194 epoch 6 - iter 175/252 - loss 2.64898511\n",
      "2019-03-17 23:41:41,531 epoch 6 - iter 200/252 - loss 2.64377062\n",
      "2019-03-17 23:41:47,145 epoch 6 - iter 225/252 - loss 2.63626696\n",
      "2019-03-17 23:41:52,684 epoch 6 - iter 250/252 - loss 2.61800213\n",
      "2019-03-17 23:41:52,833 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:41:52,834 EPOCH 6 done: loss 2.6166 - lr 0.1000 - bad epochs 0\n",
      "2019-03-17 23:41:59,662 DEV  : loss 1.82916367 - f-score 0.9566 - acc 0.9169\n",
      "2019-03-17 23:42:17,521 TEST : loss 2.06817913 - f-score 0.9569 - acc 0.9174\n",
      "2019-03-17 23:42:33,466 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:42:34,363 epoch 7 - iter 0/252 - loss 2.04064298\n",
      "2019-03-17 23:42:41,666 epoch 7 - iter 25/252 - loss 2.63194660\n",
      "2019-03-17 23:42:48,351 epoch 7 - iter 50/252 - loss 2.53950730\n",
      "2019-03-17 23:42:55,242 epoch 7 - iter 75/252 - loss 2.44889180\n",
      "2019-03-17 23:43:01,944 epoch 7 - iter 100/252 - loss 2.42918706\n",
      "2019-03-17 23:43:08,638 epoch 7 - iter 125/252 - loss 2.39598587\n",
      "2019-03-17 23:43:15,404 epoch 7 - iter 150/252 - loss 2.42008005\n",
      "2019-03-17 23:43:22,045 epoch 7 - iter 175/252 - loss 2.41960332\n",
      "2019-03-17 23:43:28,710 epoch 7 - iter 200/252 - loss 2.40912749\n",
      "2019-03-17 23:43:35,565 epoch 7 - iter 225/252 - loss 2.41798212\n",
      "2019-03-17 23:43:42,007 epoch 7 - iter 250/252 - loss 2.44058900\n",
      "2019-03-17 23:43:42,180 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-17 23:43:42,181 EPOCH 7 done: loss 2.4415 - lr 0.1000 - bad epochs 0\n",
      "2019-03-17 23:43:51,894 DEV  : loss 1.68751574 - f-score 0.9574 - acc 0.9183\n",
      "2019-03-17 23:44:09,700 TEST : loss 1.94558537 - f-score 0.9573 - acc 0.9181\n",
      "2019-03-17 23:44:25,233 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:44:25,778 epoch 8 - iter 0/252 - loss 2.90777588\n",
      "2019-03-17 23:44:31,226 epoch 8 - iter 25/252 - loss 2.27331991\n",
      "2019-03-17 23:44:35,478 epoch 8 - iter 50/252 - loss 2.40950742\n",
      "2019-03-17 23:44:40,311 epoch 8 - iter 75/252 - loss 2.44182437\n",
      "2019-03-17 23:44:47,138 epoch 8 - iter 100/252 - loss 2.38026772\n",
      "2019-03-17 23:44:54,299 epoch 8 - iter 125/252 - loss 2.37121129\n",
      "2019-03-17 23:45:01,141 epoch 8 - iter 150/252 - loss 2.35108398\n",
      "2019-03-17 23:45:07,737 epoch 8 - iter 175/252 - loss 2.32164540\n",
      "2019-03-17 23:45:14,413 epoch 8 - iter 200/252 - loss 2.30462948\n",
      "2019-03-17 23:45:21,019 epoch 8 - iter 225/252 - loss 2.30583923\n",
      "2019-03-17 23:45:27,684 epoch 8 - iter 250/252 - loss 2.29026254\n",
      "2019-03-17 23:45:27,828 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:45:27,829 EPOCH 8 done: loss 2.2891 - lr 0.1000 - bad epochs 0\n",
      "2019-03-17 23:45:37,784 DEV  : loss 1.61788929 - f-score 0.9603 - acc 0.9236\n",
      "2019-03-17 23:46:00,883 TEST : loss 1.95693302 - f-score 0.9590 - acc 0.9213\n",
      "2019-03-17 23:46:17,437 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:46:17,988 epoch 9 - iter 0/252 - loss 1.54333699\n",
      "2019-03-17 23:46:23,299 epoch 9 - iter 25/252 - loss 2.09427362\n",
      "2019-03-17 23:46:28,817 epoch 9 - iter 50/252 - loss 2.14381875\n",
      "2019-03-17 23:46:34,355 epoch 9 - iter 75/252 - loss 2.16384937\n",
      "2019-03-17 23:46:39,910 epoch 9 - iter 100/252 - loss 2.16205763\n",
      "2019-03-17 23:46:45,286 epoch 9 - iter 125/252 - loss 2.14363199\n",
      "2019-03-17 23:46:50,852 epoch 9 - iter 150/252 - loss 2.11977808\n",
      "2019-03-17 23:46:56,471 epoch 9 - iter 175/252 - loss 2.12991812\n",
      "2019-03-17 23:47:01,943 epoch 9 - iter 200/252 - loss 2.15417670\n",
      "2019-03-17 23:47:07,233 epoch 9 - iter 225/252 - loss 2.15583078\n",
      "2019-03-17 23:47:11,555 epoch 9 - iter 250/252 - loss 2.19176451\n",
      "2019-03-17 23:47:11,725 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:47:11,726 EPOCH 9 done: loss 2.1937 - lr 0.1000 - bad epochs 0\n",
      "2019-03-17 23:47:18,906 DEV  : loss 1.56122482 - f-score 0.9598 - acc 0.9227\n",
      "2019-03-17 23:47:41,444 TEST : loss 1.92009938 - f-score 0.9578 - acc 0.9190\n",
      "2019-03-17 23:47:55,647 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:47:57,212 epoch 10 - iter 0/252 - loss 2.82997990\n",
      "2019-03-17 23:48:03,868 epoch 10 - iter 25/252 - loss 2.03869755\n",
      "2019-03-17 23:48:10,735 epoch 10 - iter 50/252 - loss 1.91813650\n",
      "2019-03-17 23:48:17,561 epoch 10 - iter 75/252 - loss 1.94910829\n",
      "2019-03-17 23:48:24,394 epoch 10 - iter 100/252 - loss 1.99254538\n",
      "2019-03-17 23:48:31,410 epoch 10 - iter 125/252 - loss 2.01306536\n",
      "2019-03-17 23:48:38,020 epoch 10 - iter 150/252 - loss 2.06363138\n",
      "2019-03-17 23:48:44,739 epoch 10 - iter 175/252 - loss 2.07248145\n",
      "2019-03-17 23:48:51,337 epoch 10 - iter 200/252 - loss 2.06667177\n",
      "2019-03-17 23:48:57,746 epoch 10 - iter 225/252 - loss 2.05672964\n",
      "2019-03-17 23:49:03,132 epoch 10 - iter 250/252 - loss 2.02937313\n",
      "2019-03-17 23:49:03,290 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:49:03,291 EPOCH 10 done: loss 2.0297 - lr 0.1000 - bad epochs 0\n",
      "2019-03-17 23:49:11,169 DEV  : loss 1.65947092 - f-score 0.9585 - acc 0.9203\n",
      "2019-03-17 23:49:29,342 TEST : loss 1.95881128 - f-score 0.9595 - acc 0.9221\n",
      "2019-03-17 23:49:46,995 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:49:47,180 epoch 11 - iter 0/252 - loss 1.40287483\n",
      "2019-03-17 23:49:52,515 epoch 11 - iter 25/252 - loss 2.06082639\n",
      "2019-03-17 23:49:59,104 epoch 11 - iter 50/252 - loss 1.95448408\n",
      "2019-03-17 23:50:06,057 epoch 11 - iter 75/252 - loss 1.88946512\n",
      "2019-03-17 23:50:13,157 epoch 11 - iter 100/252 - loss 1.98440299\n",
      "2019-03-17 23:50:19,828 epoch 11 - iter 125/252 - loss 2.00018546\n",
      "2019-03-17 23:50:26,603 epoch 11 - iter 150/252 - loss 1.98867165\n",
      "2019-03-17 23:50:33,172 epoch 11 - iter 175/252 - loss 1.97354036\n",
      "2019-03-17 23:50:40,085 epoch 11 - iter 200/252 - loss 1.95568216\n",
      "2019-03-17 23:50:46,585 epoch 11 - iter 225/252 - loss 1.93664342\n",
      "2019-03-17 23:50:53,092 epoch 11 - iter 250/252 - loss 1.93063822\n",
      "2019-03-17 23:50:53,272 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:50:53,273 EPOCH 11 done: loss 1.9309 - lr 0.1000 - bad epochs 0\n",
      "2019-03-17 23:51:03,085 DEV  : loss 1.57736051 - f-score 0.9606 - acc 0.9241\n",
      "2019-03-17 23:51:25,384 TEST : loss 1.89987612 - f-score 0.9583 - acc 0.9200\n",
      "2019-03-17 23:51:40,924 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:51:41,148 epoch 12 - iter 0/252 - loss 2.12214541\n",
      "2019-03-17 23:51:46,835 epoch 12 - iter 25/252 - loss 1.98913514\n",
      "2019-03-17 23:51:52,377 epoch 12 - iter 50/252 - loss 1.79521881\n",
      "2019-03-17 23:51:57,800 epoch 12 - iter 75/252 - loss 1.73841032\n",
      "2019-03-17 23:52:03,175 epoch 12 - iter 100/252 - loss 1.82364656\n",
      "2019-03-17 23:52:08,730 epoch 12 - iter 125/252 - loss 1.89518676\n",
      "2019-03-17 23:52:13,936 epoch 12 - iter 150/252 - loss 1.87166306\n",
      "2019-03-17 23:52:18,966 epoch 12 - iter 175/252 - loss 1.86831078\n",
      "2019-03-17 23:52:23,325 epoch 12 - iter 200/252 - loss 1.86671349\n",
      "2019-03-17 23:52:29,074 epoch 12 - iter 225/252 - loss 1.86299586\n",
      "2019-03-17 23:52:35,822 epoch 12 - iter 250/252 - loss 1.86695662\n",
      "2019-03-17 23:52:36,042 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:52:36,043 EPOCH 12 done: loss 1.8679 - lr 0.1000 - bad epochs 0\n",
      "2019-03-17 23:52:45,909 DEV  : loss 1.51402640 - f-score 0.9613 - acc 0.9256\n",
      "2019-03-17 23:53:08,512 TEST : loss 1.86740875 - f-score 0.9605 - acc 0.9241\n",
      "2019-03-17 23:53:24,367 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:53:24,643 epoch 13 - iter 0/252 - loss 1.89602566\n",
      "2019-03-17 23:53:31,422 epoch 13 - iter 25/252 - loss 1.64952868\n",
      "2019-03-17 23:53:38,441 epoch 13 - iter 50/252 - loss 1.70855943\n",
      "2019-03-17 23:53:45,091 epoch 13 - iter 75/252 - loss 1.67208643\n",
      "2019-03-17 23:53:51,761 epoch 13 - iter 100/252 - loss 1.63861103\n",
      "2019-03-17 23:53:58,676 epoch 13 - iter 125/252 - loss 1.71127671\n",
      "2019-03-17 23:54:05,180 epoch 13 - iter 150/252 - loss 1.69115374\n",
      "2019-03-17 23:54:10,501 epoch 13 - iter 175/252 - loss 1.73545551\n",
      "2019-03-17 23:54:15,864 epoch 13 - iter 200/252 - loss 1.74784257\n",
      "2019-03-17 23:54:21,284 epoch 13 - iter 225/252 - loss 1.75461245\n",
      "2019-03-17 23:54:27,022 epoch 13 - iter 250/252 - loss 1.74814542\n",
      "2019-03-17 23:54:27,198 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:54:27,199 EPOCH 13 done: loss 1.7475 - lr 0.1000 - bad epochs 0\n",
      "2019-03-17 23:54:34,995 DEV  : loss 1.45403683 - f-score 0.9629 - acc 0.9284\n",
      "2019-03-17 23:54:52,044 TEST : loss 1.83034217 - f-score 0.9607 - acc 0.9243\n",
      "2019-03-17 23:55:07,828 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:55:08,120 epoch 14 - iter 0/252 - loss 1.12803423\n",
      "2019-03-17 23:55:14,737 epoch 14 - iter 25/252 - loss 1.40813638\n",
      "2019-03-17 23:55:21,198 epoch 14 - iter 50/252 - loss 1.53977368\n",
      "2019-03-17 23:55:27,963 epoch 14 - iter 75/252 - loss 1.51736681\n",
      "2019-03-17 23:55:34,692 epoch 14 - iter 100/252 - loss 1.52282467\n",
      "2019-03-17 23:55:41,352 epoch 14 - iter 125/252 - loss 1.53667421\n",
      "2019-03-17 23:55:48,153 epoch 14 - iter 150/252 - loss 1.55941140\n",
      "2019-03-17 23:55:54,806 epoch 14 - iter 175/252 - loss 1.59409842\n",
      "2019-03-17 23:56:01,504 epoch 14 - iter 200/252 - loss 1.58552178\n",
      "2019-03-17 23:56:08,307 epoch 14 - iter 225/252 - loss 1.58914710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-17 23:56:15,293 epoch 14 - iter 250/252 - loss 1.61858657\n",
      "2019-03-17 23:56:15,474 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:56:15,475 EPOCH 14 done: loss 1.6194 - lr 0.1000 - bad epochs 0\n",
      "2019-03-17 23:56:25,115 DEV  : loss 1.45441425 - f-score 0.9628 - acc 0.9284\n",
      "2019-03-17 23:56:45,738 TEST : loss 1.78657722 - f-score 0.9614 - acc 0.9258\n",
      "2019-03-17 23:57:01,550 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:57:01,772 epoch 15 - iter 0/252 - loss 1.91809320\n",
      "2019-03-17 23:57:07,243 epoch 15 - iter 25/252 - loss 1.61261233\n",
      "2019-03-17 23:57:12,830 epoch 15 - iter 50/252 - loss 1.69123557\n",
      "2019-03-17 23:57:18,271 epoch 15 - iter 75/252 - loss 1.60682987\n",
      "2019-03-17 23:57:23,184 epoch 15 - iter 100/252 - loss 1.59830611\n",
      "2019-03-17 23:57:27,743 epoch 15 - iter 125/252 - loss 1.59793631\n",
      "2019-03-17 23:57:34,440 epoch 15 - iter 150/252 - loss 1.60021255\n",
      "2019-03-17 23:57:41,342 epoch 15 - iter 175/252 - loss 1.57974821\n",
      "2019-03-17 23:57:48,222 epoch 15 - iter 200/252 - loss 1.60245290\n",
      "2019-03-17 23:57:54,965 epoch 15 - iter 225/252 - loss 1.57695827\n",
      "2019-03-17 23:58:01,635 epoch 15 - iter 250/252 - loss 1.57815315\n",
      "2019-03-17 23:58:01,829 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:58:01,830 EPOCH 15 done: loss 1.5768 - lr 0.1000 - bad epochs 0\n",
      "2019-03-17 23:58:11,623 DEV  : loss 1.43323696 - f-score 0.9628 - acc 0.9282\n",
      "2019-03-17 23:58:33,262 TEST : loss 1.80736470 - f-score 0.9614 - acc 0.9257\n",
      "2019-03-17 23:58:47,919 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:58:48,728 epoch 16 - iter 0/252 - loss 1.81843758\n",
      "2019-03-17 23:58:55,341 epoch 16 - iter 25/252 - loss 1.69448696\n",
      "2019-03-17 23:59:02,045 epoch 16 - iter 50/252 - loss 1.49718225\n",
      "2019-03-17 23:59:08,927 epoch 16 - iter 75/252 - loss 1.55269674\n",
      "2019-03-17 23:59:14,249 epoch 16 - iter 100/252 - loss 1.50468409\n",
      "2019-03-17 23:59:19,570 epoch 16 - iter 125/252 - loss 1.49890772\n",
      "2019-03-17 23:59:25,119 epoch 16 - iter 150/252 - loss 1.47007437\n",
      "2019-03-17 23:59:30,711 epoch 16 - iter 175/252 - loss 1.45693587\n",
      "2019-03-17 23:59:36,179 epoch 16 - iter 200/252 - loss 1.43859716\n",
      "2019-03-17 23:59:41,495 epoch 16 - iter 225/252 - loss 1.46758873\n",
      "2019-03-17 23:59:46,803 epoch 16 - iter 250/252 - loss 1.49226197\n",
      "2019-03-17 23:59:46,932 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-17 23:59:46,932 EPOCH 16 done: loss 1.4928 - lr 0.1000 - bad epochs 0\n",
      "2019-03-17 23:59:54,623 DEV  : loss 1.38930118 - f-score 0.9636 - acc 0.9297\n",
      "2019-03-18 00:00:11,382 TEST : loss 1.79753315 - f-score 0.9613 - acc 0.9255\n",
      "2019-03-18 00:00:26,232 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:00:27,453 epoch 17 - iter 0/252 - loss 0.83019829\n",
      "2019-03-18 00:00:34,073 epoch 17 - iter 25/252 - loss 1.33496961\n",
      "2019-03-18 00:00:40,788 epoch 17 - iter 50/252 - loss 1.36178093\n",
      "2019-03-18 00:00:47,333 epoch 17 - iter 75/252 - loss 1.48338130\n",
      "2019-03-18 00:00:53,724 epoch 17 - iter 100/252 - loss 1.49672388\n",
      "2019-03-18 00:01:00,221 epoch 17 - iter 125/252 - loss 1.49368771\n",
      "2019-03-18 00:01:06,819 epoch 17 - iter 150/252 - loss 1.49450438\n",
      "2019-03-18 00:01:13,509 epoch 17 - iter 175/252 - loss 1.49567932\n",
      "2019-03-18 00:01:20,319 epoch 17 - iter 200/252 - loss 1.49252568\n",
      "2019-03-18 00:01:26,860 epoch 17 - iter 225/252 - loss 1.48394859\n",
      "2019-03-18 00:01:33,451 epoch 17 - iter 250/252 - loss 1.50258517\n",
      "2019-03-18 00:01:33,642 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:01:33,643 EPOCH 17 done: loss 1.5012 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:01:43,115 DEV  : loss 1.41400802 - f-score 0.9649 - acc 0.9323\n",
      "2019-03-18 00:02:00,940 TEST : loss 1.79430258 - f-score 0.9611 - acc 0.9251\n",
      "2019-03-18 00:02:00,944 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:02:01,237 epoch 18 - iter 0/252 - loss 1.98844504\n",
      "2019-03-18 00:02:06,559 epoch 18 - iter 25/252 - loss 1.42621350\n",
      "2019-03-18 00:02:11,882 epoch 18 - iter 50/252 - loss 1.39916654\n",
      "2019-03-18 00:02:17,314 epoch 18 - iter 75/252 - loss 1.51161419\n",
      "2019-03-18 00:02:22,752 epoch 18 - iter 100/252 - loss 1.48924307\n",
      "2019-03-18 00:02:28,487 epoch 18 - iter 125/252 - loss 1.48299422\n",
      "2019-03-18 00:02:33,598 epoch 18 - iter 150/252 - loss 1.46757907\n",
      "2019-03-18 00:02:37,731 epoch 18 - iter 175/252 - loss 1.46985189\n",
      "2019-03-18 00:02:42,206 epoch 18 - iter 200/252 - loss 1.44263961\n",
      "2019-03-18 00:02:48,979 epoch 18 - iter 225/252 - loss 1.42190386\n",
      "2019-03-18 00:02:55,486 epoch 18 - iter 250/252 - loss 1.43530724\n",
      "2019-03-18 00:02:55,655 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:02:55,656 EPOCH 18 done: loss 1.4339 - lr 0.1000 - bad epochs 1\n",
      "2019-03-18 00:03:05,667 DEV  : loss 1.42830110 - f-score 0.9648 - acc 0.9320\n",
      "2019-03-18 00:03:28,091 TEST : loss 1.82149184 - f-score 0.9617 - acc 0.9262\n",
      "2019-03-18 00:03:39,354 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:03:41,154 epoch 19 - iter 0/252 - loss 1.93946707\n",
      "2019-03-18 00:03:47,835 epoch 19 - iter 25/252 - loss 1.22422413\n",
      "2019-03-18 00:03:54,621 epoch 19 - iter 50/252 - loss 1.33502672\n",
      "2019-03-18 00:04:01,252 epoch 19 - iter 75/252 - loss 1.28382017\n",
      "2019-03-18 00:04:08,025 epoch 19 - iter 100/252 - loss 1.25115421\n",
      "2019-03-18 00:04:14,613 epoch 19 - iter 125/252 - loss 1.29592462\n",
      "2019-03-18 00:04:21,179 epoch 19 - iter 150/252 - loss 1.30842985\n",
      "2019-03-18 00:04:27,153 epoch 19 - iter 175/252 - loss 1.32322876\n",
      "2019-03-18 00:04:32,606 epoch 19 - iter 200/252 - loss 1.34975405\n",
      "2019-03-18 00:04:38,190 epoch 19 - iter 225/252 - loss 1.37469414\n",
      "2019-03-18 00:04:43,542 epoch 19 - iter 250/252 - loss 1.36921934\n",
      "2019-03-18 00:04:43,696 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:04:43,697 EPOCH 19 done: loss 1.3684 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:04:51,720 DEV  : loss 1.50284970 - f-score 0.9628 - acc 0.9284\n",
      "2019-03-18 00:05:09,240 TEST : loss 1.90712094 - f-score 0.9599 - acc 0.9229\n",
      "2019-03-18 00:05:24,321 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:05:24,601 epoch 20 - iter 0/252 - loss 0.88381386\n",
      "2019-03-18 00:05:31,322 epoch 20 - iter 25/252 - loss 1.16436072\n",
      "2019-03-18 00:05:37,753 epoch 20 - iter 50/252 - loss 1.20273352\n",
      "2019-03-18 00:05:44,379 epoch 20 - iter 75/252 - loss 1.19808780\n",
      "2019-03-18 00:05:50,970 epoch 20 - iter 100/252 - loss 1.19622943\n",
      "2019-03-18 00:05:57,513 epoch 20 - iter 125/252 - loss 1.21402252\n",
      "2019-03-18 00:06:03,964 epoch 20 - iter 150/252 - loss 1.26986375\n",
      "2019-03-18 00:06:10,700 epoch 20 - iter 175/252 - loss 1.26412506\n",
      "2019-03-18 00:06:17,268 epoch 20 - iter 200/252 - loss 1.24891675\n",
      "2019-03-18 00:06:23,987 epoch 20 - iter 225/252 - loss 1.25291699\n",
      "2019-03-18 00:06:30,523 epoch 20 - iter 250/252 - loss 1.27714906\n",
      "2019-03-18 00:06:30,675 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:06:30,675 EPOCH 20 done: loss 1.2778 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:06:40,144 DEV  : loss 1.38619530 - f-score 0.9641 - acc 0.9307\n",
      "2019-03-18 00:07:00,899 TEST : loss 1.80809760 - f-score 0.9612 - acc 0.9254\n",
      "2019-03-18 00:07:14,651 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:07:15,349 epoch 21 - iter 0/252 - loss 0.70517629\n",
      "2019-03-18 00:07:20,960 epoch 21 - iter 25/252 - loss 1.19265220\n",
      "2019-03-18 00:07:26,384 epoch 21 - iter 50/252 - loss 1.25471579\n",
      "2019-03-18 00:07:31,775 epoch 21 - iter 75/252 - loss 1.28484686\n",
      "2019-03-18 00:07:37,218 epoch 21 - iter 100/252 - loss 1.29797701\n",
      "2019-03-18 00:07:42,479 epoch 21 - iter 125/252 - loss 1.31756531\n",
      "2019-03-18 00:07:46,758 epoch 21 - iter 150/252 - loss 1.27841835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 00:07:53,150 epoch 21 - iter 175/252 - loss 1.30064517\n",
      "2019-03-18 00:07:59,990 epoch 21 - iter 200/252 - loss 1.29456116\n",
      "2019-03-18 00:08:06,675 epoch 21 - iter 225/252 - loss 1.31048936\n",
      "2019-03-18 00:08:13,426 epoch 21 - iter 250/252 - loss 1.30656933\n",
      "2019-03-18 00:08:13,603 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:08:13,604 EPOCH 21 done: loss 1.3061 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:08:23,101 DEV  : loss 1.42353547 - f-score 0.9651 - acc 0.9325\n",
      "2019-03-18 00:08:44,910 TEST : loss 1.85697222 - f-score 0.9628 - acc 0.9284\n",
      "2019-03-18 00:08:44,912 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:08:45,233 epoch 22 - iter 0/252 - loss 1.38127995\n",
      "2019-03-18 00:08:51,918 epoch 22 - iter 25/252 - loss 1.24377674\n",
      "2019-03-18 00:08:58,560 epoch 22 - iter 50/252 - loss 1.25182308\n",
      "2019-03-18 00:09:05,304 epoch 22 - iter 75/252 - loss 1.25142489\n",
      "2019-03-18 00:09:12,021 epoch 22 - iter 100/252 - loss 1.25355080\n",
      "2019-03-18 00:09:18,944 epoch 22 - iter 125/252 - loss 1.22808269\n",
      "2019-03-18 00:09:25,604 epoch 22 - iter 150/252 - loss 1.21318738\n",
      "2019-03-18 00:09:32,242 epoch 22 - iter 175/252 - loss 1.20039635\n",
      "2019-03-18 00:09:38,660 epoch 22 - iter 200/252 - loss 1.20670597\n",
      "2019-03-18 00:09:43,979 epoch 22 - iter 225/252 - loss 1.21973050\n",
      "2019-03-18 00:09:49,334 epoch 22 - iter 250/252 - loss 1.22808102\n",
      "2019-03-18 00:09:49,485 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:09:49,486 EPOCH 22 done: loss 1.2273 - lr 0.1000 - bad epochs 1\n",
      "2019-03-18 00:09:57,379 DEV  : loss 1.49673474 - f-score 0.9654 - acc 0.9333\n",
      "2019-03-18 00:10:14,824 TEST : loss 1.91357207 - f-score 0.9611 - acc 0.9252\n",
      "2019-03-18 00:10:32,661 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:10:33,945 epoch 23 - iter 0/252 - loss 0.54161322\n",
      "2019-03-18 00:10:40,755 epoch 23 - iter 25/252 - loss 1.07920832\n",
      "2019-03-18 00:10:47,477 epoch 23 - iter 50/252 - loss 1.23663289\n",
      "2019-03-18 00:10:54,397 epoch 23 - iter 75/252 - loss 1.21971308\n",
      "2019-03-18 00:11:00,707 epoch 23 - iter 100/252 - loss 1.19676914\n",
      "2019-03-18 00:11:07,159 epoch 23 - iter 125/252 - loss 1.16808059\n",
      "2019-03-18 00:11:13,956 epoch 23 - iter 150/252 - loss 1.20058272\n",
      "2019-03-18 00:11:20,600 epoch 23 - iter 175/252 - loss 1.20241461\n",
      "2019-03-18 00:11:27,260 epoch 23 - iter 200/252 - loss 1.20466404\n",
      "2019-03-18 00:11:33,960 epoch 23 - iter 225/252 - loss 1.20416178\n",
      "2019-03-18 00:11:40,414 epoch 23 - iter 250/252 - loss 1.20064366\n",
      "2019-03-18 00:11:40,592 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:11:40,593 EPOCH 23 done: loss 1.2001 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:11:51,263 DEV  : loss 1.46537137 - f-score 0.9656 - acc 0.9335\n",
      "2019-03-18 00:12:13,441 TEST : loss 1.86336589 - f-score 0.9615 - acc 0.9259\n",
      "2019-03-18 00:12:28,503 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:12:28,799 epoch 24 - iter 0/252 - loss 1.47955275\n",
      "2019-03-18 00:12:34,465 epoch 24 - iter 25/252 - loss 1.26539809\n",
      "2019-03-18 00:12:40,032 epoch 24 - iter 50/252 - loss 1.20768704\n",
      "2019-03-18 00:12:45,430 epoch 24 - iter 75/252 - loss 1.13360463\n",
      "2019-03-18 00:12:51,136 epoch 24 - iter 100/252 - loss 1.14264360\n",
      "2019-03-18 00:12:56,661 epoch 24 - iter 125/252 - loss 1.13903823\n",
      "2019-03-18 00:13:01,644 epoch 24 - iter 150/252 - loss 1.15910710\n",
      "2019-03-18 00:13:05,956 epoch 24 - iter 175/252 - loss 1.15159195\n",
      "2019-03-18 00:13:11,644 epoch 24 - iter 200/252 - loss 1.17221669\n",
      "2019-03-18 00:13:18,614 epoch 24 - iter 225/252 - loss 1.16555391\n",
      "2019-03-18 00:13:25,562 epoch 24 - iter 250/252 - loss 1.14843574\n",
      "2019-03-18 00:13:25,745 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:13:25,746 EPOCH 24 done: loss 1.1487 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:13:35,670 DEV  : loss 1.43012798 - f-score 0.9640 - acc 0.9305\n",
      "2019-03-18 00:13:57,819 TEST : loss 1.87844682 - f-score 0.9616 - acc 0.9261\n",
      "2019-03-18 00:14:12,987 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:14:13,300 epoch 25 - iter 0/252 - loss 0.89269972\n",
      "2019-03-18 00:14:19,937 epoch 25 - iter 25/252 - loss 1.09618416\n",
      "2019-03-18 00:14:26,789 epoch 25 - iter 50/252 - loss 1.02578913\n",
      "2019-03-18 00:14:33,553 epoch 25 - iter 75/252 - loss 1.06063648\n",
      "2019-03-18 00:14:40,450 epoch 25 - iter 100/252 - loss 1.12186788\n",
      "2019-03-18 00:14:47,097 epoch 25 - iter 125/252 - loss 1.13016708\n",
      "2019-03-18 00:14:52,772 epoch 25 - iter 150/252 - loss 1.11720324\n",
      "2019-03-18 00:14:58,205 epoch 25 - iter 175/252 - loss 1.12484689\n",
      "2019-03-18 00:15:03,659 epoch 25 - iter 200/252 - loss 1.11595447\n",
      "2019-03-18 00:15:09,187 epoch 25 - iter 225/252 - loss 1.09054559\n",
      "2019-03-18 00:15:14,665 epoch 25 - iter 250/252 - loss 1.08773126\n",
      "2019-03-18 00:15:14,803 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:15:14,803 EPOCH 25 done: loss 1.0888 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:15:22,258 DEV  : loss 1.45068431 - f-score 0.9669 - acc 0.9360\n",
      "2019-03-18 00:15:38,512 TEST : loss 1.89623845 - f-score 0.9634 - acc 0.9294\n",
      "2019-03-18 00:15:53,098 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:15:54,640 epoch 26 - iter 0/252 - loss 1.06151056\n",
      "2019-03-18 00:16:01,589 epoch 26 - iter 25/252 - loss 0.96665019\n",
      "2019-03-18 00:16:08,182 epoch 26 - iter 50/252 - loss 0.97508391\n",
      "2019-03-18 00:16:14,805 epoch 26 - iter 75/252 - loss 0.97581215\n",
      "2019-03-18 00:16:21,304 epoch 26 - iter 100/252 - loss 1.03832255\n",
      "2019-03-18 00:16:27,967 epoch 26 - iter 125/252 - loss 1.04793313\n",
      "2019-03-18 00:16:34,533 epoch 26 - iter 150/252 - loss 1.06359701\n",
      "2019-03-18 00:16:41,209 epoch 26 - iter 175/252 - loss 1.05806137\n",
      "2019-03-18 00:16:48,119 epoch 26 - iter 200/252 - loss 1.07754890\n",
      "2019-03-18 00:16:54,606 epoch 26 - iter 225/252 - loss 1.09427754\n",
      "2019-03-18 00:17:01,425 epoch 26 - iter 250/252 - loss 1.11032876\n",
      "2019-03-18 00:17:01,617 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:17:01,617 EPOCH 26 done: loss 1.1105 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:17:11,368 DEV  : loss 1.49058485 - f-score 0.9645 - acc 0.9315\n",
      "2019-03-18 00:17:31,168 TEST : loss 1.99672818 - f-score 0.9618 - acc 0.9263\n",
      "2019-03-18 00:17:31,173 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:17:31,393 epoch 27 - iter 0/252 - loss 1.23918271\n",
      "2019-03-18 00:17:36,973 epoch 27 - iter 25/252 - loss 0.87796116\n",
      "2019-03-18 00:17:42,511 epoch 27 - iter 50/252 - loss 1.05985213\n",
      "2019-03-18 00:17:47,854 epoch 27 - iter 75/252 - loss 1.13267949\n",
      "2019-03-18 00:17:53,643 epoch 27 - iter 100/252 - loss 1.12536925\n",
      "2019-03-18 00:17:59,005 epoch 27 - iter 125/252 - loss 1.11539244\n",
      "2019-03-18 00:18:04,335 epoch 27 - iter 150/252 - loss 1.09556043\n",
      "2019-03-18 00:18:09,711 epoch 27 - iter 175/252 - loss 1.07658226\n",
      "2019-03-18 00:18:14,320 epoch 27 - iter 200/252 - loss 1.09979762\n",
      "2019-03-18 00:18:18,696 epoch 27 - iter 225/252 - loss 1.07353622\n",
      "2019-03-18 00:18:25,453 epoch 27 - iter 250/252 - loss 1.08394976\n",
      "2019-03-18 00:18:25,655 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:18:25,656 EPOCH 27 done: loss 1.0848 - lr 0.1000 - bad epochs 1\n",
      "2019-03-18 00:18:35,390 DEV  : loss 1.40391624 - f-score 0.9661 - acc 0.9345\n",
      "2019-03-18 00:18:57,358 TEST : loss 1.94426858 - f-score 0.9627 - acc 0.9281\n",
      "2019-03-18 00:19:10,756 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:19:13,284 epoch 28 - iter 0/252 - loss 0.67517173\n",
      "2019-03-18 00:19:20,110 epoch 28 - iter 25/252 - loss 0.97736130\n",
      "2019-03-18 00:19:26,579 epoch 28 - iter 50/252 - loss 0.95755464\n",
      "2019-03-18 00:19:33,048 epoch 28 - iter 75/252 - loss 0.95169981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 00:19:39,438 epoch 28 - iter 100/252 - loss 0.98534920\n",
      "2019-03-18 00:19:46,200 epoch 28 - iter 125/252 - loss 1.00326335\n",
      "2019-03-18 00:19:52,896 epoch 28 - iter 150/252 - loss 1.03841952\n",
      "2019-03-18 00:19:59,233 epoch 28 - iter 175/252 - loss 1.02182412\n",
      "2019-03-18 00:20:05,212 epoch 28 - iter 200/252 - loss 1.01105713\n",
      "2019-03-18 00:20:10,767 epoch 28 - iter 225/252 - loss 1.02168390\n",
      "2019-03-18 00:20:16,395 epoch 28 - iter 250/252 - loss 1.02908807\n",
      "2019-03-18 00:20:16,508 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:20:16,509 EPOCH 28 done: loss 1.0284 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:20:24,566 DEV  : loss 1.43233895 - f-score 0.9654 - acc 0.9333\n",
      "2019-03-18 00:20:42,543 TEST : loss 1.90831208 - f-score 0.9616 - acc 0.9262\n",
      "2019-03-18 00:20:58,002 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:20:58,308 epoch 29 - iter 0/252 - loss 1.65008569\n",
      "2019-03-18 00:21:04,971 epoch 29 - iter 25/252 - loss 1.04385577\n",
      "2019-03-18 00:21:11,699 epoch 29 - iter 50/252 - loss 1.09022510\n",
      "2019-03-18 00:21:18,228 epoch 29 - iter 75/252 - loss 1.07640434\n",
      "2019-03-18 00:21:24,887 epoch 29 - iter 100/252 - loss 1.05000314\n",
      "2019-03-18 00:21:31,566 epoch 29 - iter 125/252 - loss 1.05912608\n",
      "2019-03-18 00:21:38,220 epoch 29 - iter 150/252 - loss 1.02389953\n",
      "2019-03-18 00:21:45,281 epoch 29 - iter 175/252 - loss 1.00600948\n",
      "2019-03-18 00:21:51,930 epoch 29 - iter 200/252 - loss 1.00528296\n",
      "2019-03-18 00:21:58,604 epoch 29 - iter 225/252 - loss 1.02006891\n",
      "2019-03-18 00:22:04,932 epoch 29 - iter 250/252 - loss 1.03071789\n",
      "2019-03-18 00:22:05,114 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:22:05,115 EPOCH 29 done: loss 1.0308 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:22:14,902 DEV  : loss 1.48038483 - f-score 0.9660 - acc 0.9343\n",
      "2019-03-18 00:22:35,438 TEST : loss 1.95100534 - f-score 0.9633 - acc 0.9292\n",
      "2019-03-18 00:22:35,441 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:22:35,643 epoch 30 - iter 0/252 - loss 0.41050035\n",
      "2019-03-18 00:22:41,074 epoch 30 - iter 25/252 - loss 0.88656835\n",
      "2019-03-18 00:22:46,544 epoch 30 - iter 50/252 - loss 0.89683685\n",
      "2019-03-18 00:22:51,952 epoch 30 - iter 75/252 - loss 0.89966343\n",
      "2019-03-18 00:22:57,441 epoch 30 - iter 100/252 - loss 0.90901741\n",
      "2019-03-18 00:23:02,947 epoch 30 - iter 125/252 - loss 0.91203616\n",
      "2019-03-18 00:23:08,529 epoch 30 - iter 150/252 - loss 0.93578533\n",
      "2019-03-18 00:23:13,958 epoch 30 - iter 175/252 - loss 0.93800990\n",
      "2019-03-18 00:23:19,077 epoch 30 - iter 200/252 - loss 0.95158142\n",
      "2019-03-18 00:23:23,305 epoch 30 - iter 225/252 - loss 0.94417546\n",
      "2019-03-18 00:23:28,801 epoch 30 - iter 250/252 - loss 0.97509729\n",
      "2019-03-18 00:23:28,993 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:23:28,994 EPOCH 30 done: loss 0.9745 - lr 0.1000 - bad epochs 1\n",
      "2019-03-18 00:23:38,639 DEV  : loss 1.45861566 - f-score 0.9655 - acc 0.9334\n",
      "2019-03-18 00:24:00,297 TEST : loss 1.97000051 - f-score 0.9626 - acc 0.9280\n",
      "2019-03-18 00:24:16,267 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:24:16,589 epoch 31 - iter 0/252 - loss 1.06642342\n",
      "2019-03-18 00:24:23,059 epoch 31 - iter 25/252 - loss 1.13239718\n",
      "2019-03-18 00:24:29,667 epoch 31 - iter 50/252 - loss 1.04951382\n",
      "2019-03-18 00:24:36,518 epoch 31 - iter 75/252 - loss 1.04007803\n",
      "2019-03-18 00:24:43,350 epoch 31 - iter 100/252 - loss 0.97441802\n",
      "2019-03-18 00:24:49,904 epoch 31 - iter 125/252 - loss 0.96159970\n",
      "2019-03-18 00:24:56,670 epoch 31 - iter 150/252 - loss 0.93789864\n",
      "2019-03-18 00:25:03,548 epoch 31 - iter 175/252 - loss 0.94780765\n",
      "2019-03-18 00:25:09,500 epoch 31 - iter 200/252 - loss 0.94616799\n",
      "2019-03-18 00:25:14,908 epoch 31 - iter 225/252 - loss 0.96126361\n",
      "2019-03-18 00:25:20,379 epoch 31 - iter 250/252 - loss 0.94802504\n",
      "2019-03-18 00:25:20,523 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:25:20,524 EPOCH 31 done: loss 0.9474 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:25:28,552 DEV  : loss 1.54073572 - f-score 0.9650 - acc 0.9323\n",
      "2019-03-18 00:25:46,532 TEST : loss 2.09671092 - f-score 0.9615 - acc 0.9259\n",
      "2019-03-18 00:26:07,743 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:26:08,094 epoch 32 - iter 0/252 - loss 1.47573304\n",
      "2019-03-18 00:26:14,630 epoch 32 - iter 25/252 - loss 0.99259009\n",
      "2019-03-18 00:26:21,504 epoch 32 - iter 50/252 - loss 0.99796774\n",
      "2019-03-18 00:26:28,156 epoch 32 - iter 75/252 - loss 0.91162911\n",
      "2019-03-18 00:26:35,021 epoch 32 - iter 100/252 - loss 0.95840401\n",
      "2019-03-18 00:26:41,643 epoch 32 - iter 125/252 - loss 0.95398005\n",
      "2019-03-18 00:26:48,449 epoch 32 - iter 150/252 - loss 0.98465963\n",
      "2019-03-18 00:26:55,276 epoch 32 - iter 175/252 - loss 0.96555786\n",
      "2019-03-18 00:27:02,004 epoch 32 - iter 200/252 - loss 0.95076562\n",
      "2019-03-18 00:27:08,541 epoch 32 - iter 225/252 - loss 0.95237237\n",
      "2019-03-18 00:27:15,220 epoch 32 - iter 250/252 - loss 0.94174551\n",
      "2019-03-18 00:27:15,389 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:27:15,389 EPOCH 32 done: loss 0.9424 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:27:25,484 DEV  : loss 1.49183416 - f-score 0.9662 - acc 0.9346\n",
      "2019-03-18 00:27:46,593 TEST : loss 2.00248432 - f-score 0.9631 - acc 0.9290\n",
      "2019-03-18 00:28:03,998 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:28:04,576 epoch 33 - iter 0/252 - loss 2.75626183\n",
      "2019-03-18 00:28:10,078 epoch 33 - iter 25/252 - loss 1.17785097\n",
      "2019-03-18 00:28:15,521 epoch 33 - iter 50/252 - loss 1.05426388\n",
      "2019-03-18 00:28:21,005 epoch 33 - iter 75/252 - loss 1.03217001\n",
      "2019-03-18 00:28:25,865 epoch 33 - iter 100/252 - loss 0.99032826\n",
      "2019-03-18 00:28:30,068 epoch 33 - iter 125/252 - loss 0.96506850\n",
      "2019-03-18 00:28:34,503 epoch 33 - iter 150/252 - loss 0.95634265\n",
      "2019-03-18 00:28:41,132 epoch 33 - iter 175/252 - loss 0.93855109\n",
      "2019-03-18 00:28:47,977 epoch 33 - iter 200/252 - loss 0.93331375\n",
      "2019-03-18 00:28:54,664 epoch 33 - iter 225/252 - loss 0.94102968\n",
      "2019-03-18 00:29:01,340 epoch 33 - iter 250/252 - loss 0.94971990\n",
      "2019-03-18 00:29:01,544 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:29:01,545 EPOCH 33 done: loss 0.9494 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:29:11,165 DEV  : loss 1.49773097 - f-score 0.9654 - acc 0.9331\n",
      "2019-03-18 00:29:32,913 TEST : loss 2.03027153 - f-score 0.9625 - acc 0.9279\n",
      "2019-03-18 00:29:32,916 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:29:33,191 epoch 34 - iter 0/252 - loss 0.52143979\n",
      "2019-03-18 00:29:40,032 epoch 34 - iter 25/252 - loss 0.75980935\n",
      "2019-03-18 00:29:46,788 epoch 34 - iter 50/252 - loss 0.79070323\n",
      "2019-03-18 00:29:53,416 epoch 34 - iter 75/252 - loss 0.81078210\n",
      "2019-03-18 00:29:59,858 epoch 34 - iter 100/252 - loss 0.81436598\n",
      "2019-03-18 00:30:06,440 epoch 34 - iter 125/252 - loss 0.84363937\n",
      "2019-03-18 00:30:13,156 epoch 34 - iter 150/252 - loss 0.87261198\n",
      "2019-03-18 00:30:19,692 epoch 34 - iter 175/252 - loss 0.85980951\n",
      "2019-03-18 00:30:25,164 epoch 34 - iter 200/252 - loss 0.90045728\n",
      "2019-03-18 00:30:30,506 epoch 34 - iter 225/252 - loss 0.90412481\n",
      "2019-03-18 00:30:35,927 epoch 34 - iter 250/252 - loss 0.89854401\n",
      "2019-03-18 00:30:36,068 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:30:36,069 EPOCH 34 done: loss 0.9003 - lr 0.1000 - bad epochs 1\n",
      "2019-03-18 00:30:44,018 DEV  : loss 1.48871672 - f-score 0.9675 - acc 0.9370\n",
      "2019-03-18 00:31:01,790 TEST : loss 1.98734570 - f-score 0.9629 - acc 0.9285\n",
      "2019-03-18 00:31:16,198 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:31:18,776 epoch 35 - iter 0/252 - loss 0.35267282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 00:31:25,408 epoch 35 - iter 25/252 - loss 1.11239253\n",
      "2019-03-18 00:31:32,114 epoch 35 - iter 50/252 - loss 1.05585547\n",
      "2019-03-18 00:31:38,839 epoch 35 - iter 75/252 - loss 1.00224012\n",
      "2019-03-18 00:31:45,388 epoch 35 - iter 100/252 - loss 0.94685238\n",
      "2019-03-18 00:31:51,843 epoch 35 - iter 125/252 - loss 0.91714160\n",
      "2019-03-18 00:31:58,185 epoch 35 - iter 150/252 - loss 0.96495555\n",
      "2019-03-18 00:32:04,468 epoch 35 - iter 175/252 - loss 0.95695243\n",
      "2019-03-18 00:32:11,222 epoch 35 - iter 200/252 - loss 0.97198446\n",
      "2019-03-18 00:32:17,885 epoch 35 - iter 225/252 - loss 0.96315103\n",
      "2019-03-18 00:32:24,422 epoch 35 - iter 250/252 - loss 0.95320518\n",
      "2019-03-18 00:32:24,556 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:32:24,565 EPOCH 35 done: loss 0.9530 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:32:34,169 DEV  : loss 1.49393773 - f-score 0.9663 - acc 0.9348\n",
      "2019-03-18 00:32:54,456 TEST : loss 2.03218269 - f-score 0.9624 - acc 0.9276\n",
      "2019-03-18 00:32:54,457 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:32:54,711 epoch 36 - iter 0/252 - loss 0.39508986\n",
      "2019-03-18 00:33:00,075 epoch 36 - iter 25/252 - loss 0.86362922\n",
      "2019-03-18 00:33:05,405 epoch 36 - iter 50/252 - loss 0.84258149\n",
      "2019-03-18 00:33:10,842 epoch 36 - iter 75/252 - loss 0.85391855\n",
      "2019-03-18 00:33:16,226 epoch 36 - iter 100/252 - loss 0.88803058\n",
      "2019-03-18 00:33:21,873 epoch 36 - iter 125/252 - loss 0.91755292\n",
      "2019-03-18 00:33:27,264 epoch 36 - iter 150/252 - loss 0.89324463\n",
      "2019-03-18 00:33:32,892 epoch 36 - iter 175/252 - loss 0.87892782\n",
      "2019-03-18 00:33:37,994 epoch 36 - iter 200/252 - loss 0.87506272\n",
      "2019-03-18 00:33:42,153 epoch 36 - iter 225/252 - loss 0.89995081\n",
      "2019-03-18 00:33:47,355 epoch 36 - iter 250/252 - loss 0.90317182\n",
      "2019-03-18 00:33:47,488 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:33:47,489 EPOCH 36 done: loss 0.9025 - lr 0.1000 - bad epochs 1\n",
      "2019-03-18 00:33:57,273 DEV  : loss 1.58572078 - f-score 0.9665 - acc 0.9352\n",
      "2019-03-18 00:34:19,668 TEST : loss 2.12420106 - f-score 0.9629 - acc 0.9284\n",
      "2019-03-18 00:34:19,670 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:34:19,997 epoch 37 - iter 0/252 - loss 0.27592504\n",
      "2019-03-18 00:34:26,621 epoch 37 - iter 25/252 - loss 0.69671973\n",
      "2019-03-18 00:34:33,361 epoch 37 - iter 50/252 - loss 0.72432690\n",
      "2019-03-18 00:34:40,030 epoch 37 - iter 75/252 - loss 0.76537304\n",
      "2019-03-18 00:34:46,663 epoch 37 - iter 100/252 - loss 0.76445646\n",
      "2019-03-18 00:34:53,347 epoch 37 - iter 125/252 - loss 0.77891063\n",
      "2019-03-18 00:34:59,691 epoch 37 - iter 150/252 - loss 0.77584991\n",
      "2019-03-18 00:35:06,347 epoch 37 - iter 175/252 - loss 0.80803481\n",
      "2019-03-18 00:35:13,074 epoch 37 - iter 200/252 - loss 0.83204139\n",
      "2019-03-18 00:35:19,884 epoch 37 - iter 225/252 - loss 0.83745043\n",
      "2019-03-18 00:35:26,450 epoch 37 - iter 250/252 - loss 0.85756484\n",
      "2019-03-18 00:35:26,674 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:35:26,674 EPOCH 37 done: loss 0.8570 - lr 0.1000 - bad epochs 2\n",
      "2019-03-18 00:35:35,727 DEV  : loss 1.55155647 - f-score 0.9659 - acc 0.9341\n",
      "2019-03-18 00:35:53,623 TEST : loss 2.06851077 - f-score 0.9638 - acc 0.9303\n",
      "2019-03-18 00:36:11,098 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:36:11,404 epoch 38 - iter 0/252 - loss 0.70065856\n",
      "2019-03-18 00:36:18,075 epoch 38 - iter 25/252 - loss 0.85115614\n",
      "2019-03-18 00:36:24,801 epoch 38 - iter 50/252 - loss 0.95311233\n",
      "2019-03-18 00:36:31,578 epoch 38 - iter 75/252 - loss 0.93516704\n",
      "2019-03-18 00:36:38,115 epoch 38 - iter 100/252 - loss 0.89055648\n",
      "2019-03-18 00:36:44,801 epoch 38 - iter 125/252 - loss 0.90406828\n",
      "2019-03-18 00:36:51,327 epoch 38 - iter 150/252 - loss 0.91002426\n",
      "2019-03-18 00:36:57,685 epoch 38 - iter 175/252 - loss 0.89658311\n",
      "2019-03-18 00:37:04,527 epoch 38 - iter 200/252 - loss 0.90352670\n",
      "2019-03-18 00:37:11,044 epoch 38 - iter 225/252 - loss 0.90087042\n",
      "2019-03-18 00:37:17,779 epoch 38 - iter 250/252 - loss 0.89017900\n",
      "2019-03-18 00:37:17,973 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:37:17,975 EPOCH 38 done: loss 0.8897 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:37:27,789 DEV  : loss 1.53618479 - f-score 0.9659 - acc 0.9341\n",
      "2019-03-18 00:37:49,847 TEST : loss 2.10453773 - f-score 0.9640 - acc 0.9305\n",
      "2019-03-18 00:37:49,849 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:37:50,146 epoch 39 - iter 0/252 - loss 0.89174581\n",
      "2019-03-18 00:37:56,728 epoch 39 - iter 25/252 - loss 0.95654105\n",
      "2019-03-18 00:38:02,148 epoch 39 - iter 50/252 - loss 0.83137862\n",
      "2019-03-18 00:38:07,494 epoch 39 - iter 75/252 - loss 0.80493506\n",
      "2019-03-18 00:38:13,010 epoch 39 - iter 100/252 - loss 0.82820094\n",
      "2019-03-18 00:38:18,465 epoch 39 - iter 125/252 - loss 0.83325231\n",
      "2019-03-18 00:38:23,848 epoch 39 - iter 150/252 - loss 0.80089920\n",
      "2019-03-18 00:38:29,505 epoch 39 - iter 175/252 - loss 0.81880197\n",
      "2019-03-18 00:38:34,564 epoch 39 - iter 200/252 - loss 0.80538466\n",
      "2019-03-18 00:38:40,044 epoch 39 - iter 225/252 - loss 0.83305023\n",
      "2019-03-18 00:38:45,553 epoch 39 - iter 250/252 - loss 0.81749109\n",
      "2019-03-18 00:38:45,738 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:38:45,739 EPOCH 39 done: loss 0.8181 - lr 0.1000 - bad epochs 1\n",
      "2019-03-18 00:38:51,514 DEV  : loss 1.55039716 - f-score 0.9654 - acc 0.9332\n",
      "2019-03-18 00:39:12,815 TEST : loss 2.13176775 - f-score 0.9635 - acc 0.9297\n",
      "2019-03-18 00:39:28,734 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:39:29,003 epoch 40 - iter 0/252 - loss 0.98945808\n",
      "2019-03-18 00:39:35,748 epoch 40 - iter 25/252 - loss 0.90096322\n",
      "2019-03-18 00:39:42,438 epoch 40 - iter 50/252 - loss 0.84745792\n",
      "2019-03-18 00:39:49,032 epoch 40 - iter 75/252 - loss 0.83465238\n",
      "2019-03-18 00:39:55,804 epoch 40 - iter 100/252 - loss 0.79435829\n",
      "2019-03-18 00:40:02,649 epoch 40 - iter 125/252 - loss 0.78689230\n",
      "2019-03-18 00:40:09,329 epoch 40 - iter 150/252 - loss 0.78402261\n",
      "2019-03-18 00:40:16,073 epoch 40 - iter 175/252 - loss 0.77126666\n",
      "2019-03-18 00:40:22,745 epoch 40 - iter 200/252 - loss 0.77041359\n",
      "2019-03-18 00:40:29,347 epoch 40 - iter 225/252 - loss 0.79027355\n",
      "2019-03-18 00:40:36,072 epoch 40 - iter 250/252 - loss 0.78690742\n",
      "2019-03-18 00:40:36,178 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:40:36,179 EPOCH 40 done: loss 0.7866 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:40:43,663 DEV  : loss 1.51970220 - f-score 0.9667 - acc 0.9356\n",
      "2019-03-18 00:41:01,195 TEST : loss 2.07748055 - f-score 0.9643 - acc 0.9311\n",
      "2019-03-18 00:41:17,178 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:41:17,502 epoch 41 - iter 0/252 - loss 1.20069075\n",
      "2019-03-18 00:41:24,068 epoch 41 - iter 25/252 - loss 0.80555858\n",
      "2019-03-18 00:41:30,722 epoch 41 - iter 50/252 - loss 0.81831062\n",
      "2019-03-18 00:41:37,380 epoch 41 - iter 75/252 - loss 0.77034590\n",
      "2019-03-18 00:41:43,924 epoch 41 - iter 100/252 - loss 0.74828282\n",
      "2019-03-18 00:41:50,709 epoch 41 - iter 125/252 - loss 0.74131791\n",
      "2019-03-18 00:41:57,628 epoch 41 - iter 150/252 - loss 0.73377751\n",
      "2019-03-18 00:42:04,445 epoch 41 - iter 175/252 - loss 0.75406330\n",
      "2019-03-18 00:42:10,990 epoch 41 - iter 200/252 - loss 0.75423810\n",
      "2019-03-18 00:42:17,501 epoch 41 - iter 225/252 - loss 0.75977734\n",
      "2019-03-18 00:42:24,101 epoch 41 - iter 250/252 - loss 0.77129288\n",
      "2019-03-18 00:42:24,256 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:42:24,257 EPOCH 41 done: loss 0.7707 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:42:33,929 DEV  : loss 1.57115448 - f-score 0.9670 - acc 0.9362\n",
      "2019-03-18 00:42:55,753 TEST : loss 2.11133027 - f-score 0.9635 - acc 0.9296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 00:43:11,454 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:43:11,686 epoch 42 - iter 0/252 - loss 1.05445588\n",
      "2019-03-18 00:43:16,905 epoch 42 - iter 25/252 - loss 0.89435651\n",
      "2019-03-18 00:43:22,201 epoch 42 - iter 50/252 - loss 0.88072330\n",
      "2019-03-18 00:43:27,811 epoch 42 - iter 75/252 - loss 0.83584705\n",
      "2019-03-18 00:43:33,128 epoch 42 - iter 100/252 - loss 0.84543009\n",
      "2019-03-18 00:43:38,620 epoch 42 - iter 125/252 - loss 0.86543511\n",
      "2019-03-18 00:43:44,173 epoch 42 - iter 150/252 - loss 0.84524968\n",
      "2019-03-18 00:43:50,594 epoch 42 - iter 175/252 - loss 0.83855304\n",
      "2019-03-18 00:43:57,344 epoch 42 - iter 200/252 - loss 0.84644408\n",
      "2019-03-18 00:44:03,969 epoch 42 - iter 225/252 - loss 0.83253302\n",
      "2019-03-18 00:44:10,584 epoch 42 - iter 250/252 - loss 0.82305509\n",
      "2019-03-18 00:44:10,745 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:44:10,746 EPOCH 42 done: loss 0.8223 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:44:20,568 DEV  : loss 1.64990306 - f-score 0.9647 - acc 0.9318\n",
      "2019-03-18 00:44:42,217 TEST : loss 2.27058721 - f-score 0.9627 - acc 0.9281\n",
      "2019-03-18 00:44:42,219 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:44:42,487 epoch 43 - iter 0/252 - loss 0.74963593\n",
      "2019-03-18 00:44:49,191 epoch 43 - iter 25/252 - loss 0.81045148\n",
      "2019-03-18 00:44:55,575 epoch 43 - iter 50/252 - loss 0.85976928\n",
      "2019-03-18 00:45:02,011 epoch 43 - iter 75/252 - loss 0.87406105\n",
      "2019-03-18 00:45:08,876 epoch 43 - iter 100/252 - loss 0.82388304\n",
      "2019-03-18 00:45:15,756 epoch 43 - iter 125/252 - loss 0.80831983\n",
      "2019-03-18 00:45:22,316 epoch 43 - iter 150/252 - loss 0.80027739\n",
      "2019-03-18 00:45:28,980 epoch 43 - iter 175/252 - loss 0.81025430\n",
      "2019-03-18 00:45:35,180 epoch 43 - iter 200/252 - loss 0.79904381\n",
      "2019-03-18 00:45:40,457 epoch 43 - iter 225/252 - loss 0.78495618\n",
      "2019-03-18 00:45:45,723 epoch 43 - iter 250/252 - loss 0.78284894\n",
      "2019-03-18 00:45:45,866 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:45:45,867 EPOCH 43 done: loss 0.7820 - lr 0.1000 - bad epochs 1\n",
      "2019-03-18 00:45:53,985 DEV  : loss 1.65912998 - f-score 0.9647 - acc 0.9319\n",
      "2019-03-18 00:46:11,875 TEST : loss 2.21242070 - f-score 0.9633 - acc 0.9292\n",
      "2019-03-18 00:46:11,876 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:46:12,137 epoch 44 - iter 0/252 - loss 0.92402625\n",
      "2019-03-18 00:46:17,633 epoch 44 - iter 25/252 - loss 0.65145814\n",
      "2019-03-18 00:46:22,335 epoch 44 - iter 50/252 - loss 0.74221243\n",
      "2019-03-18 00:46:28,354 epoch 44 - iter 75/252 - loss 0.72687528\n",
      "2019-03-18 00:46:34,817 epoch 44 - iter 100/252 - loss 0.76715730\n",
      "2019-03-18 00:46:41,571 epoch 44 - iter 125/252 - loss 0.76882148\n",
      "2019-03-18 00:46:48,395 epoch 44 - iter 150/252 - loss 0.77850896\n",
      "2019-03-18 00:46:54,943 epoch 44 - iter 175/252 - loss 0.77254116\n",
      "2019-03-18 00:47:01,588 epoch 44 - iter 200/252 - loss 0.74858112\n",
      "2019-03-18 00:47:08,413 epoch 44 - iter 225/252 - loss 0.74175279\n",
      "2019-03-18 00:47:15,094 epoch 44 - iter 250/252 - loss 0.73486900\n",
      "2019-03-18 00:47:15,292 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:47:15,292 EPOCH 44 done: loss 0.7344 - lr 0.1000 - bad epochs 2\n",
      "2019-03-18 00:47:25,070 DEV  : loss 1.58081138 - f-score 0.9676 - acc 0.9372\n",
      "2019-03-18 00:47:47,326 TEST : loss 2.21954179 - f-score 0.9637 - acc 0.9300\n",
      "2019-03-18 00:48:01,068 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:48:03,690 epoch 45 - iter 0/252 - loss 0.13789415\n",
      "2019-03-18 00:48:09,109 epoch 45 - iter 25/252 - loss 0.69515301\n",
      "2019-03-18 00:48:14,470 epoch 45 - iter 50/252 - loss 0.69083937\n",
      "2019-03-18 00:48:19,801 epoch 45 - iter 75/252 - loss 0.65008440\n",
      "2019-03-18 00:48:25,378 epoch 45 - iter 100/252 - loss 0.65705959\n",
      "2019-03-18 00:48:30,871 epoch 45 - iter 125/252 - loss 0.71422665\n",
      "2019-03-18 00:48:36,363 epoch 45 - iter 150/252 - loss 0.71516119\n",
      "2019-03-18 00:48:41,978 epoch 45 - iter 175/252 - loss 0.71296771\n",
      "2019-03-18 00:48:47,483 epoch 45 - iter 200/252 - loss 0.71755681\n",
      "2019-03-18 00:48:52,843 epoch 45 - iter 225/252 - loss 0.72746547\n",
      "2019-03-18 00:48:58,688 epoch 45 - iter 250/252 - loss 0.73419798\n",
      "2019-03-18 00:48:58,827 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:48:58,828 EPOCH 45 done: loss 0.7362 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:49:08,739 DEV  : loss 1.63558698 - f-score 0.9648 - acc 0.9320\n",
      "2019-03-18 00:49:30,945 TEST : loss 2.19314551 - f-score 0.9638 - acc 0.9302\n",
      "2019-03-18 00:49:30,947 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:49:31,181 epoch 46 - iter 0/252 - loss 0.35868621\n",
      "2019-03-18 00:49:37,954 epoch 46 - iter 25/252 - loss 0.69360632\n",
      "2019-03-18 00:49:44,478 epoch 46 - iter 50/252 - loss 0.70891339\n",
      "2019-03-18 00:49:51,139 epoch 46 - iter 75/252 - loss 0.71581663\n",
      "2019-03-18 00:49:58,059 epoch 46 - iter 100/252 - loss 0.72084930\n",
      "2019-03-18 00:50:04,693 epoch 46 - iter 125/252 - loss 0.67765058\n",
      "2019-03-18 00:50:11,318 epoch 46 - iter 150/252 - loss 0.68457506\n",
      "2019-03-18 00:50:17,978 epoch 46 - iter 175/252 - loss 0.67620952\n",
      "2019-03-18 00:50:24,581 epoch 46 - iter 200/252 - loss 0.69027628\n",
      "2019-03-18 00:50:31,332 epoch 46 - iter 225/252 - loss 0.71860254\n",
      "2019-03-18 00:50:37,691 epoch 46 - iter 250/252 - loss 0.73277912\n",
      "2019-03-18 00:50:37,885 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:50:37,886 EPOCH 46 done: loss 0.7321 - lr 0.1000 - bad epochs 1\n",
      "2019-03-18 00:50:46,380 DEV  : loss 1.56965637 - f-score 0.9658 - acc 0.9338\n",
      "2019-03-18 00:51:04,391 TEST : loss 2.20266461 - f-score 0.9618 - acc 0.9264\n",
      "2019-03-18 00:51:18,945 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:51:20,564 epoch 47 - iter 0/252 - loss 0.80778623\n",
      "2019-03-18 00:51:24,927 epoch 47 - iter 25/252 - loss 0.75485472\n",
      "2019-03-18 00:51:30,309 epoch 47 - iter 50/252 - loss 0.73430616\n",
      "2019-03-18 00:51:37,109 epoch 47 - iter 75/252 - loss 0.74678021\n",
      "2019-03-18 00:51:43,925 epoch 47 - iter 100/252 - loss 0.78139439\n",
      "2019-03-18 00:51:50,573 epoch 47 - iter 125/252 - loss 0.77528228\n",
      "2019-03-18 00:51:57,201 epoch 47 - iter 150/252 - loss 0.74821205\n",
      "2019-03-18 00:52:03,770 epoch 47 - iter 175/252 - loss 0.74995575\n",
      "2019-03-18 00:52:10,300 epoch 47 - iter 200/252 - loss 0.73585693\n",
      "2019-03-18 00:52:16,887 epoch 47 - iter 225/252 - loss 0.73445756\n",
      "2019-03-18 00:52:23,383 epoch 47 - iter 250/252 - loss 0.73302140\n",
      "2019-03-18 00:52:23,574 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:52:23,575 EPOCH 47 done: loss 0.7326 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:52:33,250 DEV  : loss 1.66002131 - f-score 0.9661 - acc 0.9344\n",
      "2019-03-18 00:52:55,071 TEST : loss 2.21697569 - f-score 0.9632 - acc 0.9291\n",
      "2019-03-18 00:52:55,072 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:52:55,317 epoch 48 - iter 0/252 - loss 0.44467974\n",
      "2019-03-18 00:53:01,930 epoch 48 - iter 25/252 - loss 0.66770027\n",
      "2019-03-18 00:53:08,224 epoch 48 - iter 50/252 - loss 0.71671990\n",
      "2019-03-18 00:53:15,103 epoch 48 - iter 75/252 - loss 0.73313434\n",
      "2019-03-18 00:53:20,006 epoch 48 - iter 100/252 - loss 0.72745739\n",
      "2019-03-18 00:53:25,306 epoch 48 - iter 125/252 - loss 0.72665196\n",
      "2019-03-18 00:53:30,827 epoch 48 - iter 150/252 - loss 0.74116243\n",
      "2019-03-18 00:53:36,441 epoch 48 - iter 175/252 - loss 0.73694537\n",
      "2019-03-18 00:53:41,961 epoch 48 - iter 200/252 - loss 0.74424106\n",
      "2019-03-18 00:53:47,487 epoch 48 - iter 225/252 - loss 0.77129681\n",
      "2019-03-18 00:53:52,952 epoch 48 - iter 250/252 - loss 0.76493361\n",
      "2019-03-18 00:53:53,150 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:53:53,151 EPOCH 48 done: loss 0.7660 - lr 0.1000 - bad epochs 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 00:54:01,017 DEV  : loss 1.67239141 - f-score 0.9647 - acc 0.9319\n",
      "2019-03-18 00:54:16,909 TEST : loss 2.18771577 - f-score 0.9630 - acc 0.9286\n",
      "2019-03-18 00:54:16,911 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:54:17,160 epoch 49 - iter 0/252 - loss 0.67611146\n",
      "2019-03-18 00:54:23,857 epoch 49 - iter 25/252 - loss 0.85158196\n",
      "2019-03-18 00:54:30,433 epoch 49 - iter 50/252 - loss 0.74674031\n",
      "2019-03-18 00:54:37,249 epoch 49 - iter 75/252 - loss 0.69522890\n",
      "2019-03-18 00:54:43,739 epoch 49 - iter 100/252 - loss 0.68219233\n",
      "2019-03-18 00:54:50,712 epoch 49 - iter 125/252 - loss 0.67266784\n",
      "2019-03-18 00:54:57,602 epoch 49 - iter 150/252 - loss 0.67155901\n",
      "2019-03-18 00:55:04,339 epoch 49 - iter 175/252 - loss 0.67334650\n",
      "2019-03-18 00:55:11,033 epoch 49 - iter 200/252 - loss 0.67255216\n",
      "2019-03-18 00:55:17,571 epoch 49 - iter 225/252 - loss 0.66766674\n",
      "2019-03-18 00:55:24,483 epoch 49 - iter 250/252 - loss 0.67734847\n",
      "2019-03-18 00:55:24,631 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:55:24,632 EPOCH 49 done: loss 0.6768 - lr 0.1000 - bad epochs 2\n",
      "2019-03-18 00:55:34,316 DEV  : loss 1.66097331 - f-score 0.9659 - acc 0.9342\n",
      "2019-03-18 00:55:56,329 TEST : loss 2.25000596 - f-score 0.9635 - acc 0.9296\n",
      "2019-03-18 00:56:12,199 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:56:12,897 epoch 50 - iter 0/252 - loss 1.03204489\n",
      "2019-03-18 00:56:18,497 epoch 50 - iter 25/252 - loss 0.64342752\n",
      "2019-03-18 00:56:23,894 epoch 50 - iter 50/252 - loss 0.60340229\n",
      "2019-03-18 00:56:29,405 epoch 50 - iter 75/252 - loss 0.61801249\n",
      "2019-03-18 00:56:34,950 epoch 50 - iter 100/252 - loss 0.64126295\n",
      "2019-03-18 00:56:40,583 epoch 50 - iter 125/252 - loss 0.63362508\n",
      "2019-03-18 00:56:47,346 epoch 50 - iter 150/252 - loss 0.62956176\n",
      "2019-03-18 00:56:54,076 epoch 50 - iter 175/252 - loss 0.63801824\n",
      "2019-03-18 00:57:00,845 epoch 50 - iter 200/252 - loss 0.64917080\n",
      "2019-03-18 00:57:07,612 epoch 50 - iter 225/252 - loss 0.66465167\n",
      "2019-03-18 00:57:14,313 epoch 50 - iter 250/252 - loss 0.67498474\n",
      "2019-03-18 00:57:14,512 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:57:14,513 EPOCH 50 done: loss 0.6752 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:57:24,244 DEV  : loss 1.62071717 - f-score 0.9643 - acc 0.9311\n",
      "2019-03-18 00:57:46,395 TEST : loss 2.18178916 - f-score 0.9633 - acc 0.9293\n",
      "2019-03-18 00:58:02,535 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:58:02,954 epoch 51 - iter 0/252 - loss 0.68933940\n",
      "2019-03-18 00:58:09,650 epoch 51 - iter 25/252 - loss 0.64857912\n",
      "2019-03-18 00:58:16,465 epoch 51 - iter 50/252 - loss 0.70502390\n",
      "2019-03-18 00:58:21,974 epoch 51 - iter 75/252 - loss 0.72842615\n",
      "2019-03-18 00:58:27,274 epoch 51 - iter 100/252 - loss 0.74069502\n",
      "2019-03-18 00:58:32,664 epoch 51 - iter 125/252 - loss 0.74295793\n",
      "2019-03-18 00:58:38,181 epoch 51 - iter 150/252 - loss 0.73043126\n",
      "2019-03-18 00:58:43,735 epoch 51 - iter 175/252 - loss 0.75568872\n",
      "2019-03-18 00:58:49,257 epoch 51 - iter 200/252 - loss 0.72140225\n",
      "2019-03-18 00:58:54,678 epoch 51 - iter 225/252 - loss 0.72167800\n",
      "2019-03-18 00:59:00,233 epoch 51 - iter 250/252 - loss 0.72378600\n",
      "2019-03-18 00:59:00,378 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:59:00,379 EPOCH 51 done: loss 0.7233 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 00:59:07,705 DEV  : loss 1.69237781 - f-score 0.9645 - acc 0.9315\n",
      "2019-03-18 00:59:26,358 TEST : loss 2.24924397 - f-score 0.9629 - acc 0.9285\n",
      "2019-03-18 00:59:26,360 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 00:59:26,671 epoch 52 - iter 0/252 - loss 0.76534176\n",
      "2019-03-18 00:59:33,305 epoch 52 - iter 25/252 - loss 0.77078033\n",
      "2019-03-18 00:59:39,802 epoch 52 - iter 50/252 - loss 0.68091727\n",
      "2019-03-18 00:59:46,415 epoch 52 - iter 75/252 - loss 0.63167021\n",
      "2019-03-18 00:59:53,351 epoch 52 - iter 100/252 - loss 0.65598358\n",
      "2019-03-18 00:59:59,981 epoch 52 - iter 125/252 - loss 0.66728368\n",
      "2019-03-18 01:00:06,667 epoch 52 - iter 150/252 - loss 0.68114441\n",
      "2019-03-18 01:00:13,413 epoch 52 - iter 175/252 - loss 0.69225950\n",
      "2019-03-18 01:00:19,862 epoch 52 - iter 200/252 - loss 0.69848289\n",
      "2019-03-18 01:00:26,787 epoch 52 - iter 225/252 - loss 0.69744328\n",
      "2019-03-18 01:00:33,319 epoch 52 - iter 250/252 - loss 0.69750602\n",
      "2019-03-18 01:00:33,462 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:00:33,462 EPOCH 52 done: loss 0.6977 - lr 0.1000 - bad epochs 1\n",
      "2019-03-18 01:00:43,384 DEV  : loss 1.66807640 - f-score 0.9650 - acc 0.9325\n",
      "2019-03-18 01:01:04,517 TEST : loss 2.29467058 - f-score 0.9618 - acc 0.9264\n",
      "2019-03-18 01:01:04,519 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:01:04,764 epoch 53 - iter 0/252 - loss 0.44640541\n",
      "2019-03-18 01:01:10,310 epoch 53 - iter 25/252 - loss 0.69981329\n",
      "2019-03-18 01:01:15,866 epoch 53 - iter 50/252 - loss 0.77972002\n",
      "2019-03-18 01:01:21,382 epoch 53 - iter 75/252 - loss 0.72646110\n",
      "2019-03-18 01:01:26,987 epoch 53 - iter 100/252 - loss 0.71558894\n",
      "2019-03-18 01:01:32,430 epoch 53 - iter 125/252 - loss 0.69654630\n",
      "2019-03-18 01:01:37,722 epoch 53 - iter 150/252 - loss 0.69513737\n",
      "2019-03-18 01:01:43,164 epoch 53 - iter 175/252 - loss 0.69451404\n",
      "2019-03-18 01:01:48,577 epoch 53 - iter 200/252 - loss 0.70113894\n",
      "2019-03-18 01:01:55,057 epoch 53 - iter 225/252 - loss 0.69652468\n",
      "2019-03-18 01:02:01,692 epoch 53 - iter 250/252 - loss 0.70004828\n",
      "2019-03-18 01:02:01,867 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:02:01,868 EPOCH 53 done: loss 0.7005 - lr 0.1000 - bad epochs 2\n",
      "2019-03-18 01:02:11,692 DEV  : loss 1.71170962 - f-score 0.9657 - acc 0.9337\n",
      "2019-03-18 01:02:34,174 TEST : loss 2.29948592 - f-score 0.9629 - acc 0.9286\n",
      "2019-03-18 01:02:34,175 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:02:34,417 epoch 54 - iter 0/252 - loss 0.37162155\n",
      "2019-03-18 01:02:41,053 epoch 54 - iter 25/252 - loss 0.61644186\n",
      "2019-03-18 01:02:47,677 epoch 54 - iter 50/252 - loss 0.63195002\n",
      "2019-03-18 01:02:54,447 epoch 54 - iter 75/252 - loss 0.61088652\n",
      "2019-03-18 01:03:01,068 epoch 54 - iter 100/252 - loss 0.63700177\n",
      "2019-03-18 01:03:07,935 epoch 54 - iter 125/252 - loss 0.66336370\n",
      "2019-03-18 01:03:14,822 epoch 54 - iter 150/252 - loss 0.64517025\n",
      "2019-03-18 01:03:21,451 epoch 54 - iter 175/252 - loss 0.66946180\n",
      "2019-03-18 01:03:28,172 epoch 54 - iter 200/252 - loss 0.68665960\n",
      "2019-03-18 01:03:34,824 epoch 54 - iter 225/252 - loss 0.68162194\n",
      "2019-03-18 01:03:39,881 epoch 54 - iter 250/252 - loss 0.66574501\n",
      "2019-03-18 01:03:39,990 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:03:39,991 EPOCH 54 done: loss 0.6658 - lr 0.1000 - bad epochs 3\n",
      "2019-03-18 01:03:47,394 DEV  : loss 1.71273041 - f-score 0.9660 - acc 0.9342\n",
      "2019-03-18 01:04:05,015 TEST : loss 2.30996656 - f-score 0.9642 - acc 0.9310\n",
      "2019-03-18 01:04:24,370 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:04:24,661 epoch 55 - iter 0/252 - loss 0.49974048\n",
      "2019-03-18 01:04:31,368 epoch 55 - iter 25/252 - loss 0.62550855\n",
      "2019-03-18 01:04:38,021 epoch 55 - iter 50/252 - loss 0.62935818\n",
      "2019-03-18 01:04:44,461 epoch 55 - iter 75/252 - loss 0.60525673\n",
      "2019-03-18 01:04:51,397 epoch 55 - iter 100/252 - loss 0.64797423\n",
      "2019-03-18 01:04:58,097 epoch 55 - iter 125/252 - loss 0.64885933\n",
      "2019-03-18 01:05:04,756 epoch 55 - iter 150/252 - loss 0.64605762\n",
      "2019-03-18 01:05:11,654 epoch 55 - iter 175/252 - loss 0.63899978\n",
      "2019-03-18 01:05:18,166 epoch 55 - iter 200/252 - loss 0.64185030\n",
      "2019-03-18 01:05:24,897 epoch 55 - iter 225/252 - loss 0.64343633\n",
      "2019-03-18 01:05:31,506 epoch 55 - iter 250/252 - loss 0.63062608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 01:05:31,664 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:05:31,665 EPOCH 55 done: loss 0.6314 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 01:05:41,536 DEV  : loss 1.75118661 - f-score 0.9654 - acc 0.9332\n",
      "2019-03-18 01:06:03,206 TEST : loss 2.37548447 - f-score 0.9629 - acc 0.9285\n",
      "2019-03-18 01:06:21,490 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:06:21,874 epoch 56 - iter 0/252 - loss 0.48036337\n",
      "2019-03-18 01:06:27,340 epoch 56 - iter 25/252 - loss 0.59999600\n",
      "2019-03-18 01:06:32,945 epoch 56 - iter 50/252 - loss 0.61787825\n",
      "2019-03-18 01:06:38,303 epoch 56 - iter 75/252 - loss 0.59671608\n",
      "2019-03-18 01:06:43,782 epoch 56 - iter 100/252 - loss 0.57741624\n",
      "2019-03-18 01:06:50,525 epoch 56 - iter 125/252 - loss 0.61067082\n",
      "2019-03-18 01:06:57,127 epoch 56 - iter 150/252 - loss 0.62778594\n",
      "2019-03-18 01:07:03,714 epoch 56 - iter 175/252 - loss 0.65319860\n",
      "2019-03-18 01:07:10,482 epoch 56 - iter 200/252 - loss 0.64855190\n",
      "2019-03-18 01:07:17,073 epoch 56 - iter 225/252 - loss 0.64685594\n",
      "2019-03-18 01:07:23,611 epoch 56 - iter 250/252 - loss 0.65527716\n",
      "2019-03-18 01:07:23,775 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:07:23,776 EPOCH 56 done: loss 0.6552 - lr 0.1000 - bad epochs 0\n",
      "2019-03-18 01:07:33,646 DEV  : loss 1.76778269 - f-score 0.9666 - acc 0.9353\n",
      "2019-03-18 01:07:55,988 TEST : loss 2.42900896 - f-score 0.9619 - acc 0.9267\n",
      "2019-03-18 01:07:55,990 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:07:56,284 epoch 57 - iter 0/252 - loss 0.64609611\n",
      "2019-03-18 01:08:02,744 epoch 57 - iter 25/252 - loss 0.60162864\n",
      "2019-03-18 01:08:09,596 epoch 57 - iter 50/252 - loss 0.63295615\n",
      "2019-03-18 01:08:16,171 epoch 57 - iter 75/252 - loss 0.66453565\n",
      "2019-03-18 01:08:22,677 epoch 57 - iter 100/252 - loss 0.65891045\n",
      "2019-03-18 01:08:29,635 epoch 57 - iter 125/252 - loss 0.65540359\n",
      "2019-03-18 01:08:35,357 epoch 57 - iter 150/252 - loss 0.66854625\n",
      "2019-03-18 01:08:40,922 epoch 57 - iter 175/252 - loss 0.66578539\n",
      "2019-03-18 01:08:46,406 epoch 57 - iter 200/252 - loss 0.66291397\n",
      "2019-03-18 01:08:52,378 epoch 57 - iter 225/252 - loss 0.65253722\n",
      "2019-03-18 01:08:57,833 epoch 57 - iter 250/252 - loss 0.66809176\n",
      "2019-03-18 01:08:57,991 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:08:57,991 EPOCH 57 done: loss 0.6680 - lr 0.1000 - bad epochs 1\n",
      "2019-03-18 01:09:05,705 DEV  : loss 1.70436358 - f-score 0.9678 - acc 0.9376\n",
      "2019-03-18 01:09:24,699 TEST : loss 2.33353305 - f-score 0.9623 - acc 0.9275\n",
      "2019-03-18 01:09:24,700 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:09:25,016 epoch 58 - iter 0/252 - loss 0.81056070\n",
      "2019-03-18 01:09:31,740 epoch 58 - iter 25/252 - loss 0.67304787\n",
      "2019-03-18 01:09:38,263 epoch 58 - iter 50/252 - loss 0.75836494\n",
      "2019-03-18 01:09:45,033 epoch 58 - iter 75/252 - loss 0.68855489\n",
      "2019-03-18 01:09:51,153 epoch 58 - iter 100/252 - loss 0.68698820\n",
      "2019-03-18 01:09:57,986 epoch 58 - iter 125/252 - loss 0.66851978\n",
      "2019-03-18 01:10:04,625 epoch 58 - iter 150/252 - loss 0.67466552\n",
      "2019-03-18 01:10:11,203 epoch 58 - iter 175/252 - loss 0.68625277\n",
      "2019-03-18 01:10:17,981 epoch 58 - iter 200/252 - loss 0.68393392\n",
      "2019-03-18 01:10:24,711 epoch 58 - iter 225/252 - loss 0.70460219\n",
      "2019-03-18 01:10:31,523 epoch 58 - iter 250/252 - loss 0.69296816\n",
      "2019-03-18 01:10:31,714 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:10:31,715 EPOCH 58 done: loss 0.6924 - lr 0.1000 - bad epochs 2\n",
      "2019-03-18 01:10:41,591 DEV  : loss 1.75082469 - f-score 0.9662 - acc 0.9347\n",
      "2019-03-18 01:11:03,773 TEST : loss 2.31575489 - f-score 0.9625 - acc 0.9277\n",
      "2019-03-18 01:11:03,775 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:11:03,985 epoch 59 - iter 0/252 - loss 2.51535463\n",
      "2019-03-18 01:11:09,434 epoch 59 - iter 25/252 - loss 0.80398642\n",
      "2019-03-18 01:11:14,700 epoch 59 - iter 50/252 - loss 0.70645373\n",
      "2019-03-18 01:11:20,174 epoch 59 - iter 75/252 - loss 0.70958981\n",
      "2019-03-18 01:11:25,834 epoch 59 - iter 100/252 - loss 0.68188314\n",
      "2019-03-18 01:11:31,330 epoch 59 - iter 125/252 - loss 0.65394284\n",
      "2019-03-18 01:11:36,790 epoch 59 - iter 150/252 - loss 0.63779701\n",
      "2019-03-18 01:11:42,257 epoch 59 - iter 175/252 - loss 0.63548984\n",
      "2019-03-18 01:11:47,658 epoch 59 - iter 200/252 - loss 0.63307338\n",
      "2019-03-18 01:11:52,986 epoch 59 - iter 225/252 - loss 0.63446563\n",
      "2019-03-18 01:11:57,214 epoch 59 - iter 250/252 - loss 0.64151348\n",
      "2019-03-18 01:11:57,384 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:11:57,384 EPOCH 59 done: loss 0.6408 - lr 0.1000 - bad epochs 3\n",
      "2019-03-18 01:12:03,855 DEV  : loss 1.74953640 - f-score 0.9663 - acc 0.9349\n",
      "2019-03-18 01:12:26,171 TEST : loss 2.34548116 - f-score 0.9632 - acc 0.9292\n",
      "Epoch    58: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2019-03-18 01:12:26,172 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:12:26,451 epoch 60 - iter 0/252 - loss 0.23523092\n",
      "2019-03-18 01:12:33,273 epoch 60 - iter 25/252 - loss 0.54463035\n",
      "2019-03-18 01:12:39,863 epoch 60 - iter 50/252 - loss 0.56467376\n",
      "2019-03-18 01:12:46,835 epoch 60 - iter 75/252 - loss 0.56729598\n",
      "2019-03-18 01:12:53,404 epoch 60 - iter 100/252 - loss 0.55493543\n",
      "2019-03-18 01:12:59,983 epoch 60 - iter 125/252 - loss 0.53620682\n",
      "2019-03-18 01:13:06,790 epoch 60 - iter 150/252 - loss 0.57509177\n",
      "2019-03-18 01:13:13,509 epoch 60 - iter 175/252 - loss 0.56209317\n",
      "2019-03-18 01:13:20,304 epoch 60 - iter 200/252 - loss 0.57511101\n",
      "2019-03-18 01:13:27,355 epoch 60 - iter 225/252 - loss 0.56703987\n",
      "2019-03-18 01:13:33,953 epoch 60 - iter 250/252 - loss 0.56642399\n",
      "2019-03-18 01:13:34,104 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:13:34,105 EPOCH 60 done: loss 0.5659 - lr 0.0500 - bad epochs 0\n",
      "2019-03-18 01:13:44,216 DEV  : loss 1.74127281 - f-score 0.9672 - acc 0.9365\n",
      "2019-03-18 01:14:02,689 TEST : loss 2.34643102 - f-score 0.9636 - acc 0.9298\n",
      "2019-03-18 01:14:24,651 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:14:24,799 epoch 61 - iter 0/252 - loss 0.31249666\n",
      "2019-03-18 01:14:29,170 epoch 61 - iter 25/252 - loss 0.62236664\n",
      "2019-03-18 01:14:33,391 epoch 61 - iter 50/252 - loss 0.59276413\n",
      "2019-03-18 01:14:40,039 epoch 61 - iter 75/252 - loss 0.56880262\n",
      "2019-03-18 01:14:46,961 epoch 61 - iter 100/252 - loss 0.57582606\n",
      "2019-03-18 01:14:53,752 epoch 61 - iter 125/252 - loss 0.55083248\n",
      "2019-03-18 01:15:00,157 epoch 61 - iter 150/252 - loss 0.53371591\n",
      "2019-03-18 01:15:06,819 epoch 61 - iter 175/252 - loss 0.53922228\n",
      "2019-03-18 01:15:13,338 epoch 61 - iter 200/252 - loss 0.53167900\n",
      "2019-03-18 01:15:19,932 epoch 61 - iter 225/252 - loss 0.53579806\n",
      "2019-03-18 01:15:26,518 epoch 61 - iter 250/252 - loss 0.53834560\n",
      "2019-03-18 01:15:26,678 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:15:26,678 EPOCH 61 done: loss 0.5383 - lr 0.0500 - bad epochs 0\n",
      "2019-03-18 01:15:36,431 DEV  : loss 1.74238050 - f-score 0.9680 - acc 0.9379\n",
      "2019-03-18 01:15:58,027 TEST : loss 2.39287400 - f-score 0.9631 - acc 0.9288\n",
      "2019-03-18 01:16:17,678 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:16:17,904 epoch 62 - iter 0/252 - loss 0.36652374\n",
      "2019-03-18 01:16:23,492 epoch 62 - iter 25/252 - loss 0.44939555\n",
      "2019-03-18 01:16:29,081 epoch 62 - iter 50/252 - loss 0.49240407\n",
      "2019-03-18 01:16:34,659 epoch 62 - iter 75/252 - loss 0.54979030\n",
      "2019-03-18 01:16:40,333 epoch 62 - iter 100/252 - loss 0.57220440\n",
      "2019-03-18 01:16:45,868 epoch 62 - iter 125/252 - loss 0.56129046\n",
      "2019-03-18 01:16:51,295 epoch 62 - iter 150/252 - loss 0.54220737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 01:16:56,695 epoch 62 - iter 175/252 - loss 0.53124483\n",
      "2019-03-18 01:17:01,751 epoch 62 - iter 200/252 - loss 0.54173053\n",
      "2019-03-18 01:17:05,974 epoch 62 - iter 225/252 - loss 0.52458798\n",
      "2019-03-18 01:17:10,740 epoch 62 - iter 250/252 - loss 0.52728567\n",
      "2019-03-18 01:17:10,885 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:17:10,886 EPOCH 62 done: loss 0.5267 - lr 0.0500 - bad epochs 0\n",
      "2019-03-18 01:17:20,920 DEV  : loss 1.76516414 - f-score 0.9677 - acc 0.9375\n",
      "2019-03-18 01:17:43,384 TEST : loss 2.42677307 - f-score 0.9642 - acc 0.9310\n",
      "2019-03-18 01:18:00,018 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:18:00,249 epoch 63 - iter 0/252 - loss 0.10931206\n",
      "2019-03-18 01:18:06,732 epoch 63 - iter 25/252 - loss 0.45053110\n",
      "2019-03-18 01:18:13,492 epoch 63 - iter 50/252 - loss 0.42269150\n",
      "2019-03-18 01:18:20,269 epoch 63 - iter 75/252 - loss 0.42620128\n",
      "2019-03-18 01:18:26,774 epoch 63 - iter 100/252 - loss 0.43846222\n",
      "2019-03-18 01:18:33,392 epoch 63 - iter 125/252 - loss 0.44070789\n",
      "2019-03-18 01:18:40,045 epoch 63 - iter 150/252 - loss 0.44244794\n",
      "2019-03-18 01:18:46,554 epoch 63 - iter 175/252 - loss 0.46487923\n",
      "2019-03-18 01:18:52,276 epoch 63 - iter 200/252 - loss 0.45860872\n",
      "2019-03-18 01:18:57,470 epoch 63 - iter 225/252 - loss 0.46009981\n",
      "2019-03-18 01:19:02,720 epoch 63 - iter 250/252 - loss 0.47145440\n",
      "2019-03-18 01:19:02,872 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:19:02,872 EPOCH 63 done: loss 0.4711 - lr 0.0500 - bad epochs 0\n",
      "2019-03-18 01:19:11,035 DEV  : loss 1.77114606 - f-score 0.9676 - acc 0.9373\n",
      "2019-03-18 01:19:29,020 TEST : loss 2.40584040 - f-score 0.9643 - acc 0.9311\n",
      "2019-03-18 01:19:49,794 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:19:50,122 epoch 64 - iter 0/252 - loss 0.54440308\n",
      "2019-03-18 01:19:56,704 epoch 64 - iter 25/252 - loss 0.55231711\n",
      "2019-03-18 01:20:03,279 epoch 64 - iter 50/252 - loss 0.54518918\n",
      "2019-03-18 01:20:10,370 epoch 64 - iter 75/252 - loss 0.51463927\n",
      "2019-03-18 01:20:17,029 epoch 64 - iter 100/252 - loss 0.51328674\n",
      "2019-03-18 01:20:23,528 epoch 64 - iter 125/252 - loss 0.49145303\n",
      "2019-03-18 01:20:29,746 epoch 64 - iter 150/252 - loss 0.50277542\n",
      "2019-03-18 01:20:36,561 epoch 64 - iter 175/252 - loss 0.49432847\n",
      "2019-03-18 01:20:42,995 epoch 64 - iter 200/252 - loss 0.48918341\n",
      "2019-03-18 01:20:49,446 epoch 64 - iter 225/252 - loss 0.48507035\n",
      "2019-03-18 01:20:56,141 epoch 64 - iter 250/252 - loss 0.48255876\n",
      "2019-03-18 01:20:56,306 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:20:56,307 EPOCH 64 done: loss 0.4826 - lr 0.0500 - bad epochs 0\n",
      "2019-03-18 01:21:06,140 DEV  : loss 1.77868438 - f-score 0.9677 - acc 0.9375\n",
      "2019-03-18 01:21:27,894 TEST : loss 2.46622229 - f-score 0.9646 - acc 0.9316\n",
      "2019-03-18 01:21:27,895 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:21:28,081 epoch 65 - iter 0/252 - loss 0.45130920\n",
      "2019-03-18 01:21:33,437 epoch 65 - iter 25/252 - loss 0.42782783\n",
      "2019-03-18 01:21:38,837 epoch 65 - iter 50/252 - loss 0.50136371\n",
      "2019-03-18 01:21:44,015 epoch 65 - iter 75/252 - loss 0.48139393\n",
      "2019-03-18 01:21:49,574 epoch 65 - iter 100/252 - loss 0.47547667\n",
      "2019-03-18 01:21:55,128 epoch 65 - iter 125/252 - loss 0.49350091\n",
      "2019-03-18 01:22:00,744 epoch 65 - iter 150/252 - loss 0.48215094\n",
      "2019-03-18 01:22:06,157 epoch 65 - iter 175/252 - loss 0.48738711\n",
      "2019-03-18 01:22:11,662 epoch 65 - iter 200/252 - loss 0.50757778\n",
      "2019-03-18 01:22:17,282 epoch 65 - iter 225/252 - loss 0.50131585\n",
      "2019-03-18 01:22:24,090 epoch 65 - iter 250/252 - loss 0.51173447\n",
      "2019-03-18 01:22:24,269 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:22:24,270 EPOCH 65 done: loss 0.5115 - lr 0.0500 - bad epochs 1\n",
      "2019-03-18 01:22:34,166 DEV  : loss 1.76973605 - f-score 0.9674 - acc 0.9370\n",
      "2019-03-18 01:22:56,837 TEST : loss 2.40007806 - f-score 0.9644 - acc 0.9312\n",
      "2019-03-18 01:22:56,839 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:22:57,154 epoch 66 - iter 0/252 - loss 0.68788910\n",
      "2019-03-18 01:23:03,688 epoch 66 - iter 25/252 - loss 0.57549376\n",
      "2019-03-18 01:23:10,473 epoch 66 - iter 50/252 - loss 0.58441887\n",
      "2019-03-18 01:23:17,423 epoch 66 - iter 75/252 - loss 0.55539536\n",
      "2019-03-18 01:23:24,271 epoch 66 - iter 100/252 - loss 0.53483005\n",
      "2019-03-18 01:23:30,896 epoch 66 - iter 125/252 - loss 0.54449109\n",
      "2019-03-18 01:23:37,693 epoch 66 - iter 150/252 - loss 0.54386579\n",
      "2019-03-18 01:23:44,319 epoch 66 - iter 175/252 - loss 0.52173545\n",
      "2019-03-18 01:23:50,683 epoch 66 - iter 200/252 - loss 0.51168215\n",
      "2019-03-18 01:23:57,384 epoch 66 - iter 225/252 - loss 0.50797330\n",
      "2019-03-18 01:24:03,590 epoch 66 - iter 250/252 - loss 0.50492246\n",
      "2019-03-18 01:24:03,712 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:24:03,713 EPOCH 66 done: loss 0.5068 - lr 0.0500 - bad epochs 2\n",
      "2019-03-18 01:24:11,099 DEV  : loss 1.84660900 - f-score 0.9668 - acc 0.9357\n",
      "2019-03-18 01:24:29,329 TEST : loss 2.48048377 - f-score 0.9643 - acc 0.9311\n",
      "2019-03-18 01:24:29,330 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:24:29,525 epoch 67 - iter 0/252 - loss 0.66697514\n",
      "2019-03-18 01:24:35,056 epoch 67 - iter 25/252 - loss 0.70526795\n",
      "2019-03-18 01:24:40,232 epoch 67 - iter 50/252 - loss 0.62296764\n",
      "2019-03-18 01:24:45,583 epoch 67 - iter 75/252 - loss 0.59645347\n",
      "2019-03-18 01:24:50,970 epoch 67 - iter 100/252 - loss 0.57224532\n",
      "2019-03-18 01:24:57,776 epoch 67 - iter 125/252 - loss 0.56846060\n",
      "2019-03-18 01:25:04,692 epoch 67 - iter 150/252 - loss 0.57190584\n",
      "2019-03-18 01:25:11,266 epoch 67 - iter 175/252 - loss 0.55237295\n",
      "2019-03-18 01:25:17,771 epoch 67 - iter 200/252 - loss 0.54820097\n",
      "2019-03-18 01:25:24,015 epoch 67 - iter 225/252 - loss 0.53088527\n",
      "2019-03-18 01:25:30,556 epoch 67 - iter 250/252 - loss 0.53653696\n",
      "2019-03-18 01:25:30,719 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:25:30,720 EPOCH 67 done: loss 0.5364 - lr 0.0500 - bad epochs 3\n",
      "2019-03-18 01:25:40,469 DEV  : loss 1.81226885 - f-score 0.9674 - acc 0.9368\n",
      "2019-03-18 01:26:03,002 TEST : loss 2.40927529 - f-score 0.9644 - acc 0.9313\n",
      "Epoch    66: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2019-03-18 01:26:03,003 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:26:03,258 epoch 68 - iter 0/252 - loss 0.25444508\n",
      "2019-03-18 01:26:10,152 epoch 68 - iter 25/252 - loss 0.61728470\n",
      "2019-03-18 01:26:17,025 epoch 68 - iter 50/252 - loss 0.55858031\n",
      "2019-03-18 01:26:23,903 epoch 68 - iter 75/252 - loss 0.53235266\n",
      "2019-03-18 01:26:30,615 epoch 68 - iter 100/252 - loss 0.51556291\n",
      "2019-03-18 01:26:37,007 epoch 68 - iter 125/252 - loss 0.56042306\n",
      "2019-03-18 01:26:42,542 epoch 68 - iter 150/252 - loss 0.54076684\n",
      "2019-03-18 01:26:47,990 epoch 68 - iter 175/252 - loss 0.52702307\n",
      "2019-03-18 01:26:53,421 epoch 68 - iter 200/252 - loss 0.53980037\n",
      "2019-03-18 01:26:59,118 epoch 68 - iter 225/252 - loss 0.53397325\n",
      "2019-03-18 01:27:04,302 epoch 68 - iter 250/252 - loss 0.51923407\n",
      "2019-03-18 01:27:04,494 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:27:04,495 EPOCH 68 done: loss 0.5194 - lr 0.0250 - bad epochs 0\n",
      "2019-03-18 01:27:12,476 DEV  : loss 1.81691432 - f-score 0.9683 - acc 0.9387\n",
      "2019-03-18 01:27:31,859 TEST : loss 2.47408986 - f-score 0.9639 - acc 0.9304\n",
      "2019-03-18 01:27:31,861 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:27:32,097 epoch 69 - iter 0/252 - loss 0.34874153\n",
      "2019-03-18 01:27:38,770 epoch 69 - iter 25/252 - loss 0.43510467\n",
      "2019-03-18 01:27:45,506 epoch 69 - iter 50/252 - loss 0.41738581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 01:27:52,029 epoch 69 - iter 75/252 - loss 0.42771632\n",
      "2019-03-18 01:27:58,773 epoch 69 - iter 100/252 - loss 0.44503409\n",
      "2019-03-18 01:28:05,689 epoch 69 - iter 125/252 - loss 0.45002533\n",
      "2019-03-18 01:28:12,395 epoch 69 - iter 150/252 - loss 0.46328723\n",
      "2019-03-18 01:28:19,251 epoch 69 - iter 175/252 - loss 0.45479031\n",
      "2019-03-18 01:28:26,048 epoch 69 - iter 200/252 - loss 0.48183334\n",
      "2019-03-18 01:28:32,848 epoch 69 - iter 225/252 - loss 0.48467682\n",
      "2019-03-18 01:28:39,573 epoch 69 - iter 250/252 - loss 0.48606820\n",
      "2019-03-18 01:28:39,749 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:28:39,750 EPOCH 69 done: loss 0.4859 - lr 0.0250 - bad epochs 1\n",
      "2019-03-18 01:28:49,456 DEV  : loss 1.77473772 - f-score 0.9686 - acc 0.9392\n",
      "2019-03-18 01:29:11,313 TEST : loss 2.41981173 - f-score 0.9649 - acc 0.9322\n",
      "2019-03-18 01:29:11,314 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:29:11,510 epoch 70 - iter 0/252 - loss 0.21164179\n",
      "2019-03-18 01:29:16,796 epoch 70 - iter 25/252 - loss 0.46787441\n",
      "2019-03-18 01:29:22,329 epoch 70 - iter 50/252 - loss 0.47564077\n",
      "2019-03-18 01:29:27,844 epoch 70 - iter 75/252 - loss 0.48796598\n",
      "2019-03-18 01:29:33,461 epoch 70 - iter 100/252 - loss 0.47280244\n",
      "2019-03-18 01:29:38,907 epoch 70 - iter 125/252 - loss 0.45883247\n",
      "2019-03-18 01:29:44,421 epoch 70 - iter 150/252 - loss 0.46098943\n",
      "2019-03-18 01:29:49,799 epoch 70 - iter 175/252 - loss 0.47806143\n",
      "2019-03-18 01:29:55,126 epoch 70 - iter 200/252 - loss 0.47282045\n",
      "2019-03-18 01:30:00,344 epoch 70 - iter 225/252 - loss 0.46784040\n",
      "2019-03-18 01:30:04,654 epoch 70 - iter 250/252 - loss 0.47052498\n",
      "2019-03-18 01:30:04,775 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:30:04,776 EPOCH 70 done: loss 0.4710 - lr 0.0250 - bad epochs 2\n",
      "2019-03-18 01:30:13,042 DEV  : loss 1.80755055 - f-score 0.9682 - acc 0.9385\n",
      "2019-03-18 01:30:35,131 TEST : loss 2.46225929 - f-score 0.9644 - acc 0.9313\n",
      "2019-03-18 01:30:35,133 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:30:35,407 epoch 71 - iter 0/252 - loss 0.11094809\n",
      "2019-03-18 01:30:42,351 epoch 71 - iter 25/252 - loss 0.42748399\n",
      "2019-03-18 01:30:48,966 epoch 71 - iter 50/252 - loss 0.44038181\n",
      "2019-03-18 01:30:55,709 epoch 71 - iter 75/252 - loss 0.44763880\n",
      "2019-03-18 01:31:02,429 epoch 71 - iter 100/252 - loss 0.43916012\n",
      "2019-03-18 01:31:09,274 epoch 71 - iter 125/252 - loss 0.43507490\n",
      "2019-03-18 01:31:15,915 epoch 71 - iter 150/252 - loss 0.47434551\n",
      "2019-03-18 01:31:22,597 epoch 71 - iter 175/252 - loss 0.46958769\n",
      "2019-03-18 01:31:29,264 epoch 71 - iter 200/252 - loss 0.47305839\n",
      "2019-03-18 01:31:36,102 epoch 71 - iter 225/252 - loss 0.47182744\n",
      "2019-03-18 01:31:42,597 epoch 71 - iter 250/252 - loss 0.47001284\n",
      "2019-03-18 01:31:42,802 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:31:42,803 EPOCH 71 done: loss 0.4695 - lr 0.0250 - bad epochs 3\n",
      "2019-03-18 01:31:52,518 DEV  : loss 1.78826058 - f-score 0.9682 - acc 0.9384\n",
      "2019-03-18 01:32:10,576 TEST : loss 2.46756792 - f-score 0.9646 - acc 0.9315\n",
      "2019-03-18 01:32:25,624 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:32:25,866 epoch 72 - iter 0/252 - loss 0.30455035\n",
      "2019-03-18 01:32:31,355 epoch 72 - iter 25/252 - loss 0.33700427\n",
      "2019-03-18 01:32:37,716 epoch 72 - iter 50/252 - loss 0.47535679\n",
      "2019-03-18 01:32:44,117 epoch 72 - iter 75/252 - loss 0.45948364\n",
      "2019-03-18 01:32:50,699 epoch 72 - iter 100/252 - loss 0.48078651\n",
      "2019-03-18 01:32:57,228 epoch 72 - iter 125/252 - loss 0.46435880\n",
      "2019-03-18 01:33:04,131 epoch 72 - iter 150/252 - loss 0.47057168\n",
      "2019-03-18 01:33:10,684 epoch 72 - iter 175/252 - loss 0.47400247\n",
      "2019-03-18 01:33:17,459 epoch 72 - iter 200/252 - loss 0.46490612\n",
      "2019-03-18 01:33:24,085 epoch 72 - iter 225/252 - loss 0.46829956\n",
      "2019-03-18 01:33:30,641 epoch 72 - iter 250/252 - loss 0.46919093\n",
      "2019-03-18 01:33:30,850 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:33:30,851 EPOCH 72 done: loss 0.4689 - lr 0.0250 - bad epochs 0\n",
      "2019-03-18 01:33:40,729 DEV  : loss 1.81099117 - f-score 0.9684 - acc 0.9389\n",
      "2019-03-18 01:34:03,145 TEST : loss 2.48631573 - f-score 0.9638 - acc 0.9302\n",
      "2019-03-18 01:34:18,263 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:34:18,719 epoch 73 - iter 0/252 - loss 3.18141842\n",
      "2019-03-18 01:34:24,081 epoch 73 - iter 25/252 - loss 0.46319112\n",
      "2019-03-18 01:34:29,581 epoch 73 - iter 50/252 - loss 0.41925767\n",
      "2019-03-18 01:34:35,128 epoch 73 - iter 75/252 - loss 0.42568954\n",
      "2019-03-18 01:34:40,704 epoch 73 - iter 100/252 - loss 0.40441344\n",
      "2019-03-18 01:34:46,218 epoch 73 - iter 125/252 - loss 0.41480560\n",
      "2019-03-18 01:34:51,318 epoch 73 - iter 150/252 - loss 0.40964447\n",
      "2019-03-18 01:34:56,721 epoch 73 - iter 175/252 - loss 0.41076124\n",
      "2019-03-18 01:35:01,847 epoch 73 - iter 200/252 - loss 0.41280162\n",
      "2019-03-18 01:35:06,429 epoch 73 - iter 225/252 - loss 0.40964449\n",
      "2019-03-18 01:35:11,133 epoch 73 - iter 250/252 - loss 0.40291496\n",
      "2019-03-18 01:35:11,311 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:35:11,312 EPOCH 73 done: loss 0.4031 - lr 0.0250 - bad epochs 0\n",
      "2019-03-18 01:35:20,896 DEV  : loss 1.83547795 - f-score 0.9676 - acc 0.9374\n",
      "2019-03-18 01:35:42,790 TEST : loss 2.52185678 - f-score 0.9637 - acc 0.9300\n",
      "2019-03-18 01:35:57,939 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:35:58,248 epoch 74 - iter 0/252 - loss 0.61516416\n",
      "2019-03-18 01:36:05,157 epoch 74 - iter 25/252 - loss 0.43468890\n",
      "2019-03-18 01:36:11,852 epoch 74 - iter 50/252 - loss 0.41642142\n",
      "2019-03-18 01:36:18,623 epoch 74 - iter 75/252 - loss 0.39500409\n",
      "2019-03-18 01:36:25,236 epoch 74 - iter 100/252 - loss 0.40227130\n",
      "2019-03-18 01:36:31,877 epoch 74 - iter 125/252 - loss 0.39339678\n",
      "2019-03-18 01:36:38,617 epoch 74 - iter 150/252 - loss 0.38411502\n",
      "2019-03-18 01:36:45,356 epoch 74 - iter 175/252 - loss 0.38816828\n",
      "2019-03-18 01:36:52,011 epoch 74 - iter 200/252 - loss 0.38710468\n",
      "2019-03-18 01:36:57,397 epoch 74 - iter 225/252 - loss 0.40546296\n",
      "2019-03-18 01:37:02,979 epoch 74 - iter 250/252 - loss 0.41835864\n",
      "2019-03-18 01:37:03,108 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:37:03,109 EPOCH 74 done: loss 0.4185 - lr 0.0250 - bad epochs 0\n",
      "2019-03-18 01:37:11,110 DEV  : loss 1.83837724 - f-score 0.9680 - acc 0.9380\n",
      "2019-03-18 01:37:28,869 TEST : loss 2.51652169 - f-score 0.9636 - acc 0.9298\n",
      "2019-03-18 01:37:28,871 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:37:29,109 epoch 75 - iter 0/252 - loss 0.19872391\n",
      "2019-03-18 01:37:34,515 epoch 75 - iter 25/252 - loss 0.51471634\n",
      "2019-03-18 01:37:39,159 epoch 75 - iter 50/252 - loss 0.46872090\n",
      "2019-03-18 01:37:43,230 epoch 75 - iter 75/252 - loss 0.43655975\n",
      "2019-03-18 01:37:49,479 epoch 75 - iter 100/252 - loss 0.48607565\n",
      "2019-03-18 01:37:55,945 epoch 75 - iter 125/252 - loss 0.47564533\n",
      "2019-03-18 01:38:02,759 epoch 75 - iter 150/252 - loss 0.50622888\n",
      "2019-03-18 01:38:09,420 epoch 75 - iter 175/252 - loss 0.51721929\n",
      "2019-03-18 01:38:16,252 epoch 75 - iter 200/252 - loss 0.50334837\n",
      "2019-03-18 01:38:23,166 epoch 75 - iter 225/252 - loss 0.50276793\n",
      "2019-03-18 01:38:30,068 epoch 75 - iter 250/252 - loss 0.50786257\n",
      "2019-03-18 01:38:30,197 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:38:30,198 EPOCH 75 done: loss 0.5081 - lr 0.0250 - bad epochs 1\n",
      "2019-03-18 01:38:39,385 DEV  : loss 1.82514000 - f-score 0.9675 - acc 0.9371\n",
      "2019-03-18 01:39:01,894 TEST : loss 2.49761009 - f-score 0.9641 - acc 0.9307\n",
      "2019-03-18 01:39:01,896 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 01:39:02,232 epoch 76 - iter 0/252 - loss 0.87012577\n",
      "2019-03-18 01:39:08,932 epoch 76 - iter 25/252 - loss 0.41196173\n",
      "2019-03-18 01:39:15,756 epoch 76 - iter 50/252 - loss 0.42539312\n",
      "2019-03-18 01:39:22,122 epoch 76 - iter 75/252 - loss 0.44127733\n",
      "2019-03-18 01:39:28,808 epoch 76 - iter 100/252 - loss 0.41969630\n",
      "2019-03-18 01:39:34,751 epoch 76 - iter 125/252 - loss 0.40938221\n",
      "2019-03-18 01:39:40,078 epoch 76 - iter 150/252 - loss 0.38961621\n",
      "2019-03-18 01:39:45,644 epoch 76 - iter 175/252 - loss 0.38573451\n",
      "2019-03-18 01:39:51,123 epoch 76 - iter 200/252 - loss 0.40088592\n",
      "2019-03-18 01:39:56,687 epoch 76 - iter 225/252 - loss 0.40229823\n",
      "2019-03-18 01:40:02,445 epoch 76 - iter 250/252 - loss 0.39813220\n",
      "2019-03-18 01:40:02,591 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:40:02,592 EPOCH 76 done: loss 0.3983 - lr 0.0250 - bad epochs 2\n",
      "2019-03-18 01:40:10,549 DEV  : loss 1.82789660 - f-score 0.9680 - acc 0.9380\n",
      "2019-03-18 01:40:30,938 TEST : loss 2.51237011 - f-score 0.9642 - acc 0.9309\n",
      "2019-03-18 01:40:45,489 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:40:45,714 epoch 77 - iter 0/252 - loss 0.46387351\n",
      "2019-03-18 01:40:52,571 epoch 77 - iter 25/252 - loss 0.45762029\n",
      "2019-03-18 01:40:59,286 epoch 77 - iter 50/252 - loss 0.41433501\n",
      "2019-03-18 01:41:05,949 epoch 77 - iter 75/252 - loss 0.40335396\n",
      "2019-03-18 01:41:12,343 epoch 77 - iter 100/252 - loss 0.39398138\n",
      "2019-03-18 01:41:18,885 epoch 77 - iter 125/252 - loss 0.39562121\n",
      "2019-03-18 01:41:25,633 epoch 77 - iter 150/252 - loss 0.40121282\n",
      "2019-03-18 01:41:32,427 epoch 77 - iter 175/252 - loss 0.39565727\n",
      "2019-03-18 01:41:39,342 epoch 77 - iter 200/252 - loss 0.40066818\n",
      "2019-03-18 01:41:45,900 epoch 77 - iter 225/252 - loss 0.40175578\n",
      "2019-03-18 01:41:52,646 epoch 77 - iter 250/252 - loss 0.40940743\n",
      "2019-03-18 01:41:52,839 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:41:52,840 EPOCH 77 done: loss 0.4089 - lr 0.0250 - bad epochs 0\n",
      "2019-03-18 01:42:02,516 DEV  : loss 1.82480741 - f-score 0.9685 - acc 0.9390\n",
      "2019-03-18 01:42:20,401 TEST : loss 2.49982953 - f-score 0.9646 - acc 0.9317\n",
      "2019-03-18 01:42:20,404 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:42:20,607 epoch 78 - iter 0/252 - loss 0.54354262\n",
      "2019-03-18 01:42:25,950 epoch 78 - iter 25/252 - loss 0.38149249\n",
      "2019-03-18 01:42:31,442 epoch 78 - iter 50/252 - loss 0.43211350\n",
      "2019-03-18 01:42:37,046 epoch 78 - iter 75/252 - loss 0.39299228\n",
      "2019-03-18 01:42:42,426 epoch 78 - iter 100/252 - loss 0.39021985\n",
      "2019-03-18 01:42:47,722 epoch 78 - iter 125/252 - loss 0.39402565\n",
      "2019-03-18 01:42:52,158 epoch 78 - iter 150/252 - loss 0.39631572\n",
      "2019-03-18 01:42:56,672 epoch 78 - iter 175/252 - loss 0.39505130\n",
      "2019-03-18 01:43:03,382 epoch 78 - iter 200/252 - loss 0.40516167\n",
      "2019-03-18 01:43:09,974 epoch 78 - iter 225/252 - loss 0.41216788\n",
      "2019-03-18 01:43:16,659 epoch 78 - iter 250/252 - loss 0.41970487\n",
      "2019-03-18 01:43:16,777 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:43:16,778 EPOCH 78 done: loss 0.4195 - lr 0.0250 - bad epochs 1\n",
      "2019-03-18 01:43:26,318 DEV  : loss 1.83228040 - f-score 0.9681 - acc 0.9382\n",
      "2019-03-18 01:43:47,385 TEST : loss 2.53433943 - f-score 0.9642 - acc 0.9310\n",
      "2019-03-18 01:43:47,386 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:43:47,630 epoch 79 - iter 0/252 - loss 0.72861004\n",
      "2019-03-18 01:43:54,369 epoch 79 - iter 25/252 - loss 0.35476342\n",
      "2019-03-18 01:44:00,876 epoch 79 - iter 50/252 - loss 0.41282353\n",
      "2019-03-18 01:44:07,323 epoch 79 - iter 75/252 - loss 0.39495286\n",
      "2019-03-18 01:44:13,967 epoch 79 - iter 100/252 - loss 0.43749137\n",
      "2019-03-18 01:44:20,735 epoch 79 - iter 125/252 - loss 0.44848610\n",
      "2019-03-18 01:44:27,230 epoch 79 - iter 150/252 - loss 0.44051002\n",
      "2019-03-18 01:44:34,052 epoch 79 - iter 175/252 - loss 0.44100654\n",
      "2019-03-18 01:44:40,714 epoch 79 - iter 200/252 - loss 0.44411146\n",
      "2019-03-18 01:44:46,634 epoch 79 - iter 225/252 - loss 0.44566836\n",
      "2019-03-18 01:44:51,852 epoch 79 - iter 250/252 - loss 0.43965250\n",
      "2019-03-18 01:44:51,991 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:44:51,991 EPOCH 79 done: loss 0.4394 - lr 0.0250 - bad epochs 2\n",
      "2019-03-18 01:44:59,853 DEV  : loss 1.84671664 - f-score 0.9684 - acc 0.9388\n",
      "2019-03-18 01:45:18,045 TEST : loss 2.54940152 - f-score 0.9645 - acc 0.9315\n",
      "2019-03-18 01:45:18,047 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:45:18,323 epoch 80 - iter 0/252 - loss 0.32996464\n",
      "2019-03-18 01:45:23,869 epoch 80 - iter 25/252 - loss 0.39581181\n",
      "2019-03-18 01:45:29,258 epoch 80 - iter 50/252 - loss 0.42603131\n",
      "2019-03-18 01:45:35,955 epoch 80 - iter 75/252 - loss 0.42200522\n",
      "2019-03-18 01:45:42,482 epoch 80 - iter 100/252 - loss 0.40213330\n",
      "2019-03-18 01:45:49,092 epoch 80 - iter 125/252 - loss 0.41817030\n",
      "2019-03-18 01:45:55,914 epoch 80 - iter 150/252 - loss 0.42507471\n",
      "2019-03-18 01:46:02,567 epoch 80 - iter 175/252 - loss 0.42073599\n",
      "2019-03-18 01:46:09,326 epoch 80 - iter 200/252 - loss 0.41288899\n",
      "2019-03-18 01:46:16,211 epoch 80 - iter 225/252 - loss 0.41983060\n",
      "2019-03-18 01:46:22,824 epoch 80 - iter 250/252 - loss 0.41249649\n",
      "2019-03-18 01:46:23,031 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:46:23,032 EPOCH 80 done: loss 0.4128 - lr 0.0250 - bad epochs 3\n",
      "2019-03-18 01:46:33,579 DEV  : loss 1.87089956 - f-score 0.9674 - acc 0.9369\n",
      "2019-03-18 01:46:55,718 TEST : loss 2.57594299 - f-score 0.9641 - acc 0.9307\n",
      "Epoch    79: reducing learning rate of group 0 to 1.2500e-02.\n",
      "2019-03-18 01:46:55,719 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:46:56,114 epoch 81 - iter 0/252 - loss 0.22872722\n",
      "2019-03-18 01:47:02,926 epoch 81 - iter 25/252 - loss 0.43807712\n",
      "2019-03-18 01:47:09,621 epoch 81 - iter 50/252 - loss 0.36358604\n",
      "2019-03-18 01:47:16,153 epoch 81 - iter 75/252 - loss 0.41829806\n",
      "2019-03-18 01:47:21,965 epoch 81 - iter 100/252 - loss 0.40979149\n",
      "2019-03-18 01:47:27,154 epoch 81 - iter 125/252 - loss 0.40223625\n",
      "2019-03-18 01:47:32,640 epoch 81 - iter 150/252 - loss 0.40093204\n",
      "2019-03-18 01:47:38,270 epoch 81 - iter 175/252 - loss 0.41147534\n",
      "2019-03-18 01:47:43,900 epoch 81 - iter 200/252 - loss 0.41727971\n",
      "2019-03-18 01:47:49,447 epoch 81 - iter 225/252 - loss 0.40979926\n",
      "2019-03-18 01:47:55,018 epoch 81 - iter 250/252 - loss 0.40897292\n",
      "2019-03-18 01:47:55,138 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:47:55,139 EPOCH 81 done: loss 0.4086 - lr 0.0125 - bad epochs 0\n",
      "2019-03-18 01:48:02,982 DEV  : loss 1.87018657 - f-score 0.9682 - acc 0.9383\n",
      "2019-03-18 01:48:19,377 TEST : loss 2.56885242 - f-score 0.9641 - acc 0.9307\n",
      "2019-03-18 01:48:19,379 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:48:19,670 epoch 82 - iter 0/252 - loss 0.20905137\n",
      "2019-03-18 01:48:26,574 epoch 82 - iter 25/252 - loss 0.38530786\n",
      "2019-03-18 01:48:33,053 epoch 82 - iter 50/252 - loss 0.34941502\n",
      "2019-03-18 01:48:39,660 epoch 82 - iter 75/252 - loss 0.33350038\n",
      "2019-03-18 01:48:46,341 epoch 82 - iter 100/252 - loss 0.33629293\n",
      "2019-03-18 01:48:53,074 epoch 82 - iter 125/252 - loss 0.33991858\n",
      "2019-03-18 01:48:59,870 epoch 82 - iter 150/252 - loss 0.35161773\n",
      "2019-03-18 01:49:06,686 epoch 82 - iter 175/252 - loss 0.35887977\n",
      "2019-03-18 01:49:12,691 epoch 82 - iter 200/252 - loss 0.36033706\n",
      "2019-03-18 01:49:19,438 epoch 82 - iter 225/252 - loss 0.39006565\n",
      "2019-03-18 01:49:25,861 epoch 82 - iter 250/252 - loss 0.38924310\n",
      "2019-03-18 01:49:26,109 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:49:26,109 EPOCH 82 done: loss 0.3890 - lr 0.0125 - bad epochs 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 01:49:35,908 DEV  : loss 1.85016775 - f-score 0.9682 - acc 0.9384\n",
      "2019-03-18 01:49:58,018 TEST : loss 2.54591465 - f-score 0.9640 - acc 0.9305\n",
      "2019-03-18 01:50:13,084 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:50:13,685 epoch 83 - iter 0/252 - loss 0.45771861\n",
      "2019-03-18 01:50:19,086 epoch 83 - iter 25/252 - loss 0.37230780\n",
      "2019-03-18 01:50:24,596 epoch 83 - iter 50/252 - loss 0.39066860\n",
      "2019-03-18 01:50:30,166 epoch 83 - iter 75/252 - loss 0.37833085\n",
      "2019-03-18 01:50:35,474 epoch 83 - iter 100/252 - loss 0.36410244\n",
      "2019-03-18 01:50:40,849 epoch 83 - iter 125/252 - loss 0.36399973\n",
      "2019-03-18 01:50:47,382 epoch 83 - iter 150/252 - loss 0.37296017\n",
      "2019-03-18 01:50:54,253 epoch 83 - iter 175/252 - loss 0.36790828\n",
      "2019-03-18 01:51:00,932 epoch 83 - iter 200/252 - loss 0.37709492\n",
      "2019-03-18 01:51:07,595 epoch 83 - iter 225/252 - loss 0.37545536\n",
      "2019-03-18 01:51:14,168 epoch 83 - iter 250/252 - loss 0.37720644\n",
      "2019-03-18 01:51:14,341 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:51:14,342 EPOCH 83 done: loss 0.3768 - lr 0.0125 - bad epochs 0\n",
      "2019-03-18 01:51:24,422 DEV  : loss 1.86331403 - f-score 0.9683 - acc 0.9387\n",
      "2019-03-18 01:51:47,058 TEST : loss 2.56459594 - f-score 0.9642 - acc 0.9310\n",
      "2019-03-18 01:52:02,222 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:52:02,448 epoch 84 - iter 0/252 - loss 0.12927365\n",
      "2019-03-18 01:52:09,221 epoch 84 - iter 25/252 - loss 0.39544592\n",
      "2019-03-18 01:52:16,400 epoch 84 - iter 50/252 - loss 0.48766577\n",
      "2019-03-18 01:52:22,966 epoch 84 - iter 75/252 - loss 0.45175355\n",
      "2019-03-18 01:52:28,379 epoch 84 - iter 100/252 - loss 0.42113920\n",
      "2019-03-18 01:52:33,772 epoch 84 - iter 125/252 - loss 0.39667837\n",
      "2019-03-18 01:52:38,886 epoch 84 - iter 150/252 - loss 0.41052089\n",
      "2019-03-18 01:52:44,277 epoch 84 - iter 175/252 - loss 0.40876421\n",
      "2019-03-18 01:52:49,809 epoch 84 - iter 200/252 - loss 0.41170589\n",
      "2019-03-18 01:52:55,182 epoch 84 - iter 225/252 - loss 0.40728209\n",
      "2019-03-18 01:53:00,584 epoch 84 - iter 250/252 - loss 0.40429146\n",
      "2019-03-18 01:53:00,768 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:53:00,768 EPOCH 84 done: loss 0.4045 - lr 0.0125 - bad epochs 0\n",
      "2019-03-18 01:53:08,487 DEV  : loss 1.85134029 - f-score 0.9686 - acc 0.9392\n",
      "2019-03-18 01:53:24,710 TEST : loss 2.56072831 - f-score 0.9641 - acc 0.9307\n",
      "2019-03-18 01:53:24,712 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:53:25,033 epoch 85 - iter 0/252 - loss 0.26898289\n",
      "2019-03-18 01:53:31,916 epoch 85 - iter 25/252 - loss 0.42192217\n",
      "2019-03-18 01:53:38,449 epoch 85 - iter 50/252 - loss 0.42318870\n",
      "2019-03-18 01:53:45,406 epoch 85 - iter 75/252 - loss 0.43085250\n",
      "2019-03-18 01:53:51,922 epoch 85 - iter 100/252 - loss 0.39714350\n",
      "2019-03-18 01:53:58,299 epoch 85 - iter 125/252 - loss 0.39442285\n",
      "2019-03-18 01:54:05,197 epoch 85 - iter 150/252 - loss 0.37574992\n",
      "2019-03-18 01:54:11,600 epoch 85 - iter 175/252 - loss 0.38138372\n",
      "2019-03-18 01:54:18,446 epoch 85 - iter 200/252 - loss 0.37594307\n",
      "2019-03-18 01:54:25,009 epoch 85 - iter 225/252 - loss 0.38837759\n",
      "2019-03-18 01:54:31,587 epoch 85 - iter 250/252 - loss 0.38515063\n",
      "2019-03-18 01:54:31,737 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:54:31,738 EPOCH 85 done: loss 0.3852 - lr 0.0125 - bad epochs 1\n",
      "2019-03-18 01:54:41,346 DEV  : loss 1.84890199 - f-score 0.9681 - acc 0.9383\n",
      "2019-03-18 01:55:03,002 TEST : loss 2.55104041 - f-score 0.9639 - acc 0.9304\n",
      "2019-03-18 01:55:03,004 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:55:03,361 epoch 86 - iter 0/252 - loss 0.12633240\n",
      "2019-03-18 01:55:08,828 epoch 86 - iter 25/252 - loss 0.35267755\n",
      "2019-03-18 01:55:14,360 epoch 86 - iter 50/252 - loss 0.38757150\n",
      "2019-03-18 01:55:19,708 epoch 86 - iter 75/252 - loss 0.37628215\n",
      "2019-03-18 01:55:25,473 epoch 86 - iter 100/252 - loss 0.41061523\n",
      "2019-03-18 01:55:30,913 epoch 86 - iter 125/252 - loss 0.39198575\n",
      "2019-03-18 01:55:36,391 epoch 86 - iter 150/252 - loss 0.38480998\n",
      "2019-03-18 01:55:41,959 epoch 86 - iter 175/252 - loss 0.39274776\n",
      "2019-03-18 01:55:47,564 epoch 86 - iter 200/252 - loss 0.38746941\n",
      "2019-03-18 01:55:52,734 epoch 86 - iter 225/252 - loss 0.37827871\n",
      "2019-03-18 01:55:59,324 epoch 86 - iter 250/252 - loss 0.37060425\n",
      "2019-03-18 01:55:59,544 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:55:59,544 EPOCH 86 done: loss 0.3714 - lr 0.0125 - bad epochs 2\n",
      "2019-03-18 01:56:09,414 DEV  : loss 1.87250042 - f-score 0.9688 - acc 0.9395\n",
      "2019-03-18 01:56:31,711 TEST : loss 2.56720448 - f-score 0.9644 - acc 0.9312\n",
      "2019-03-18 01:56:44,487 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:56:45,458 epoch 87 - iter 0/252 - loss 0.29890323\n",
      "2019-03-18 01:56:53,871 epoch 87 - iter 25/252 - loss 0.48690472\n",
      "2019-03-18 01:57:00,153 epoch 87 - iter 50/252 - loss 0.45739171\n",
      "2019-03-18 01:57:06,815 epoch 87 - iter 75/252 - loss 0.45047514\n",
      "2019-03-18 01:57:13,257 epoch 87 - iter 100/252 - loss 0.43459619\n",
      "2019-03-18 01:57:20,112 epoch 87 - iter 125/252 - loss 0.41503425\n",
      "2019-03-18 01:57:26,516 epoch 87 - iter 150/252 - loss 0.39723003\n",
      "2019-03-18 01:57:33,281 epoch 87 - iter 175/252 - loss 0.41320547\n",
      "2019-03-18 01:57:39,125 epoch 87 - iter 200/252 - loss 0.39719597\n",
      "2019-03-18 01:57:44,559 epoch 87 - iter 225/252 - loss 0.38747178\n",
      "2019-03-18 01:57:49,863 epoch 87 - iter 250/252 - loss 0.38712537\n",
      "2019-03-18 01:57:49,989 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:57:49,990 EPOCH 87 done: loss 0.3868 - lr 0.0125 - bad epochs 0\n",
      "2019-03-18 01:57:58,068 DEV  : loss 1.87866700 - f-score 0.9688 - acc 0.9396\n",
      "2019-03-18 01:58:16,140 TEST : loss 2.57125592 - f-score 0.9641 - acc 0.9308\n",
      "2019-03-18 01:58:16,141 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:58:16,333 epoch 88 - iter 0/252 - loss 0.22019458\n",
      "2019-03-18 01:58:21,507 epoch 88 - iter 25/252 - loss 0.38762678\n",
      "2019-03-18 01:58:28,135 epoch 88 - iter 50/252 - loss 0.39370494\n",
      "2019-03-18 01:58:34,892 epoch 88 - iter 75/252 - loss 0.40687953\n",
      "2019-03-18 01:58:41,695 epoch 88 - iter 100/252 - loss 0.39794644\n",
      "2019-03-18 01:58:48,304 epoch 88 - iter 125/252 - loss 0.38555588\n",
      "2019-03-18 01:58:55,056 epoch 88 - iter 150/252 - loss 0.36688981\n",
      "2019-03-18 01:59:01,851 epoch 88 - iter 175/252 - loss 0.35745488\n",
      "2019-03-18 01:59:08,432 epoch 88 - iter 200/252 - loss 0.37843294\n",
      "2019-03-18 01:59:15,236 epoch 88 - iter 225/252 - loss 0.37054395\n",
      "2019-03-18 01:59:21,804 epoch 88 - iter 250/252 - loss 0.36622422\n",
      "2019-03-18 01:59:21,940 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 01:59:21,941 EPOCH 88 done: loss 0.3659 - lr 0.0125 - bad epochs 1\n",
      "2019-03-18 01:59:31,544 DEV  : loss 1.90036535 - f-score 0.9680 - acc 0.9381\n",
      "2019-03-18 01:59:53,625 TEST : loss 2.60283470 - f-score 0.9645 - acc 0.9314\n",
      "2019-03-18 02:00:06,538 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:00:09,221 epoch 89 - iter 0/252 - loss 0.44631290\n",
      "2019-03-18 02:00:14,736 epoch 89 - iter 25/252 - loss 0.39699497\n",
      "2019-03-18 02:00:20,341 epoch 89 - iter 50/252 - loss 0.32029478\n",
      "2019-03-18 02:00:26,038 epoch 89 - iter 75/252 - loss 0.33068368\n",
      "2019-03-18 02:00:31,617 epoch 89 - iter 100/252 - loss 0.34751463\n",
      "2019-03-18 02:00:37,237 epoch 89 - iter 125/252 - loss 0.36072800\n",
      "2019-03-18 02:00:42,656 epoch 89 - iter 150/252 - loss 0.37709259\n",
      "2019-03-18 02:00:47,963 epoch 89 - iter 175/252 - loss 0.38375394\n",
      "2019-03-18 02:00:53,140 epoch 89 - iter 200/252 - loss 0.38343591\n",
      "2019-03-18 02:00:57,330 epoch 89 - iter 225/252 - loss 0.37937680\n",
      "2019-03-18 02:01:03,780 epoch 89 - iter 250/252 - loss 0.37425455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 02:01:03,959 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:01:03,960 EPOCH 89 done: loss 0.3747 - lr 0.0125 - bad epochs 0\n",
      "2019-03-18 02:01:13,846 DEV  : loss 1.88343143 - f-score 0.9687 - acc 0.9394\n",
      "2019-03-18 02:01:36,262 TEST : loss 2.58502412 - f-score 0.9646 - acc 0.9316\n",
      "2019-03-18 02:01:36,264 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:01:36,636 epoch 90 - iter 0/252 - loss 0.18564129\n",
      "2019-03-18 02:01:43,387 epoch 90 - iter 25/252 - loss 0.65261563\n",
      "2019-03-18 02:01:50,235 epoch 90 - iter 50/252 - loss 0.47972836\n",
      "2019-03-18 02:01:56,856 epoch 90 - iter 75/252 - loss 0.43740006\n",
      "2019-03-18 02:02:03,651 epoch 90 - iter 100/252 - loss 0.42427865\n",
      "2019-03-18 02:02:10,408 epoch 90 - iter 125/252 - loss 0.41174400\n",
      "2019-03-18 02:02:16,923 epoch 90 - iter 150/252 - loss 0.40348161\n",
      "2019-03-18 02:02:23,430 epoch 90 - iter 175/252 - loss 0.38385349\n",
      "2019-03-18 02:02:30,383 epoch 90 - iter 200/252 - loss 0.38206474\n",
      "2019-03-18 02:02:37,092 epoch 90 - iter 225/252 - loss 0.39071544\n",
      "2019-03-18 02:02:43,590 epoch 90 - iter 250/252 - loss 0.38975637\n",
      "2019-03-18 02:02:43,769 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:02:43,770 EPOCH 90 done: loss 0.3893 - lr 0.0125 - bad epochs 1\n",
      "2019-03-18 02:02:51,585 DEV  : loss 1.89430559 - f-score 0.9688 - acc 0.9395\n",
      "2019-03-18 02:03:09,635 TEST : loss 2.60858893 - f-score 0.9640 - acc 0.9305\n",
      "2019-03-18 02:03:09,636 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:03:09,822 epoch 91 - iter 0/252 - loss 0.30019951\n",
      "2019-03-18 02:03:15,429 epoch 91 - iter 25/252 - loss 0.39729252\n",
      "2019-03-18 02:03:20,934 epoch 91 - iter 50/252 - loss 0.41134919\n",
      "2019-03-18 02:03:26,220 epoch 91 - iter 75/252 - loss 0.38623081\n",
      "2019-03-18 02:03:31,865 epoch 91 - iter 100/252 - loss 0.38762829\n",
      "2019-03-18 02:03:38,391 epoch 91 - iter 125/252 - loss 0.36873853\n",
      "2019-03-18 02:03:45,154 epoch 91 - iter 150/252 - loss 0.35633112\n",
      "2019-03-18 02:03:51,838 epoch 91 - iter 175/252 - loss 0.36610368\n",
      "2019-03-18 02:03:58,472 epoch 91 - iter 200/252 - loss 0.37433666\n",
      "2019-03-18 02:04:05,186 epoch 91 - iter 225/252 - loss 0.37232321\n",
      "2019-03-18 02:04:11,989 epoch 91 - iter 250/252 - loss 0.36074769\n",
      "2019-03-18 02:04:12,169 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:04:12,170 EPOCH 91 done: loss 0.3606 - lr 0.0125 - bad epochs 2\n",
      "2019-03-18 02:04:21,694 DEV  : loss 1.90839243 - f-score 0.9677 - acc 0.9373\n",
      "2019-03-18 02:04:43,384 TEST : loss 2.63713980 - f-score 0.9646 - acc 0.9318\n",
      "2019-03-18 02:04:56,336 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:04:58,648 epoch 92 - iter 0/252 - loss 0.25158596\n",
      "2019-03-18 02:05:05,410 epoch 92 - iter 25/252 - loss 0.53760676\n",
      "2019-03-18 02:05:11,969 epoch 92 - iter 50/252 - loss 0.43698390\n",
      "2019-03-18 02:05:17,272 epoch 92 - iter 75/252 - loss 0.46780511\n",
      "2019-03-18 02:05:22,700 epoch 92 - iter 100/252 - loss 0.46347135\n",
      "2019-03-18 02:05:28,093 epoch 92 - iter 125/252 - loss 0.44703242\n",
      "2019-03-18 02:05:33,820 epoch 92 - iter 150/252 - loss 0.43788330\n",
      "2019-03-18 02:05:39,344 epoch 92 - iter 175/252 - loss 0.44508944\n",
      "2019-03-18 02:05:44,663 epoch 92 - iter 200/252 - loss 0.43505580\n",
      "2019-03-18 02:05:50,097 epoch 92 - iter 225/252 - loss 0.42577236\n",
      "2019-03-18 02:05:55,684 epoch 92 - iter 250/252 - loss 0.41971227\n",
      "2019-03-18 02:05:55,859 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:05:55,860 EPOCH 92 done: loss 0.4219 - lr 0.0125 - bad epochs 0\n",
      "2019-03-18 02:06:04,386 DEV  : loss 1.89972746 - f-score 0.9677 - acc 0.9375\n",
      "2019-03-18 02:06:27,254 TEST : loss 2.61797976 - f-score 0.9641 - acc 0.9306\n",
      "2019-03-18 02:06:27,256 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:06:27,507 epoch 93 - iter 0/252 - loss 0.13384008\n",
      "2019-03-18 02:06:33,809 epoch 93 - iter 25/252 - loss 0.48741368\n",
      "2019-03-18 02:06:40,524 epoch 93 - iter 50/252 - loss 0.49583851\n",
      "2019-03-18 02:06:47,018 epoch 93 - iter 75/252 - loss 0.45563324\n",
      "2019-03-18 02:06:53,671 epoch 93 - iter 100/252 - loss 0.40754778\n",
      "2019-03-18 02:07:00,126 epoch 93 - iter 125/252 - loss 0.39953128\n",
      "2019-03-18 02:07:06,991 epoch 93 - iter 150/252 - loss 0.40424719\n",
      "2019-03-18 02:07:13,509 epoch 93 - iter 175/252 - loss 0.40756508\n",
      "2019-03-18 02:07:20,420 epoch 93 - iter 200/252 - loss 0.41070753\n",
      "2019-03-18 02:07:27,329 epoch 93 - iter 225/252 - loss 0.41489903\n",
      "2019-03-18 02:07:34,329 epoch 93 - iter 250/252 - loss 0.40870408\n",
      "2019-03-18 02:07:34,504 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:07:34,505 EPOCH 93 done: loss 0.4083 - lr 0.0125 - bad epochs 1\n",
      "2019-03-18 02:07:44,381 DEV  : loss 1.89077365 - f-score 0.9680 - acc 0.9380\n",
      "2019-03-18 02:08:02,622 TEST : loss 2.62190604 - f-score 0.9644 - acc 0.9313\n",
      "2019-03-18 02:08:02,623 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:08:02,901 epoch 94 - iter 0/252 - loss 1.12535286\n",
      "2019-03-18 02:08:08,635 epoch 94 - iter 25/252 - loss 0.37557016\n",
      "2019-03-18 02:08:14,138 epoch 94 - iter 50/252 - loss 0.42334837\n",
      "2019-03-18 02:08:19,624 epoch 94 - iter 75/252 - loss 0.38564425\n",
      "2019-03-18 02:08:25,342 epoch 94 - iter 100/252 - loss 0.36391279\n",
      "2019-03-18 02:08:30,793 epoch 94 - iter 125/252 - loss 0.37003177\n",
      "2019-03-18 02:08:35,520 epoch 94 - iter 150/252 - loss 0.39067324\n",
      "2019-03-18 02:08:39,733 epoch 94 - iter 175/252 - loss 0.38884520\n",
      "2019-03-18 02:08:46,252 epoch 94 - iter 200/252 - loss 0.39513098\n",
      "2019-03-18 02:08:52,774 epoch 94 - iter 225/252 - loss 0.38371803\n",
      "2019-03-18 02:08:59,394 epoch 94 - iter 250/252 - loss 0.37732980\n",
      "2019-03-18 02:08:59,596 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:08:59,597 EPOCH 94 done: loss 0.3776 - lr 0.0125 - bad epochs 2\n",
      "2019-03-18 02:09:09,291 DEV  : loss 1.88846731 - f-score 0.9680 - acc 0.9379\n",
      "2019-03-18 02:09:32,351 TEST : loss 2.63988924 - f-score 0.9640 - acc 0.9307\n",
      "2019-03-18 02:09:32,352 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:09:32,624 epoch 95 - iter 0/252 - loss 0.52108169\n",
      "2019-03-18 02:09:39,386 epoch 95 - iter 25/252 - loss 0.28311531\n",
      "2019-03-18 02:09:46,023 epoch 95 - iter 50/252 - loss 0.30529812\n",
      "2019-03-18 02:09:53,103 epoch 95 - iter 75/252 - loss 0.34696130\n",
      "2019-03-18 02:10:00,064 epoch 95 - iter 100/252 - loss 0.34801300\n",
      "2019-03-18 02:10:06,510 epoch 95 - iter 125/252 - loss 0.37190401\n",
      "2019-03-18 02:10:12,552 epoch 95 - iter 150/252 - loss 0.36504193\n",
      "2019-03-18 02:10:19,077 epoch 95 - iter 175/252 - loss 0.35986048\n",
      "2019-03-18 02:10:25,893 epoch 95 - iter 200/252 - loss 0.35648958\n",
      "2019-03-18 02:10:31,207 epoch 95 - iter 225/252 - loss 0.35996667\n",
      "2019-03-18 02:10:36,376 epoch 95 - iter 250/252 - loss 0.37745820\n",
      "2019-03-18 02:10:36,527 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:10:36,527 EPOCH 95 done: loss 0.3771 - lr 0.0125 - bad epochs 3\n",
      "2019-03-18 02:10:44,268 DEV  : loss 1.89204776 - f-score 0.9679 - acc 0.9379\n",
      "2019-03-18 02:11:01,835 TEST : loss 2.62823677 - f-score 0.9642 - acc 0.9310\n",
      "Epoch    94: reducing learning rate of group 0 to 6.2500e-03.\n",
      "2019-03-18 02:11:01,836 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:11:02,059 epoch 96 - iter 0/252 - loss 0.23347068\n",
      "2019-03-18 02:11:07,344 epoch 96 - iter 25/252 - loss 0.37572662\n",
      "2019-03-18 02:11:12,730 epoch 96 - iter 50/252 - loss 0.29553903\n",
      "2019-03-18 02:11:18,772 epoch 96 - iter 75/252 - loss 0.32905377\n",
      "2019-03-18 02:11:25,369 epoch 96 - iter 100/252 - loss 0.36158339\n",
      "2019-03-18 02:11:32,177 epoch 96 - iter 125/252 - loss 0.35790393\n",
      "2019-03-18 02:11:38,521 epoch 96 - iter 150/252 - loss 0.37205996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 02:11:44,913 epoch 96 - iter 175/252 - loss 0.36410582\n",
      "2019-03-18 02:11:51,647 epoch 96 - iter 200/252 - loss 0.35694413\n",
      "2019-03-18 02:11:57,995 epoch 96 - iter 225/252 - loss 0.38172091\n",
      "2019-03-18 02:12:04,639 epoch 96 - iter 250/252 - loss 0.37444542\n",
      "2019-03-18 02:12:04,804 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:12:04,805 EPOCH 96 done: loss 0.3741 - lr 0.0063 - bad epochs 0\n",
      "2019-03-18 02:12:14,670 DEV  : loss 1.88458323 - f-score 0.9682 - acc 0.9383\n",
      "2019-03-18 02:12:37,162 TEST : loss 2.62432718 - f-score 0.9640 - acc 0.9305\n",
      "2019-03-18 02:12:37,164 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:12:37,421 epoch 97 - iter 0/252 - loss 0.27676380\n",
      "2019-03-18 02:12:44,105 epoch 97 - iter 25/252 - loss 0.38975389\n",
      "2019-03-18 02:12:50,527 epoch 97 - iter 50/252 - loss 0.36241199\n",
      "2019-03-18 02:12:57,150 epoch 97 - iter 75/252 - loss 0.33688737\n",
      "2019-03-18 02:13:03,037 epoch 97 - iter 100/252 - loss 0.34777943\n",
      "2019-03-18 02:13:08,237 epoch 97 - iter 125/252 - loss 0.33889844\n",
      "2019-03-18 02:13:13,631 epoch 97 - iter 150/252 - loss 0.36178924\n",
      "2019-03-18 02:13:19,322 epoch 97 - iter 175/252 - loss 0.36225011\n",
      "2019-03-18 02:13:24,894 epoch 97 - iter 200/252 - loss 0.36937938\n",
      "2019-03-18 02:13:30,536 epoch 97 - iter 225/252 - loss 0.36785685\n",
      "2019-03-18 02:13:35,952 epoch 97 - iter 250/252 - loss 0.36846646\n",
      "2019-03-18 02:13:36,100 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:13:36,100 EPOCH 97 done: loss 0.3684 - lr 0.0063 - bad epochs 1\n",
      "2019-03-18 02:13:43,749 DEV  : loss 1.90224850 - f-score 0.9685 - acc 0.9389\n",
      "2019-03-18 02:14:04,313 TEST : loss 2.64547372 - f-score 0.9642 - acc 0.9310\n",
      "2019-03-18 02:14:04,315 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:14:04,667 epoch 98 - iter 0/252 - loss 0.17313695\n",
      "2019-03-18 02:14:11,353 epoch 98 - iter 25/252 - loss 0.37372665\n",
      "2019-03-18 02:14:17,870 epoch 98 - iter 50/252 - loss 0.41110922\n",
      "2019-03-18 02:14:24,711 epoch 98 - iter 75/252 - loss 0.41336707\n",
      "2019-03-18 02:14:31,526 epoch 98 - iter 100/252 - loss 0.41909527\n",
      "2019-03-18 02:14:38,037 epoch 98 - iter 125/252 - loss 0.39248434\n",
      "2019-03-18 02:14:44,520 epoch 98 - iter 150/252 - loss 0.39687436\n",
      "2019-03-18 02:14:51,053 epoch 98 - iter 175/252 - loss 0.39224490\n",
      "2019-03-18 02:14:57,812 epoch 98 - iter 200/252 - loss 0.39370612\n",
      "2019-03-18 02:15:04,411 epoch 98 - iter 225/252 - loss 0.39064936\n",
      "2019-03-18 02:15:11,116 epoch 98 - iter 250/252 - loss 0.39317576\n",
      "2019-03-18 02:15:11,278 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:15:11,279 EPOCH 98 done: loss 0.3929 - lr 0.0063 - bad epochs 2\n",
      "2019-03-18 02:15:21,087 DEV  : loss 1.87501442 - f-score 0.9679 - acc 0.9378\n",
      "2019-03-18 02:15:41,629 TEST : loss 2.62403917 - f-score 0.9643 - acc 0.9311\n",
      "2019-03-18 02:15:41,631 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:15:41,898 epoch 99 - iter 0/252 - loss 0.76427650\n",
      "2019-03-18 02:15:47,310 epoch 99 - iter 25/252 - loss 0.38628237\n",
      "2019-03-18 02:15:52,823 epoch 99 - iter 50/252 - loss 0.32123222\n",
      "2019-03-18 02:15:58,259 epoch 99 - iter 75/252 - loss 0.34609612\n",
      "2019-03-18 02:16:03,867 epoch 99 - iter 100/252 - loss 0.33705116\n",
      "2019-03-18 02:16:09,055 epoch 99 - iter 125/252 - loss 0.32570074\n",
      "2019-03-18 02:16:14,942 epoch 99 - iter 150/252 - loss 0.32881960\n",
      "2019-03-18 02:16:20,326 epoch 99 - iter 175/252 - loss 0.31786495\n",
      "2019-03-18 02:16:26,513 epoch 99 - iter 200/252 - loss 0.31880004\n",
      "2019-03-18 02:16:33,085 epoch 99 - iter 225/252 - loss 0.31261245\n",
      "2019-03-18 02:16:39,741 epoch 99 - iter 250/252 - loss 0.31195808\n",
      "2019-03-18 02:16:39,937 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:16:39,938 EPOCH 99 done: loss 0.3127 - lr 0.0063 - bad epochs 3\n",
      "2019-03-18 02:16:49,869 DEV  : loss 1.88686621 - f-score 0.9683 - acc 0.9386\n",
      "2019-03-18 02:17:11,418 TEST : loss 2.61644745 - f-score 0.9645 - acc 0.9314\n",
      "2019-03-18 02:17:25,080 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:17:25,947 epoch 100 - iter 0/252 - loss 0.16646647\n",
      "2019-03-18 02:17:34,182 epoch 100 - iter 25/252 - loss 0.32611111\n",
      "2019-03-18 02:17:40,941 epoch 100 - iter 50/252 - loss 0.29652233\n",
      "2019-03-18 02:17:47,554 epoch 100 - iter 75/252 - loss 0.30469914\n",
      "2019-03-18 02:17:54,323 epoch 100 - iter 100/252 - loss 0.33038644\n",
      "2019-03-18 02:18:00,764 epoch 100 - iter 125/252 - loss 0.33846541\n",
      "2019-03-18 02:18:06,871 epoch 100 - iter 150/252 - loss 0.32533883\n",
      "2019-03-18 02:18:12,093 epoch 100 - iter 175/252 - loss 0.32948264\n",
      "2019-03-18 02:18:17,506 epoch 100 - iter 200/252 - loss 0.33391625\n",
      "2019-03-18 02:18:23,089 epoch 100 - iter 225/252 - loss 0.33775129\n",
      "2019-03-18 02:18:28,744 epoch 100 - iter 250/252 - loss 0.33597231\n",
      "2019-03-18 02:18:28,893 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:18:28,894 EPOCH 100 done: loss 0.3357 - lr 0.0063 - bad epochs 0\n",
      "2019-03-18 02:18:36,801 DEV  : loss 1.89421737 - f-score 0.9679 - acc 0.9379\n",
      "2019-03-18 02:18:53,239 TEST : loss 2.64535046 - f-score 0.9641 - acc 0.9308\n",
      "2019-03-18 02:18:53,242 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:18:53,406 epoch 101 - iter 0/252 - loss 0.06849551\n",
      "2019-03-18 02:18:58,136 epoch 101 - iter 25/252 - loss 0.37721371\n",
      "2019-03-18 02:19:05,089 epoch 101 - iter 50/252 - loss 0.37432203\n",
      "2019-03-18 02:19:11,872 epoch 101 - iter 75/252 - loss 0.37251709\n",
      "2019-03-18 02:19:18,444 epoch 101 - iter 100/252 - loss 0.38353589\n",
      "2019-03-18 02:19:25,018 epoch 101 - iter 125/252 - loss 0.36946912\n",
      "2019-03-18 02:19:31,653 epoch 101 - iter 150/252 - loss 0.36099641\n",
      "2019-03-18 02:19:38,439 epoch 101 - iter 175/252 - loss 0.35493564\n",
      "2019-03-18 02:19:44,674 epoch 101 - iter 200/252 - loss 0.35560590\n",
      "2019-03-18 02:19:51,450 epoch 101 - iter 225/252 - loss 0.36126932\n",
      "2019-03-18 02:19:57,792 epoch 101 - iter 250/252 - loss 0.35912364\n",
      "2019-03-18 02:19:57,967 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:19:57,968 EPOCH 101 done: loss 0.3587 - lr 0.0063 - bad epochs 1\n",
      "2019-03-18 02:20:07,564 DEV  : loss 1.89602625 - f-score 0.9681 - acc 0.9383\n",
      "2019-03-18 02:20:29,977 TEST : loss 2.64807534 - f-score 0.9644 - acc 0.9313\n",
      "2019-03-18 02:20:29,979 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:20:30,275 epoch 102 - iter 0/252 - loss 0.62841821\n",
      "2019-03-18 02:20:36,775 epoch 102 - iter 25/252 - loss 0.30462891\n",
      "2019-03-18 02:20:43,256 epoch 102 - iter 50/252 - loss 0.35466765\n",
      "2019-03-18 02:20:48,934 epoch 102 - iter 75/252 - loss 0.35885770\n",
      "2019-03-18 02:20:54,330 epoch 102 - iter 100/252 - loss 0.38327618\n",
      "2019-03-18 02:20:59,809 epoch 102 - iter 125/252 - loss 0.39091617\n",
      "2019-03-18 02:21:05,423 epoch 102 - iter 150/252 - loss 0.38374525\n",
      "2019-03-18 02:21:10,876 epoch 102 - iter 175/252 - loss 0.36487700\n",
      "2019-03-18 02:21:16,453 epoch 102 - iter 200/252 - loss 0.36746463\n",
      "2019-03-18 02:21:21,648 epoch 102 - iter 225/252 - loss 0.36809511\n",
      "2019-03-18 02:21:27,302 epoch 102 - iter 250/252 - loss 0.37102585\n",
      "2019-03-18 02:21:27,479 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:21:27,480 EPOCH 102 done: loss 0.3706 - lr 0.0063 - bad epochs 2\n",
      "2019-03-18 02:21:35,918 DEV  : loss 1.89389539 - f-score 0.9681 - acc 0.9383\n",
      "2019-03-18 02:21:58,328 TEST : loss 2.64129019 - f-score 0.9646 - acc 0.9316\n",
      "2019-03-18 02:21:58,329 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:21:58,574 epoch 103 - iter 0/252 - loss 0.22766840\n",
      "2019-03-18 02:22:05,372 epoch 103 - iter 25/252 - loss 0.25014224\n",
      "2019-03-18 02:22:11,499 epoch 103 - iter 50/252 - loss 0.30597185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 02:22:18,214 epoch 103 - iter 75/252 - loss 0.29775273\n",
      "2019-03-18 02:22:24,983 epoch 103 - iter 100/252 - loss 0.31117548\n",
      "2019-03-18 02:22:31,646 epoch 103 - iter 125/252 - loss 0.33452622\n",
      "2019-03-18 02:22:38,358 epoch 103 - iter 150/252 - loss 0.34798793\n",
      "2019-03-18 02:22:45,067 epoch 103 - iter 175/252 - loss 0.34291075\n",
      "2019-03-18 02:22:51,651 epoch 103 - iter 200/252 - loss 0.34017011\n",
      "2019-03-18 02:22:58,155 epoch 103 - iter 225/252 - loss 0.33411163\n",
      "2019-03-18 02:23:04,636 epoch 103 - iter 250/252 - loss 0.33395518\n",
      "2019-03-18 02:23:04,816 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:23:04,817 EPOCH 103 done: loss 0.3337 - lr 0.0063 - bad epochs 3\n",
      "2019-03-18 02:23:14,745 DEV  : loss 1.88585496 - f-score 0.9680 - acc 0.9379\n",
      "2019-03-18 02:23:33,486 TEST : loss 2.63816810 - f-score 0.9641 - acc 0.9307\n",
      "Epoch   102: reducing learning rate of group 0 to 3.1250e-03.\n",
      "2019-03-18 02:23:33,488 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:23:33,835 epoch 104 - iter 0/252 - loss 0.36061335\n",
      "2019-03-18 02:23:39,332 epoch 104 - iter 25/252 - loss 0.36070141\n",
      "2019-03-18 02:23:44,730 epoch 104 - iter 50/252 - loss 0.33857125\n",
      "2019-03-18 02:23:50,159 epoch 104 - iter 75/252 - loss 0.31797454\n",
      "2019-03-18 02:23:55,640 epoch 104 - iter 100/252 - loss 0.31246354\n",
      "2019-03-18 02:24:01,099 epoch 104 - iter 125/252 - loss 0.35539653\n",
      "2019-03-18 02:24:06,711 epoch 104 - iter 150/252 - loss 0.35410811\n",
      "2019-03-18 02:24:13,474 epoch 104 - iter 175/252 - loss 0.35917155\n",
      "2019-03-18 02:24:20,385 epoch 104 - iter 200/252 - loss 0.35490368\n",
      "2019-03-18 02:24:27,433 epoch 104 - iter 225/252 - loss 0.34221508\n",
      "2019-03-18 02:24:34,012 epoch 104 - iter 250/252 - loss 0.33749221\n",
      "2019-03-18 02:24:34,206 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:24:34,207 EPOCH 104 done: loss 0.3371 - lr 0.0031 - bad epochs 0\n",
      "2019-03-18 02:24:43,889 DEV  : loss 1.89188147 - f-score 0.9680 - acc 0.9380\n",
      "2019-03-18 02:25:06,293 TEST : loss 2.64747190 - f-score 0.9642 - acc 0.9310\n",
      "2019-03-18 02:25:06,294 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:25:06,605 epoch 105 - iter 0/252 - loss 0.12434363\n",
      "2019-03-18 02:25:13,470 epoch 105 - iter 25/252 - loss 0.37915438\n",
      "2019-03-18 02:25:20,079 epoch 105 - iter 50/252 - loss 0.35118798\n",
      "2019-03-18 02:25:26,681 epoch 105 - iter 75/252 - loss 0.33971918\n",
      "2019-03-18 02:25:33,411 epoch 105 - iter 100/252 - loss 0.34583671\n",
      "2019-03-18 02:25:40,113 epoch 105 - iter 125/252 - loss 0.34248255\n",
      "2019-03-18 02:25:46,824 epoch 105 - iter 150/252 - loss 0.33530042\n",
      "2019-03-18 02:25:53,634 epoch 105 - iter 175/252 - loss 0.35817016\n",
      "2019-03-18 02:25:58,983 epoch 105 - iter 200/252 - loss 0.35579409\n",
      "2019-03-18 02:26:04,583 epoch 105 - iter 225/252 - loss 0.36084185\n",
      "2019-03-18 02:26:10,029 epoch 105 - iter 250/252 - loss 0.36200943\n",
      "2019-03-18 02:26:10,174 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:26:10,175 EPOCH 105 done: loss 0.3616 - lr 0.0031 - bad epochs 1\n",
      "2019-03-18 02:26:18,011 DEV  : loss 1.89779854 - f-score 0.9678 - acc 0.9377\n",
      "2019-03-18 02:26:35,607 TEST : loss 2.65026546 - f-score 0.9643 - acc 0.9311\n",
      "2019-03-18 02:26:35,609 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:26:35,812 epoch 106 - iter 0/252 - loss 0.11225080\n",
      "2019-03-18 02:26:41,614 epoch 106 - iter 25/252 - loss 0.33300547\n",
      "2019-03-18 02:26:48,392 epoch 106 - iter 50/252 - loss 0.33798014\n",
      "2019-03-18 02:26:55,173 epoch 106 - iter 75/252 - loss 0.38501062\n",
      "2019-03-18 02:27:01,995 epoch 106 - iter 100/252 - loss 0.38090658\n",
      "2019-03-18 02:27:08,420 epoch 106 - iter 125/252 - loss 0.36956099\n",
      "2019-03-18 02:27:15,155 epoch 106 - iter 150/252 - loss 0.34577999\n",
      "2019-03-18 02:27:22,159 epoch 106 - iter 175/252 - loss 0.33024066\n",
      "2019-03-18 02:27:28,747 epoch 106 - iter 200/252 - loss 0.32258702\n",
      "2019-03-18 02:27:35,520 epoch 106 - iter 225/252 - loss 0.32865958\n",
      "2019-03-18 02:27:42,280 epoch 106 - iter 250/252 - loss 0.33758218\n",
      "2019-03-18 02:27:42,428 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:27:42,429 EPOCH 106 done: loss 0.3374 - lr 0.0031 - bad epochs 2\n",
      "2019-03-18 02:27:52,061 DEV  : loss 1.90181839 - f-score 0.9686 - acc 0.9391\n",
      "2019-03-18 02:28:13,910 TEST : loss 2.65405416 - f-score 0.9644 - acc 0.9313\n",
      "2019-03-18 02:28:13,912 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:28:14,154 epoch 107 - iter 0/252 - loss 0.19899678\n",
      "2019-03-18 02:28:20,825 epoch 107 - iter 25/252 - loss 0.38835426\n",
      "2019-03-18 02:28:27,204 epoch 107 - iter 50/252 - loss 0.36478713\n",
      "2019-03-18 02:28:32,499 epoch 107 - iter 75/252 - loss 0.33307705\n",
      "2019-03-18 02:28:37,545 epoch 107 - iter 100/252 - loss 0.32854823\n",
      "2019-03-18 02:28:43,027 epoch 107 - iter 125/252 - loss 0.34039849\n",
      "2019-03-18 02:28:48,711 epoch 107 - iter 150/252 - loss 0.33466445\n",
      "2019-03-18 02:28:54,195 epoch 107 - iter 175/252 - loss 0.33875320\n",
      "2019-03-18 02:28:59,705 epoch 107 - iter 200/252 - loss 0.33863083\n",
      "2019-03-18 02:29:05,188 epoch 107 - iter 225/252 - loss 0.34656079\n",
      "2019-03-18 02:29:10,747 epoch 107 - iter 250/252 - loss 0.34608661\n",
      "2019-03-18 02:29:10,891 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:29:10,892 EPOCH 107 done: loss 0.3460 - lr 0.0031 - bad epochs 3\n",
      "2019-03-18 02:29:19,394 DEV  : loss 1.89807606 - f-score 0.9686 - acc 0.9392\n",
      "2019-03-18 02:29:41,225 TEST : loss 2.65556955 - f-score 0.9646 - acc 0.9317\n",
      "Epoch   106: reducing learning rate of group 0 to 1.5625e-03.\n",
      "2019-03-18 02:29:41,226 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:29:41,509 epoch 108 - iter 0/252 - loss 0.06250799\n",
      "2019-03-18 02:29:48,181 epoch 108 - iter 25/252 - loss 0.44437102\n",
      "2019-03-18 02:29:54,836 epoch 108 - iter 50/252 - loss 0.45282532\n",
      "2019-03-18 02:30:01,552 epoch 108 - iter 75/252 - loss 0.40335194\n",
      "2019-03-18 02:30:08,130 epoch 108 - iter 100/252 - loss 0.38221416\n",
      "2019-03-18 02:30:14,819 epoch 108 - iter 125/252 - loss 0.36307220\n",
      "2019-03-18 02:30:21,270 epoch 108 - iter 150/252 - loss 0.37124891\n",
      "2019-03-18 02:30:27,906 epoch 108 - iter 175/252 - loss 0.37498898\n",
      "2019-03-18 02:30:34,871 epoch 108 - iter 200/252 - loss 0.39121554\n",
      "2019-03-18 02:30:41,709 epoch 108 - iter 225/252 - loss 0.39034699\n",
      "2019-03-18 02:30:48,211 epoch 108 - iter 250/252 - loss 0.39082657\n",
      "2019-03-18 02:30:48,414 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:30:48,415 EPOCH 108 done: loss 0.3905 - lr 0.0016 - bad epochs 0\n",
      "2019-03-18 02:30:58,216 DEV  : loss 1.89547884 - f-score 0.9684 - acc 0.9389\n",
      "2019-03-18 02:31:17,459 TEST : loss 2.65363121 - f-score 0.9644 - acc 0.9314\n",
      "2019-03-18 02:31:17,461 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:31:17,675 epoch 109 - iter 0/252 - loss 0.48047709\n",
      "2019-03-18 02:31:23,052 epoch 109 - iter 25/252 - loss 0.28372005\n",
      "2019-03-18 02:31:28,719 epoch 109 - iter 50/252 - loss 0.27501202\n",
      "2019-03-18 02:31:34,483 epoch 109 - iter 75/252 - loss 0.29678892\n",
      "2019-03-18 02:31:40,066 epoch 109 - iter 100/252 - loss 0.31121456\n",
      "2019-03-18 02:31:45,348 epoch 109 - iter 125/252 - loss 0.31364876\n",
      "2019-03-18 02:31:50,002 epoch 109 - iter 150/252 - loss 0.31756246\n",
      "2019-03-18 02:31:54,565 epoch 109 - iter 175/252 - loss 0.31345023\n",
      "2019-03-18 02:32:01,266 epoch 109 - iter 200/252 - loss 0.32230553\n",
      "2019-03-18 02:32:07,823 epoch 109 - iter 225/252 - loss 0.32823275\n",
      "2019-03-18 02:32:14,635 epoch 109 - iter 250/252 - loss 0.32459875\n",
      "2019-03-18 02:32:14,829 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:32:14,829 EPOCH 109 done: loss 0.3245 - lr 0.0016 - bad epochs 1\n",
      "2019-03-18 02:32:24,720 DEV  : loss 1.89587736 - f-score 0.9683 - acc 0.9386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 02:32:47,437 TEST : loss 2.65898895 - f-score 0.9641 - acc 0.9308\n",
      "2019-03-18 02:32:47,439 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:32:47,813 epoch 110 - iter 0/252 - loss 1.07988238\n",
      "2019-03-18 02:32:54,407 epoch 110 - iter 25/252 - loss 0.38733920\n",
      "2019-03-18 02:33:01,270 epoch 110 - iter 50/252 - loss 0.37071088\n",
      "2019-03-18 02:33:08,216 epoch 110 - iter 75/252 - loss 0.39064711\n",
      "2019-03-18 02:33:14,710 epoch 110 - iter 100/252 - loss 0.38503901\n",
      "2019-03-18 02:33:21,418 epoch 110 - iter 125/252 - loss 0.37708403\n",
      "2019-03-18 02:33:28,255 epoch 110 - iter 150/252 - loss 0.38127676\n",
      "2019-03-18 02:33:34,831 epoch 110 - iter 175/252 - loss 0.37965028\n",
      "2019-03-18 02:33:41,598 epoch 110 - iter 200/252 - loss 0.39601203\n",
      "2019-03-18 02:33:46,563 epoch 110 - iter 225/252 - loss 0.39174918\n",
      "2019-03-18 02:33:51,934 epoch 110 - iter 250/252 - loss 0.39652019\n",
      "2019-03-18 02:33:52,151 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:33:52,152 EPOCH 110 done: loss 0.3971 - lr 0.0016 - bad epochs 2\n",
      "2019-03-18 02:34:00,112 DEV  : loss 1.89414895 - f-score 0.9686 - acc 0.9393\n",
      "2019-03-18 02:34:18,299 TEST : loss 2.65680289 - f-score 0.9643 - acc 0.9310\n",
      "2019-03-18 02:34:18,300 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:34:18,540 epoch 111 - iter 0/252 - loss 0.34827828\n",
      "2019-03-18 02:34:24,061 epoch 111 - iter 25/252 - loss 0.37471833\n",
      "2019-03-18 02:34:29,901 epoch 111 - iter 50/252 - loss 0.36532482\n",
      "2019-03-18 02:34:36,651 epoch 111 - iter 75/252 - loss 0.35850168\n",
      "2019-03-18 02:34:43,582 epoch 111 - iter 100/252 - loss 0.38164760\n",
      "2019-03-18 02:34:50,228 epoch 111 - iter 125/252 - loss 0.35564105\n",
      "2019-03-18 02:34:56,454 epoch 111 - iter 150/252 - loss 0.34287595\n",
      "2019-03-18 02:35:03,238 epoch 111 - iter 175/252 - loss 0.33679423\n",
      "2019-03-18 02:35:09,820 epoch 111 - iter 200/252 - loss 0.34685106\n",
      "2019-03-18 02:35:16,151 epoch 111 - iter 225/252 - loss 0.33637801\n",
      "2019-03-18 02:35:22,841 epoch 111 - iter 250/252 - loss 0.34434725\n",
      "2019-03-18 02:35:23,045 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:35:23,045 EPOCH 111 done: loss 0.3440 - lr 0.0016 - bad epochs 3\n",
      "2019-03-18 02:35:32,923 DEV  : loss 1.90159285 - f-score 0.9683 - acc 0.9387\n",
      "2019-03-18 02:35:54,880 TEST : loss 2.66183162 - f-score 0.9642 - acc 0.9310\n",
      "Epoch   110: reducing learning rate of group 0 to 7.8125e-04.\n",
      "2019-03-18 02:35:54,881 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:35:55,121 epoch 112 - iter 0/252 - loss 0.27495965\n",
      "2019-03-18 02:36:01,676 epoch 112 - iter 25/252 - loss 0.27220837\n",
      "2019-03-18 02:36:08,289 epoch 112 - iter 50/252 - loss 0.30635318\n",
      "2019-03-18 02:36:14,756 epoch 112 - iter 75/252 - loss 0.30546714\n",
      "2019-03-18 02:36:20,342 epoch 112 - iter 100/252 - loss 0.35380642\n",
      "2019-03-18 02:36:25,744 epoch 112 - iter 125/252 - loss 0.36873266\n",
      "2019-03-18 02:36:31,109 epoch 112 - iter 150/252 - loss 0.38855476\n",
      "2019-03-18 02:36:36,633 epoch 112 - iter 175/252 - loss 0.38031837\n",
      "2019-03-18 02:36:42,352 epoch 112 - iter 200/252 - loss 0.39203541\n",
      "2019-03-18 02:36:47,855 epoch 112 - iter 225/252 - loss 0.37471192\n",
      "2019-03-18 02:36:53,390 epoch 112 - iter 250/252 - loss 0.37432157\n",
      "2019-03-18 02:36:53,545 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:36:53,546 EPOCH 112 done: loss 0.3739 - lr 0.0008 - bad epochs 0\n",
      "2019-03-18 02:37:01,331 DEV  : loss 1.89994621 - f-score 0.9684 - acc 0.9388\n",
      "2019-03-18 02:37:16,854 TEST : loss 2.66028428 - f-score 0.9642 - acc 0.9309\n",
      "2019-03-18 02:37:16,856 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:37:17,110 epoch 113 - iter 0/252 - loss 0.05062366\n",
      "2019-03-18 02:37:23,617 epoch 113 - iter 25/252 - loss 0.27652512\n",
      "2019-03-18 02:37:30,288 epoch 113 - iter 50/252 - loss 0.25976096\n",
      "2019-03-18 02:37:37,108 epoch 113 - iter 75/252 - loss 0.27564986\n",
      "2019-03-18 02:37:43,857 epoch 113 - iter 100/252 - loss 0.30939535\n",
      "2019-03-18 02:37:50,639 epoch 113 - iter 125/252 - loss 0.31350575\n",
      "2019-03-18 02:37:57,256 epoch 113 - iter 150/252 - loss 0.34019070\n",
      "2019-03-18 02:38:04,086 epoch 113 - iter 175/252 - loss 0.35440461\n",
      "2019-03-18 02:38:10,704 epoch 113 - iter 200/252 - loss 0.35051091\n",
      "2019-03-18 02:38:17,434 epoch 113 - iter 225/252 - loss 0.35432629\n",
      "2019-03-18 02:38:23,952 epoch 113 - iter 250/252 - loss 0.35655466\n",
      "2019-03-18 02:38:24,191 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:38:24,192 EPOCH 113 done: loss 0.3566 - lr 0.0008 - bad epochs 1\n",
      "2019-03-18 02:38:34,070 DEV  : loss 1.90080547 - f-score 0.9684 - acc 0.9388\n",
      "2019-03-18 02:38:56,446 TEST : loss 2.66090107 - f-score 0.9642 - acc 0.9309\n",
      "2019-03-18 02:38:56,447 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:38:56,738 epoch 114 - iter 0/252 - loss 0.57678342\n",
      "2019-03-18 02:39:01,980 epoch 114 - iter 25/252 - loss 0.31262663\n",
      "2019-03-18 02:39:07,358 epoch 114 - iter 50/252 - loss 0.34438571\n",
      "2019-03-18 02:39:12,958 epoch 114 - iter 75/252 - loss 0.33967226\n",
      "2019-03-18 02:39:18,698 epoch 114 - iter 100/252 - loss 0.34333754\n",
      "2019-03-18 02:39:24,241 epoch 114 - iter 125/252 - loss 0.33334264\n",
      "2019-03-18 02:39:29,655 epoch 114 - iter 150/252 - loss 0.33944563\n",
      "2019-03-18 02:39:35,280 epoch 114 - iter 175/252 - loss 0.33041351\n",
      "2019-03-18 02:39:40,643 epoch 114 - iter 200/252 - loss 0.32314088\n",
      "2019-03-18 02:39:46,328 epoch 114 - iter 225/252 - loss 0.32834464\n",
      "2019-03-18 02:39:52,884 epoch 114 - iter 250/252 - loss 0.33068043\n",
      "2019-03-18 02:39:53,052 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:39:53,053 EPOCH 114 done: loss 0.3317 - lr 0.0008 - bad epochs 2\n",
      "2019-03-18 02:40:03,137 DEV  : loss 1.90173984 - f-score 0.9684 - acc 0.9387\n",
      "2019-03-18 02:40:25,711 TEST : loss 2.66192317 - f-score 0.9642 - acc 0.9310\n",
      "2019-03-18 02:40:25,712 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:40:26,010 epoch 115 - iter 0/252 - loss 0.42447519\n",
      "2019-03-18 02:40:32,435 epoch 115 - iter 25/252 - loss 0.37147861\n",
      "2019-03-18 02:40:39,231 epoch 115 - iter 50/252 - loss 0.35066262\n",
      "2019-03-18 02:40:45,853 epoch 115 - iter 75/252 - loss 0.35784595\n",
      "2019-03-18 02:40:52,599 epoch 115 - iter 100/252 - loss 0.36627943\n",
      "2019-03-18 02:40:59,497 epoch 115 - iter 125/252 - loss 0.36231233\n",
      "2019-03-18 02:41:06,066 epoch 115 - iter 150/252 - loss 0.35115116\n",
      "2019-03-18 02:41:12,664 epoch 115 - iter 175/252 - loss 0.39008337\n",
      "2019-03-18 02:41:19,246 epoch 115 - iter 200/252 - loss 0.38474777\n",
      "2019-03-18 02:41:25,889 epoch 115 - iter 225/252 - loss 0.36514370\n",
      "2019-03-18 02:41:32,684 epoch 115 - iter 250/252 - loss 0.36715964\n",
      "2019-03-18 02:41:32,846 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:41:32,847 EPOCH 115 done: loss 0.3668 - lr 0.0008 - bad epochs 3\n",
      "2019-03-18 02:41:40,845 DEV  : loss 1.90219080 - f-score 0.9683 - acc 0.9385\n",
      "2019-03-18 02:41:59,083 TEST : loss 2.66026044 - f-score 0.9642 - acc 0.9309\n",
      "Epoch   114: reducing learning rate of group 0 to 3.9063e-04.\n",
      "2019-03-18 02:41:59,084 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:41:59,295 epoch 116 - iter 0/252 - loss 0.41572809\n",
      "2019-03-18 02:42:04,529 epoch 116 - iter 25/252 - loss 0.43241162\n",
      "2019-03-18 02:42:09,802 epoch 116 - iter 50/252 - loss 0.37940330\n",
      "2019-03-18 02:42:15,190 epoch 116 - iter 75/252 - loss 0.39232861\n",
      "2019-03-18 02:42:20,685 epoch 116 - iter 100/252 - loss 0.38716190\n",
      "2019-03-18 02:42:27,574 epoch 116 - iter 125/252 - loss 0.37708719\n",
      "2019-03-18 02:42:34,095 epoch 116 - iter 150/252 - loss 0.37682373\n",
      "2019-03-18 02:42:40,938 epoch 116 - iter 175/252 - loss 0.37510782\n",
      "2019-03-18 02:42:47,289 epoch 116 - iter 200/252 - loss 0.37683469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 02:42:54,008 epoch 116 - iter 225/252 - loss 0.37198618\n",
      "2019-03-18 02:43:00,741 epoch 116 - iter 250/252 - loss 0.37387157\n",
      "2019-03-18 02:43:00,892 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:43:00,893 EPOCH 116 done: loss 0.3746 - lr 0.0004 - bad epochs 0\n",
      "2019-03-18 02:43:10,772 DEV  : loss 1.90232861 - f-score 0.9683 - acc 0.9386\n",
      "2019-03-18 02:43:33,067 TEST : loss 2.66108942 - f-score 0.9643 - acc 0.9311\n",
      "2019-03-18 02:43:33,069 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:43:33,328 epoch 117 - iter 0/252 - loss 0.55773926\n",
      "2019-03-18 02:43:40,057 epoch 117 - iter 25/252 - loss 0.40707734\n",
      "2019-03-18 02:43:46,612 epoch 117 - iter 50/252 - loss 0.41808533\n",
      "2019-03-18 02:43:53,041 epoch 117 - iter 75/252 - loss 0.35383529\n",
      "2019-03-18 02:43:59,719 epoch 117 - iter 100/252 - loss 0.38286741\n",
      "2019-03-18 02:44:06,500 epoch 117 - iter 125/252 - loss 0.39030026\n",
      "2019-03-18 02:44:12,060 epoch 117 - iter 150/252 - loss 0.39364751\n",
      "2019-03-18 02:44:17,382 epoch 117 - iter 175/252 - loss 0.39492580\n",
      "2019-03-18 02:44:22,663 epoch 117 - iter 200/252 - loss 0.39303329\n",
      "2019-03-18 02:44:28,679 epoch 117 - iter 225/252 - loss 0.40253775\n",
      "2019-03-18 02:44:34,261 epoch 117 - iter 250/252 - loss 0.41792976\n",
      "2019-03-18 02:44:34,426 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:44:34,426 EPOCH 117 done: loss 0.4175 - lr 0.0004 - bad epochs 1\n",
      "2019-03-18 02:44:42,358 DEV  : loss 1.90150452 - f-score 0.9683 - acc 0.9386\n",
      "2019-03-18 02:45:01,943 TEST : loss 2.65924478 - f-score 0.9642 - acc 0.9309\n",
      "2019-03-18 02:45:01,944 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:45:02,315 epoch 118 - iter 0/252 - loss 0.01231289\n",
      "2019-03-18 02:45:08,779 epoch 118 - iter 25/252 - loss 0.26241869\n",
      "2019-03-18 02:45:15,488 epoch 118 - iter 50/252 - loss 0.27710901\n",
      "2019-03-18 02:45:22,501 epoch 118 - iter 75/252 - loss 0.28123916\n",
      "2019-03-18 02:45:29,143 epoch 118 - iter 100/252 - loss 0.29891133\n",
      "2019-03-18 02:45:35,340 epoch 118 - iter 125/252 - loss 0.32269773\n",
      "2019-03-18 02:45:41,835 epoch 118 - iter 150/252 - loss 0.31119404\n",
      "2019-03-18 02:45:48,487 epoch 118 - iter 175/252 - loss 0.33449483\n",
      "2019-03-18 02:45:55,154 epoch 118 - iter 200/252 - loss 0.35351567\n",
      "2019-03-18 02:46:01,935 epoch 118 - iter 225/252 - loss 0.34610350\n",
      "2019-03-18 02:46:08,697 epoch 118 - iter 250/252 - loss 0.34650804\n",
      "2019-03-18 02:46:08,831 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:46:08,832 EPOCH 118 done: loss 0.3469 - lr 0.0004 - bad epochs 2\n",
      "2019-03-18 02:46:18,838 DEV  : loss 1.90317655 - f-score 0.9683 - acc 0.9386\n",
      "2019-03-18 02:46:41,136 TEST : loss 2.66105056 - f-score 0.9642 - acc 0.9310\n",
      "2019-03-18 02:46:41,138 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:46:41,344 epoch 119 - iter 0/252 - loss 0.42851186\n",
      "2019-03-18 02:46:46,780 epoch 119 - iter 25/252 - loss 0.37922511\n",
      "2019-03-18 02:46:52,331 epoch 119 - iter 50/252 - loss 0.34969662\n",
      "2019-03-18 02:46:58,083 epoch 119 - iter 75/252 - loss 0.34646083\n",
      "2019-03-18 02:47:03,517 epoch 119 - iter 100/252 - loss 0.34162091\n",
      "2019-03-18 02:47:09,060 epoch 119 - iter 125/252 - loss 0.34960372\n",
      "2019-03-18 02:47:14,516 epoch 119 - iter 150/252 - loss 0.33923376\n",
      "2019-03-18 02:47:20,155 epoch 119 - iter 175/252 - loss 0.33073913\n",
      "2019-03-18 02:47:25,451 epoch 119 - iter 200/252 - loss 0.35521828\n",
      "2019-03-18 02:47:31,428 epoch 119 - iter 225/252 - loss 0.35576238\n",
      "2019-03-18 02:47:38,234 epoch 119 - iter 250/252 - loss 0.35411468\n",
      "2019-03-18 02:47:38,385 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:47:38,386 EPOCH 119 done: loss 0.3538 - lr 0.0004 - bad epochs 3\n",
      "2019-03-18 02:47:48,383 DEV  : loss 1.90380609 - f-score 0.9683 - acc 0.9386\n",
      "2019-03-18 02:48:10,461 TEST : loss 2.66188025 - f-score 0.9642 - acc 0.9309\n",
      "Epoch   118: reducing learning rate of group 0 to 1.9531e-04.\n",
      "2019-03-18 02:48:10,463 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:48:10,760 epoch 120 - iter 0/252 - loss 0.07135725\n",
      "2019-03-18 02:48:17,246 epoch 120 - iter 25/252 - loss 0.33552633\n",
      "2019-03-18 02:48:23,830 epoch 120 - iter 50/252 - loss 0.32249106\n",
      "2019-03-18 02:48:30,578 epoch 120 - iter 75/252 - loss 0.34057302\n",
      "2019-03-18 02:48:37,185 epoch 120 - iter 100/252 - loss 0.34094197\n",
      "2019-03-18 02:48:43,771 epoch 120 - iter 125/252 - loss 0.35575838\n",
      "2019-03-18 02:48:50,299 epoch 120 - iter 150/252 - loss 0.33534525\n",
      "2019-03-18 02:48:56,981 epoch 120 - iter 175/252 - loss 0.33977782\n",
      "2019-03-18 02:49:03,603 epoch 120 - iter 200/252 - loss 0.33888282\n",
      "2019-03-18 02:49:10,256 epoch 120 - iter 225/252 - loss 0.34136418\n",
      "2019-03-18 02:49:16,648 epoch 120 - iter 250/252 - loss 0.34307799\n",
      "2019-03-18 02:49:16,792 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:49:16,793 EPOCH 120 done: loss 0.3442 - lr 0.0002 - bad epochs 0\n",
      "2019-03-18 02:49:24,394 DEV  : loss 1.90373158 - f-score 0.9683 - acc 0.9386\n",
      "2019-03-18 02:49:42,601 TEST : loss 2.66162181 - f-score 0.9642 - acc 0.9310\n",
      "2019-03-18 02:49:42,603 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:49:42,872 epoch 121 - iter 0/252 - loss 0.30362225\n",
      "2019-03-18 02:49:48,327 epoch 121 - iter 25/252 - loss 0.22661371\n",
      "2019-03-18 02:49:53,752 epoch 121 - iter 50/252 - loss 0.26139656\n",
      "2019-03-18 02:49:59,096 epoch 121 - iter 75/252 - loss 0.28374771\n",
      "2019-03-18 02:50:03,960 epoch 121 - iter 100/252 - loss 0.27402361\n",
      "2019-03-18 02:50:09,631 epoch 121 - iter 125/252 - loss 0.27944687\n",
      "2019-03-18 02:50:16,690 epoch 121 - iter 150/252 - loss 0.29585809\n",
      "2019-03-18 02:50:23,233 epoch 121 - iter 175/252 - loss 0.29715873\n",
      "2019-03-18 02:50:30,234 epoch 121 - iter 200/252 - loss 0.30432100\n",
      "2019-03-18 02:50:36,732 epoch 121 - iter 225/252 - loss 0.29979846\n",
      "2019-03-18 02:50:43,082 epoch 121 - iter 250/252 - loss 0.30164294\n",
      "2019-03-18 02:50:43,264 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:50:43,265 EPOCH 121 done: loss 0.3013 - lr 0.0002 - bad epochs 1\n",
      "2019-03-18 02:50:52,610 DEV  : loss 1.90341377 - f-score 0.9683 - acc 0.9386\n",
      "2019-03-18 02:51:15,114 TEST : loss 2.66155648 - f-score 0.9643 - acc 0.9310\n",
      "2019-03-18 02:51:30,758 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:51:31,022 epoch 122 - iter 0/252 - loss 0.28694773\n",
      "2019-03-18 02:51:37,731 epoch 122 - iter 25/252 - loss 0.39873989\n",
      "2019-03-18 02:51:44,499 epoch 122 - iter 50/252 - loss 0.35838409\n",
      "2019-03-18 02:51:50,705 epoch 122 - iter 75/252 - loss 0.33082172\n",
      "2019-03-18 02:51:56,127 epoch 122 - iter 100/252 - loss 0.34721359\n",
      "2019-03-18 02:52:01,532 epoch 122 - iter 125/252 - loss 0.34918795\n",
      "2019-03-18 02:52:07,106 epoch 122 - iter 150/252 - loss 0.36145175\n",
      "2019-03-18 02:52:12,774 epoch 122 - iter 175/252 - loss 0.35992011\n",
      "2019-03-18 02:52:18,120 epoch 122 - iter 200/252 - loss 0.35336140\n",
      "2019-03-18 02:52:23,563 epoch 122 - iter 225/252 - loss 0.34352116\n",
      "2019-03-18 02:52:28,899 epoch 122 - iter 250/252 - loss 0.33683596\n",
      "2019-03-18 02:52:29,092 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:52:29,093 EPOCH 122 done: loss 0.3373 - lr 0.0002 - bad epochs 0\n",
      "2019-03-18 02:52:37,275 DEV  : loss 1.90385234 - f-score 0.9683 - acc 0.9386\n",
      "2019-03-18 02:53:00,018 TEST : loss 2.66212177 - f-score 0.9643 - acc 0.9311\n",
      "2019-03-18 02:53:00,021 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:53:00,315 epoch 123 - iter 0/252 - loss 0.11495113\n",
      "2019-03-18 02:53:06,809 epoch 123 - iter 25/252 - loss 0.26580662\n",
      "2019-03-18 02:53:13,724 epoch 123 - iter 50/252 - loss 0.32695049\n",
      "2019-03-18 02:53:20,221 epoch 123 - iter 75/252 - loss 0.33555779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18 02:53:26,913 epoch 123 - iter 100/252 - loss 0.35942106\n",
      "2019-03-18 02:53:33,517 epoch 123 - iter 125/252 - loss 0.36980523\n",
      "2019-03-18 02:53:40,228 epoch 123 - iter 150/252 - loss 0.37346624\n",
      "2019-03-18 02:53:46,613 epoch 123 - iter 175/252 - loss 0.37139376\n",
      "2019-03-18 02:53:53,313 epoch 123 - iter 200/252 - loss 0.36535145\n",
      "2019-03-18 02:54:00,048 epoch 123 - iter 225/252 - loss 0.35931171\n",
      "2019-03-18 02:54:06,845 epoch 123 - iter 250/252 - loss 0.35805020\n",
      "2019-03-18 02:54:07,019 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:54:07,020 EPOCH 123 done: loss 0.3578 - lr 0.0002 - bad epochs 1\n",
      "2019-03-18 02:54:17,562 DEV  : loss 1.90360594 - f-score 0.9683 - acc 0.9386\n",
      "2019-03-18 02:54:36,297 TEST : loss 2.66191983 - f-score 0.9642 - acc 0.9310\n",
      "2019-03-18 02:54:36,299 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:54:36,518 epoch 124 - iter 0/252 - loss 0.34017420\n",
      "2019-03-18 02:54:41,877 epoch 124 - iter 25/252 - loss 0.26812140\n",
      "2019-03-18 02:54:47,284 epoch 124 - iter 50/252 - loss 0.31261516\n",
      "2019-03-18 02:54:52,920 epoch 124 - iter 75/252 - loss 0.34599318\n",
      "2019-03-18 02:54:58,393 epoch 124 - iter 100/252 - loss 0.36800561\n",
      "2019-03-18 02:55:03,704 epoch 124 - iter 125/252 - loss 0.37299288\n",
      "2019-03-18 02:55:08,991 epoch 124 - iter 150/252 - loss 0.37024711\n",
      "2019-03-18 02:55:13,311 epoch 124 - iter 175/252 - loss 0.36503388\n",
      "2019-03-18 02:55:18,745 epoch 124 - iter 200/252 - loss 0.35673383\n",
      "2019-03-18 02:55:25,683 epoch 124 - iter 225/252 - loss 0.35320369\n",
      "2019-03-18 02:55:32,489 epoch 124 - iter 250/252 - loss 0.34966305\n",
      "2019-03-18 02:55:32,668 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:55:32,669 EPOCH 124 done: loss 0.3499 - lr 0.0002 - bad epochs 2\n",
      "2019-03-18 02:55:42,358 DEV  : loss 1.90353000 - f-score 0.9683 - acc 0.9386\n",
      "2019-03-18 02:56:03,410 TEST : loss 2.66164422 - f-score 0.9643 - acc 0.9310\n",
      "2019-03-18 02:56:03,411 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:56:03,729 epoch 125 - iter 0/252 - loss 0.69172573\n",
      "2019-03-18 02:56:10,346 epoch 125 - iter 25/252 - loss 0.29961380\n",
      "2019-03-18 02:56:16,864 epoch 125 - iter 50/252 - loss 0.34176750\n",
      "2019-03-18 02:56:23,080 epoch 125 - iter 75/252 - loss 0.34978682\n",
      "2019-03-18 02:56:29,582 epoch 125 - iter 100/252 - loss 0.34534382\n",
      "2019-03-18 02:56:36,161 epoch 125 - iter 125/252 - loss 0.36077846\n",
      "2019-03-18 02:56:42,815 epoch 125 - iter 150/252 - loss 0.35257545\n",
      "2019-03-18 02:56:49,330 epoch 125 - iter 175/252 - loss 0.34000874\n",
      "2019-03-18 02:56:55,926 epoch 125 - iter 200/252 - loss 0.34264099\n",
      "2019-03-18 02:57:02,495 epoch 125 - iter 225/252 - loss 0.34698807\n",
      "2019-03-18 02:57:08,327 epoch 125 - iter 250/252 - loss 0.34820441\n",
      "2019-03-18 02:57:08,464 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:57:08,464 EPOCH 125 done: loss 0.3483 - lr 0.0002 - bad epochs 3\n",
      "2019-03-18 02:57:16,361 DEV  : loss 1.90334857 - f-score 0.9683 - acc 0.9386\n",
      "2019-03-18 02:57:34,648 TEST : loss 2.66154766 - f-score 0.9642 - acc 0.9309\n",
      "Epoch   124: reducing learning rate of group 0 to 9.7656e-05.\n",
      "2019-03-18 02:57:34,649 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:57:34,650 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:57:34,650 learning rate too small - quitting training!\n",
      "2019-03-18 02:57:34,650 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:57:47,877 ----------------------------------------------------------------------------------------------------\n",
      "2019-03-18 02:57:47,942 Testing using best model ...\n",
      "2019-03-18 02:57:47,944 loading file models/flair/np-chunking/best-model.pt\n",
      "2019-03-18 02:58:18,764 MICRO_AVG: acc 0.931 - f1-score 0.9643\n",
      "2019-03-18 02:58:18,765 MACRO_AVG: acc 0.6811 - f1-score 0.76209\n",
      "2019-03-18 02:58:18,766 ADJP       tp: 355 - fp: 69 - fn: 83 - tn: 355 - precision: 0.8373 - recall: 0.8105 - accuracy: 0.7002 - f1-score: 0.8237\n",
      "2019-03-18 02:58:18,766 ADVP       tp: 741 - fp: 114 - fn: 125 - tn: 741 - precision: 0.8667 - recall: 0.8557 - accuracy: 0.7561 - f1-score: 0.8612\n",
      "2019-03-18 02:58:18,766 CONJP      tp: 6 - fp: 6 - fn: 3 - tn: 6 - precision: 0.5000 - recall: 0.6667 - accuracy: 0.4000 - f1-score: 0.5714\n",
      "2019-03-18 02:58:18,767 INTJ       tp: 1 - fp: 0 - fn: 1 - tn: 1 - precision: 1.0000 - recall: 0.5000 - accuracy: 0.5000 - f1-score: 0.6667\n",
      "2019-03-18 02:58:18,767 LST        tp: 0 - fp: 0 - fn: 5 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "2019-03-18 02:58:18,767 NP         tp: 12029 - fp: 356 - fn: 393 - tn: 12029 - precision: 0.9713 - recall: 0.9684 - accuracy: 0.9414 - f1-score: 0.9698\n",
      "2019-03-18 02:58:18,768 PP         tp: 4761 - fp: 82 - fn: 50 - tn: 4761 - precision: 0.9831 - recall: 0.9896 - accuracy: 0.9730 - f1-score: 0.9863\n",
      "2019-03-18 02:58:18,768 PRT        tp: 94 - fp: 26 - fn: 12 - tn: 94 - precision: 0.7833 - recall: 0.8868 - accuracy: 0.7121 - f1-score: 0.8318\n",
      "2019-03-18 02:58:18,768 SBAR       tp: 507 - fp: 32 - fn: 28 - tn: 507 - precision: 0.9406 - recall: 0.9477 - accuracy: 0.8942 - f1-score: 0.9441\n",
      "2019-03-18 02:58:18,769 VP         tp: 4501 - fp: 161 - fn: 157 - tn: 4501 - precision: 0.9655 - recall: 0.9663 - accuracy: 0.9340 - f1-score: 0.9659\n",
      "2019-03-18 02:58:18,769 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.9643,\n",
       " 'dev_score_history': [0.9271,\n",
       "  0.9388,\n",
       "  0.9475,\n",
       "  0.9518,\n",
       "  0.9543,\n",
       "  0.9566,\n",
       "  0.9574,\n",
       "  0.9603,\n",
       "  0.9598,\n",
       "  0.9585,\n",
       "  0.9606,\n",
       "  0.9613,\n",
       "  0.9629,\n",
       "  0.9628,\n",
       "  0.9628,\n",
       "  0.9636,\n",
       "  0.9649,\n",
       "  0.9648,\n",
       "  0.9628,\n",
       "  0.9641,\n",
       "  0.9651,\n",
       "  0.9654,\n",
       "  0.9656,\n",
       "  0.964,\n",
       "  0.9669,\n",
       "  0.9645,\n",
       "  0.9661,\n",
       "  0.9654,\n",
       "  0.966,\n",
       "  0.9655,\n",
       "  0.965,\n",
       "  0.9662,\n",
       "  0.9654,\n",
       "  0.9675,\n",
       "  0.9663,\n",
       "  0.9665,\n",
       "  0.9659,\n",
       "  0.9659,\n",
       "  0.9654,\n",
       "  0.9667,\n",
       "  0.967,\n",
       "  0.9647,\n",
       "  0.9647,\n",
       "  0.9676,\n",
       "  0.9648,\n",
       "  0.9658,\n",
       "  0.9661,\n",
       "  0.9647,\n",
       "  0.9659,\n",
       "  0.9643,\n",
       "  0.9645,\n",
       "  0.965,\n",
       "  0.9657,\n",
       "  0.966,\n",
       "  0.9654,\n",
       "  0.9666,\n",
       "  0.9678,\n",
       "  0.9662,\n",
       "  0.9663,\n",
       "  0.9672,\n",
       "  0.968,\n",
       "  0.9677,\n",
       "  0.9676,\n",
       "  0.9677,\n",
       "  0.9674,\n",
       "  0.9668,\n",
       "  0.9674,\n",
       "  0.9683,\n",
       "  0.9686,\n",
       "  0.9682,\n",
       "  0.9682,\n",
       "  0.9684,\n",
       "  0.9676,\n",
       "  0.968,\n",
       "  0.9675,\n",
       "  0.968,\n",
       "  0.9685,\n",
       "  0.9681,\n",
       "  0.9684,\n",
       "  0.9674,\n",
       "  0.9682,\n",
       "  0.9682,\n",
       "  0.9683,\n",
       "  0.9686,\n",
       "  0.9681,\n",
       "  0.9688,\n",
       "  0.9688,\n",
       "  0.968,\n",
       "  0.9687,\n",
       "  0.9688,\n",
       "  0.9677,\n",
       "  0.9677,\n",
       "  0.968,\n",
       "  0.968,\n",
       "  0.9679,\n",
       "  0.9682,\n",
       "  0.9685,\n",
       "  0.9679,\n",
       "  0.9683,\n",
       "  0.9679,\n",
       "  0.9681,\n",
       "  0.9681,\n",
       "  0.968,\n",
       "  0.968,\n",
       "  0.9678,\n",
       "  0.9686,\n",
       "  0.9686,\n",
       "  0.9684,\n",
       "  0.9683,\n",
       "  0.9686,\n",
       "  0.9683,\n",
       "  0.9684,\n",
       "  0.9684,\n",
       "  0.9684,\n",
       "  0.9683,\n",
       "  0.9683,\n",
       "  0.9683,\n",
       "  0.9683,\n",
       "  0.9683,\n",
       "  0.9683,\n",
       "  0.9683,\n",
       "  0.9683,\n",
       "  0.9683,\n",
       "  0.9683,\n",
       "  0.9683],\n",
       " 'train_loss_history': [16.720992073375946,\n",
       "  5.269755555811998,\n",
       "  3.9507932394957903,\n",
       "  3.387568364478972,\n",
       "  2.9793217376048693,\n",
       "  2.6165541217446298,\n",
       "  2.4414518148084983,\n",
       "  2.28909260513355,\n",
       "  2.1937302724842525,\n",
       "  2.0296689939273587,\n",
       "  1.9308727604036633,\n",
       "  1.867931321852541,\n",
       "  1.7474522807650885,\n",
       "  1.619420184154648,\n",
       "  1.5768350804216735,\n",
       "  1.4928307347083027,\n",
       "  1.5011652723766329,\n",
       "  1.4338798356927827,\n",
       "  1.3684357350455443,\n",
       "  1.2778343315949876,\n",
       "  1.306093881213822,\n",
       "  1.2272791209075025,\n",
       "  1.200146103996271,\n",
       "  1.1487277081225589,\n",
       "  1.0887755349156159,\n",
       "  1.110480617688266,\n",
       "  1.084800169735339,\n",
       "  1.0284371536129655,\n",
       "  1.0307909329416027,\n",
       "  0.9745331791258961,\n",
       "  0.9473572984556451,\n",
       "  0.9423762017653019,\n",
       "  0.9493649574513163,\n",
       "  0.9002600987436048,\n",
       "  0.9530386528664043,\n",
       "  0.9025180523029699,\n",
       "  0.8569609607164316,\n",
       "  0.8897171524501327,\n",
       "  0.8180934703934106,\n",
       "  0.7866225121656303,\n",
       "  0.7707186448458444,\n",
       "  0.822323186277187,\n",
       "  0.7819618415073565,\n",
       "  0.7343782908997706,\n",
       "  0.7361719186731133,\n",
       "  0.7320800371888679,\n",
       "  0.7325897672169839,\n",
       "  0.7660025908738871,\n",
       "  0.6768092420142078,\n",
       "  0.6752222834816918,\n",
       "  0.723260604751196,\n",
       "  0.6976990652333382,\n",
       "  0.7004826697980667,\n",
       "  0.6657990235293335,\n",
       "  0.6314154363811031,\n",
       "  0.6552207400566961,\n",
       "  0.6680199875223254,\n",
       "  0.6923843638513314,\n",
       "  0.6407937812615437,\n",
       "  0.5658855753673069,\n",
       "  0.5383416926258981,\n",
       "  0.5266621176267375,\n",
       "  0.47108049779292865,\n",
       "  0.48259799808330484,\n",
       "  0.5115083260667825,\n",
       "  0.5067501081159772,\n",
       "  0.5363812618544966,\n",
       "  0.5194134501134896,\n",
       "  0.4858944414267342,\n",
       "  0.47104384797747767,\n",
       "  0.46948417826275185,\n",
       "  0.4688638017356055,\n",
       "  0.403096718208258,\n",
       "  0.4185396263001126,\n",
       "  0.5080621656983743,\n",
       "  0.39825525950270785,\n",
       "  0.40893305845149147,\n",
       "  0.4195321957527242,\n",
       "  0.43944305449805726,\n",
       "  0.41284681564241876,\n",
       "  0.408639991084784,\n",
       "  0.38898783126182146,\n",
       "  0.37681667877648845,\n",
       "  0.4045308370905651,\n",
       "  0.3851754735933848,\n",
       "  0.3714191119666717,\n",
       "  0.38682788119805867,\n",
       "  0.3659161257227983,\n",
       "  0.3746864388339468,\n",
       "  0.3892991994038591,\n",
       "  0.36059378628943045,\n",
       "  0.4218644378375604,\n",
       "  0.408271616651359,\n",
       "  0.3776341977625811,\n",
       "  0.3771406662046124,\n",
       "  0.3741257434400033,\n",
       "  0.36835094092349396,\n",
       "  0.39293407961018373,\n",
       "  0.3126894643252775,\n",
       "  0.33565628276598214,\n",
       "  0.35874447814269733,\n",
       "  0.370574633359494,\n",
       "  0.33366196928140507,\n",
       "  0.337098777427569,\n",
       "  0.3616461567664081,\n",
       "  0.33736263653200915,\n",
       "  0.345951125859562,\n",
       "  0.3905420471737854,\n",
       "  0.32447876025420225,\n",
       "  0.39708199118002524,\n",
       "  0.3440287492074234,\n",
       "  0.3738927531675212,\n",
       "  0.35659749043637085,\n",
       "  0.33168709168413985,\n",
       "  0.3668066211910101,\n",
       "  0.37464969426771505,\n",
       "  0.417534148565604,\n",
       "  0.34693526189975316,\n",
       "  0.35384594934611024,\n",
       "  0.34420053106847315,\n",
       "  0.3013059371325899,\n",
       "  0.33729426156997205,\n",
       "  0.35784635233126066,\n",
       "  0.34985963713022883,\n",
       "  0.3483097125748087],\n",
       " 'dev_loss_history': [4.251392364501953,\n",
       "  3.115294933319092,\n",
       "  2.419658899307251,\n",
       "  2.244626760482788,\n",
       "  1.898305058479309,\n",
       "  1.829163670539856,\n",
       "  1.6875157356262207,\n",
       "  1.6178892850875854,\n",
       "  1.5612248182296753,\n",
       "  1.6594709157943726,\n",
       "  1.5773605108261108,\n",
       "  1.514026403427124,\n",
       "  1.454036831855774,\n",
       "  1.4544142484664917,\n",
       "  1.4332369565963745,\n",
       "  1.3893011808395386,\n",
       "  1.4140080213546753,\n",
       "  1.4283010959625244,\n",
       "  1.5028496980667114,\n",
       "  1.3861953020095825,\n",
       "  1.4235354661941528,\n",
       "  1.4967347383499146,\n",
       "  1.4653713703155518,\n",
       "  1.4301279783248901,\n",
       "  1.4506843090057373,\n",
       "  1.4905848503112793,\n",
       "  1.4039162397384644,\n",
       "  1.4323389530181885,\n",
       "  1.4803848266601562,\n",
       "  1.4586156606674194,\n",
       "  1.5407357215881348,\n",
       "  1.4918341636657715,\n",
       "  1.4977309703826904,\n",
       "  1.488716721534729,\n",
       "  1.4939377307891846,\n",
       "  1.5857207775115967,\n",
       "  1.5515564680099487,\n",
       "  1.5361847877502441,\n",
       "  1.5503971576690674,\n",
       "  1.5197021961212158,\n",
       "  1.5711544752120972,\n",
       "  1.6499030590057373,\n",
       "  1.6591299772262573,\n",
       "  1.5808113813400269,\n",
       "  1.6355869770050049,\n",
       "  1.5696563720703125,\n",
       "  1.6600213050842285,\n",
       "  1.672391414642334,\n",
       "  1.660973310470581,\n",
       "  1.620717167854309,\n",
       "  1.6923778057098389,\n",
       "  1.6680763959884644,\n",
       "  1.7117096185684204,\n",
       "  1.7127304077148438,\n",
       "  1.7511866092681885,\n",
       "  1.7677826881408691,\n",
       "  1.7043635845184326,\n",
       "  1.7508246898651123,\n",
       "  1.749536395072937,\n",
       "  1.7412728071212769,\n",
       "  1.7423804998397827,\n",
       "  1.7651641368865967,\n",
       "  1.7711460590362549,\n",
       "  1.778684377670288,\n",
       "  1.7697360515594482,\n",
       "  1.8466089963912964,\n",
       "  1.812268853187561,\n",
       "  1.8169143199920654,\n",
       "  1.7747377157211304,\n",
       "  1.8075505495071411,\n",
       "  1.788260579109192,\n",
       "  1.8109911680221558,\n",
       "  1.8354779481887817,\n",
       "  1.8383772373199463,\n",
       "  1.8251399993896484,\n",
       "  1.8278965950012207,\n",
       "  1.8248074054718018,\n",
       "  1.8322803974151611,\n",
       "  1.8467166423797607,\n",
       "  1.8708995580673218,\n",
       "  1.8701865673065186,\n",
       "  1.8501677513122559,\n",
       "  1.8633140325546265,\n",
       "  1.8513402938842773,\n",
       "  1.8489019870758057,\n",
       "  1.8725004196166992,\n",
       "  1.8786669969558716,\n",
       "  1.9003653526306152,\n",
       "  1.8834314346313477,\n",
       "  1.8943055868148804,\n",
       "  1.9083924293518066,\n",
       "  1.899727463722229,\n",
       "  1.8907736539840698,\n",
       "  1.8884673118591309,\n",
       "  1.8920477628707886,\n",
       "  1.8845832347869873,\n",
       "  1.902248501777649,\n",
       "  1.8750144243240356,\n",
       "  1.8868662118911743,\n",
       "  1.8942173719406128,\n",
       "  1.8960262537002563,\n",
       "  1.8938953876495361,\n",
       "  1.885854959487915,\n",
       "  1.8918814659118652,\n",
       "  1.8977985382080078,\n",
       "  1.9018183946609497,\n",
       "  1.898076057434082,\n",
       "  1.8954788446426392,\n",
       "  1.8958773612976074,\n",
       "  1.8941489458084106,\n",
       "  1.9015928506851196,\n",
       "  1.8999462127685547,\n",
       "  1.9008054733276367,\n",
       "  1.9017398357391357,\n",
       "  1.9021908044815063,\n",
       "  1.902328610420227,\n",
       "  1.9015045166015625,\n",
       "  1.9031765460968018,\n",
       "  1.9038060903549194,\n",
       "  1.9037315845489502,\n",
       "  1.9034137725830078,\n",
       "  1.9038523435592651,\n",
       "  1.9036059379577637,\n",
       "  1.9035300016403198,\n",
       "  1.9033485651016235]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "trainer.train('models/flair/np-chunking',\n",
    "              max_epochs=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
